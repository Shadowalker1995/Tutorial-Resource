WEBVTT

00:05.670 --> 00:12.080
Welcome back to this lecture on parts of speech basics so I've already seen a little bit of parts of

00:12.080 --> 00:13.970
speech in previous lectures.

00:13.970 --> 00:19.730
But let's go and take a deeper dive into POS or parts of speech and what that really means and how we

00:19.730 --> 00:23.100
can use space to actually grab parts of speech tags.

00:23.180 --> 00:29.450
It's important to understand that most words themselves are actually rare in occurrence inside the context

00:29.510 --> 00:35.330
of an entire document and it's common for words that look completely different to mean almost the same

00:35.330 --> 00:40.400
thing and the same words in a different order can actually then and that meaning something completely

00:40.400 --> 00:41.300
different.

00:41.300 --> 00:46.460
So really there's a lot of nuance to the way that words are being used and that is why we need to also

00:46.460 --> 00:51.300
look at the part of speech that word is being used in not just the word itself.

00:52.900 --> 00:58.690
So when you're doing things like splitting text into useful word like units like tokenization that itself

00:58.690 --> 01:00.580
can be difficult in many languages.

01:00.580 --> 01:05.680
And while it's possible to solve some problems starting from only the raw characters or those raw tokens

01:06.070 --> 01:12.100
it's usually better to use linguistic knowledge to add a useful information to know if you mean a noun

01:12.130 --> 01:17.860
versus a verb versus an adjective that part of speech information is really crucial to taking your natural

01:17.860 --> 01:20.100
language processing to the next level.

01:21.380 --> 01:23.610
So that's exactly what space is designed to do.

01:23.660 --> 01:29.090
You put in the raw text and you get back this document object that comes with a variety of annotations.

01:29.090 --> 01:34.580
In this lecture we're going to be taking a closer look at the actual speech tax that spacey gives us

01:34.640 --> 01:35.780
and it gives us two types.

01:35.810 --> 01:38.400
It gives us a course part of speech tags.

01:38.420 --> 01:43.580
Those are your basic parts of speech things like nouns verbs and adjectives but it also gives you fine

01:43.580 --> 01:49.760
grained tags more nuanced information like whether something's a plural noun or a past tense for or

01:49.760 --> 01:53.440
a superlative adjective and as a quick note.

01:53.630 --> 01:58.760
Keep in mind English is an incredibly complex language with many rules and even more exceptions to the

01:58.760 --> 01:59.660
rules.

01:59.660 --> 02:04.820
Teaching English and the various grammatical rules it's outside the scope of this course you can check

02:04.820 --> 02:09.230
out the resource references to learn more about the various parts of speech tags and what they mean

02:09.230 --> 02:12.320
in English especially if English isn't your first language.

02:12.320 --> 02:18.890
Keep in mind I really wouldn't expect people to be a super in tune with the very nuanced details of

02:18.890 --> 02:24.670
grammar such as like a superlative adjective you may need to look that one up but you should be familiar

02:24.670 --> 02:29.030
with things like a noun versus a verb versus an adjective to really get the most out of this part of

02:29.030 --> 02:29.940
the course.

02:31.060 --> 02:34.920
OK let's go ahead and explore part of speech tags in more detail.

02:34.950 --> 02:38.110
Spacey I want to jump over to Jupiter notebook.

02:38.110 --> 02:42.430
All right if you take a look at the parts of speech tacking folder the very first note book we're going

02:42.430 --> 02:44.380
over is called POS basics.

02:44.560 --> 02:48.410
And if you open up that notebook and scroll through it I would highly encourage you to check out this

02:48.480 --> 02:54.400
notebook as if you this lecture because we've provided full tables that discuss the Course great speech

02:54.400 --> 02:54.990
tags.

02:55.090 --> 02:58.780
Those are the basic ones that every token has a POS tag assigned to it.

02:58.810 --> 03:04.780
So we have the code adjective position adverb etc. in description and then some examples in this table

03:04.780 --> 03:05.710
for you at checkout.

03:06.010 --> 03:08.850
But there's also fine grained parts of speech tags.

03:08.980 --> 03:14.770
So tokens are then subsequently after giving a speech tag given a fine green tag and then we have a

03:14.770 --> 03:18.900
description there as well as some morphology information about that particular tag.

03:18.910 --> 03:21.160
So these tables are really useful to reference.

03:21.250 --> 03:24.930
So let's go ahead and start going through this notebook.

03:24.970 --> 03:30.190
We're going to start by saying import Spacey and then we're going to load up the English language library

03:30.220 --> 03:39.170
by saying entropy is equal to speccy load and we'll call it an underscore core web underscore S.M. so

03:39.170 --> 03:44.860
that loads up the English language library for us Spacey and let's create a document.

03:45.160 --> 03:55.130
We're going to passen a unicode string and we'll just say the quick brown fox jumped over the lazy dog's

03:55.130 --> 03:58.070
back.

03:58.100 --> 03:58.750
All right.

03:58.880 --> 04:04.760
Now we already know that if I want to print out the entire document text I just need to say print document

04:04.760 --> 04:07.520
text and that prints out that string text.

04:07.850 --> 04:12.140
And you should recall that you can obtain particular tokens by index position.

04:12.140 --> 04:18.520
So if I want to just grab a particular token like jumped I just need to call that tokens in the Exposition

04:19.000 --> 04:25.700
at index 4 and I get back jumped and then I can call various parts of this token.

04:25.700 --> 04:32.120
So for example I can say something like grab the token text which is what I just saw here or I can grab

04:32.120 --> 04:42.500
the speech tag such as verb or if I want to grab the fine green tag I can say t a g underscore and that

04:42.500 --> 04:44.020
gives me that fine grained tags.

04:44.030 --> 04:49.580
So if we come back up here to this table again remember we have the fine grained part of speech tat.

04:49.820 --> 04:54.590
So I could end up looking for the fine grained tag and start looking up what it actually means.

04:54.590 --> 04:58.020
So in this case jumped has a tag of VB.

04:58.460 --> 05:06.040
And let's check out again what it's course tag was by saying Doc 4 and then POS underscore.

05:06.040 --> 05:06.790
And it was a verb.

05:06.800 --> 05:09.030
So it's a verb with a verb.

05:09.140 --> 05:12.320
So we come back down here to the fine grained speech.

05:12.530 --> 05:16.390
We can scroll down until we find verb and then we're looking for VB.

05:16.580 --> 05:23.150
And it says it's a past tense verb so spacey is smart enough to know that just means it was in the past

05:23.150 --> 05:23.690
tense.

05:23.690 --> 05:28.490
The fox historically jump it jumped in the past over the lazy duck's back.

05:28.490 --> 05:33.190
So we have both the speech this broader or Corser part a speech tag.

05:33.350 --> 05:38.660
And then the fine detailed tack underscore and if you don't provide the underscore it's just going to

05:38.690 --> 05:42.280
give you the numerical ID for that particular tag.

05:42.470 --> 05:49.420
So sometimes it's more useful to have the numerical ID specifically for cases of looking up information.

05:49.630 --> 05:56.500
Now what's nice is you can make yourself a little for loop by saying for token and document and then

05:56.500 --> 06:00.740
you can print out a little table of all this information using string literals.

06:00.850 --> 06:02.430
So we can say something like.

06:02.620 --> 06:09.460
Print out the token text and then print out the token part of speech.

06:10.930 --> 06:17.630
Then print out the token tag and then print out an explanation.

06:17.630 --> 06:22.180
So if you want an explanation you can just call from Spacey explain.

06:22.280 --> 06:23.730
Remember we've seen this before.

06:23.930 --> 06:26.800
And then we can explain that actual tag tag underscore.

06:27.110 --> 06:29.850
And if you run this your formatting may be a little off.

06:29.850 --> 06:31.340
So it's all put together here.

06:31.340 --> 06:33.160
Let's go ahead and give it a little bit of spacing.

06:34.510 --> 06:39.740
By adding this colon tend to a lot of these guys will say call and turn on this one.

06:39.730 --> 06:41.380
CONAN And on that one.

06:41.650 --> 06:46.250
Clinton here and now let's run this and we get an nicer looking table.

06:46.510 --> 06:53.710
So here we can see that took a text the speech the fine grained part of speech tag and then space explanation

06:54.070 --> 06:58.500
for that particular part of speech specifically the token tag.

06:58.720 --> 07:04.960
So this explanation goes with this fine grained tag be can pass in any token speech or a token text

07:05.320 --> 07:07.980
and see a space you can explain it for you.

07:08.010 --> 07:13.530
So we just saw we have the coarse green part of speech tags as well as the fine grained part of speech.

07:13.530 --> 07:18.280
Now in the English language as we mention that same string of characters can have different meanings.

07:18.360 --> 07:28.880
So let's create a documents that says we're going to make it a unicode string here that says I read

07:28.880 --> 07:31.010
books on an LP

07:34.490 --> 07:41.610
and we're actually going to say that the word we're examining is the word read.

07:41.640 --> 07:43.940
So I'm going to grab that read tokin.

07:43.950 --> 07:48.430
So if I check out word text that's that string read.

07:48.450 --> 07:53.480
So just looking at the token Right now let's go ahead and check out the explanation for it.

07:53.520 --> 07:56.540
I'm going to copy and paste this line right here.

07:59.840 --> 08:02.150
And this is printing out that token information.

08:02.370 --> 08:08.640
Let's go ahead and just redefine token as the word we're looking at and run this.

08:08.890 --> 08:10.590
And in this context I can see.

08:10.620 --> 08:15.180
I read books on an LP read is a verb which makes sense.

08:15.180 --> 08:17.480
The person is reading the book.

08:17.670 --> 08:21.170
But now let's create another document.

08:21.180 --> 08:22.730
It's going to look really similar.

08:22.740 --> 08:26.380
There's going to be slightly different.

08:27.150 --> 08:35.860
We're going to say I read a book on an LP and there is a difference here.

08:35.880 --> 08:42.030
This person reads books on an LP versus I read a book on an LP.

08:42.030 --> 08:43.570
So we're going to run that again.

08:44.280 --> 08:47.950
And now we're going to grab that cell information.

08:49.190 --> 08:56.230
Paste it in here and let's go ahead and read the fine the word again.

08:56.450 --> 09:01.730
So same thing I'm grabbing read except now the context I've read is different and the context is so

09:01.760 --> 09:04.010
my new as far as the difference here.

09:04.010 --> 09:04.760
What I'm saying.

09:04.820 --> 09:06.860
I read books on an LP.

09:06.880 --> 09:13.920
The other one saying I read a book on an LP and when you run this you'll notice that Spacey is smart

09:13.920 --> 09:19.020
enough to understand that when you say I read a book on an LP you're referring to something in the past

09:19.020 --> 09:24.540
tense versus this first one I read books on an LP is actually in the present.

09:24.600 --> 09:28.430
This person currently reads books on natural language processing and the second one.

09:28.470 --> 09:32.340
This person in the past had read a book on natural language processing.

09:32.400 --> 09:37.320
So this is kind of amazing the fact that spacey can take all of this context based off the surrounding

09:37.320 --> 09:37.990
words.

09:38.160 --> 09:42.210
It's really quite incredible what Spacey has done here next.

09:42.220 --> 09:45.510
Let's show you how you can actually count parts of speech tags.

09:45.610 --> 09:51.640
So the document object has a count by a method that accepts a specific token attribute as its argument

09:52.000 --> 09:56.950
and then returns a frequency count of the given attribute as a dictionary object.

09:56.950 --> 10:00.340
So let's show you this.

10:00.440 --> 10:01.390
I'm going to.

10:01.430 --> 10:06.360
Again let's actually use the same string as before where we said the lazy dog.

10:06.380 --> 10:08.780
So let's just copy this cell right here.

10:09.900 --> 10:12.630
And I'm going to put that in.

10:12.630 --> 10:17.970
So read the fine or document as the quick brown fox jumped over the lazy dog's back and let me show

10:17.970 --> 10:21.010
you how to get parts of speech counts.

10:21.150 --> 10:31.490
I can grab that doc and then say call count underscore by and we'll say Spacey thought Archie us are

10:31.520 --> 10:39.250
82 years excuse me for attributes and then dot P O S and what this is going to do is it's going to return

10:39.250 --> 10:42.770
a dictionary so speech counts.

10:42.910 --> 10:47.050
And then you may be wondering when you get back to dictionary what it's actually representing.

10:47.050 --> 10:53.170
Well we asked for a count by so we can tell that the values appeared to be some sort of count but these

10:53.170 --> 10:56.320
numbers aren't very useful by themselves.

10:56.320 --> 10:59.250
These numbers are actually the parts of speech code.

10:59.260 --> 11:05.440
Recall that when we came back up here if we actually didn't include an underscore we wouldn't get the

11:05.440 --> 11:07.210
name of the speech we'd get back.

11:07.220 --> 11:09.200
It's numerical identifier.

11:09.220 --> 11:16.500
Now we can always look up numerical identifiers simply by calling the documents vocab and then passing

11:16.500 --> 11:18.000
in whatever number we're interested in.

11:18.180 --> 11:20.190
So let's say I want to know what 83 stands for.

11:20.220 --> 11:27.830
I'll just pass an 83 and then ask for the text of that and I'll return back that it's an adjective again

11:27.840 --> 11:32.110
recall that if we just grab any token off of a document.

11:32.160 --> 11:41.510
So for example let's go ahead and just to make sure we run this let's grab Brown So here's document

11:41.520 --> 11:42.240
to.

11:42.330 --> 11:43.890
It's token brown.

11:44.030 --> 11:48.450
I can ask for the parts of speech underscore and it returns back a..

11:48.650 --> 11:52.960
Or if I just say POS it returns back its actual numerical code.

11:53.030 --> 11:54.560
That's what the speech counts.

11:54.580 --> 11:59.600
Dictionary is doing it's returning back the number and then how many times it showed up.

11:59.600 --> 12:04.910
So here we can tell that additives there's three adjectives inside of this document.

12:04.910 --> 12:08.590
The quick brown fox jumped over the lazy dog's back.

12:08.600 --> 12:13.610
Now if you want to actually create a frequency list of of speech tags from the entire document you can

12:13.610 --> 12:16.630
do that by simply making a for loop that looks something like this.

12:16.630 --> 12:30.490
We can say for key value in that we can call sordid POS counts items which is a essentially a list of

12:30.490 --> 12:34.090
tuples we can print out.

12:34.110 --> 12:36.740
Well let's use a little extra formatting here.

12:36.760 --> 12:42.350
We'll print out the key and then do a little space.

12:42.600 --> 12:47.070
Well say Doc vocab at that particular key.

12:47.880 --> 12:52.230
Print out the text and then add in a little bit of formatting here.

12:52.990 --> 12:55.360
The string literal formatting.

12:55.360 --> 13:00.090
And then we can passen the the actual value for it.

13:00.090 --> 13:03.050
So let's make sure we then commit a typo here run this.

13:03.230 --> 13:04.010
And there we go.

13:04.010 --> 13:09.760
So it's telling us that for numerical code 83 which is an adjective we have three adjectives.

13:09.980 --> 13:13.470
We have three nouns one punctuation and so on.

13:14.790 --> 13:20.160
And keep in mind you can do this not just for the course of speech tags but for the fine grained tags

13:20.220 --> 13:21.090
as well.

13:21.090 --> 13:26.820
Recall again from our table that right now we're just looking at this part of speech this more course

13:26.820 --> 13:27.310
tad.

13:27.480 --> 13:31.920
But there's also these fine grained tags and we could do the same thing for it so we can come back here

13:32.460 --> 13:36.780
and essentially copy and paste this.

13:37.200 --> 13:43.920
But then we're going to do is instead of saying POS counts we're going to say tag counts and let's create

13:43.950 --> 13:53.350
that tag counts object will say tag counts as equal to the documents and we'll say count by an also

13:53.390 --> 14:01.410
a Speccy €80 for attributes and instead of saying POS we'll say tag and then if you run that now you

14:01.410 --> 14:03.180
get to see the tags.

14:03.210 --> 14:09.870
So again notice here we have the numerical value the fine grained part speech and then how many times

14:09.870 --> 14:11.020
it showed up.

14:11.040 --> 14:16.530
Now you're probably wondering why are these numbers so much larger than the numbers for the basic tags

14:16.530 --> 14:18.690
like adjective and noun.

14:18.690 --> 14:26.370
Well the reason for that is because in speccy certain text values are hard coded into the document vocab.

14:26.550 --> 14:32.550
So that's the DRC that vocab and they take up the first several hundred ID numbers so the most common

14:32.550 --> 14:35.190
ones are the ones that are closer up in the list.

14:35.190 --> 14:39.180
So things like nouns and punctuations those are really commonly.

14:39.180 --> 14:43.830
So that's why they're kind of smaller numbers are further up in that list closer to that first item

14:43.830 --> 14:48.090
in the list the ones that don't get used that often they get pushed way further down.

14:48.120 --> 14:52.310
I remember Doc vocab also has essentially all the English words available to it.

14:52.350 --> 14:59.270
Remember the length of that vocab if we check it out it's actually this really big vocabulary of fifty

14:59.270 --> 15:01.180
seven thousand eight hundred sixty three.

15:01.190 --> 15:05.480
So for purposes of efficiency the most common ones are placed closer to the front of the list.

15:05.480 --> 15:07.610
That way when you search for them it's more efficient.

15:07.610 --> 15:14.810
And the ones that use less those get put down with kind of larger hashes and that really has to do more

15:14.820 --> 15:19.280
with the way that space works internally through its own internal operations.

15:19.290 --> 15:22.230
The thing is you're going to see as a user on your end.

15:22.440 --> 15:28.320
So as a user on our end work in a space the size of this number it's kind of irrelevant to us we just

15:28.320 --> 15:35.730
care that there is a number identifier for each particular text code of either a speech tag or a particular

15:35.730 --> 15:40.800
fine green tag and we can do the exact same thing for syntactic dependencies.

15:40.830 --> 15:48.530
So again we can copy and paste this code here and we can say DGP counts for syntactic dependencies.

15:48.690 --> 15:50.250
Look up the epi.

15:50.280 --> 15:56.580
So basically changing tags everywhere to the AP run that and you can see the syntactic dependency tags

15:56.950 --> 15:57.770
again.

15:57.810 --> 16:01.910
Kind of less common ones having larger numbers here and so on.

16:01.920 --> 16:05.510
OK so that's all there is for our discussion right now.

16:05.550 --> 16:09.290
Of course POS tags versus fine grained POS tags.

16:09.360 --> 16:14.700
The main thing I understand from this lecture is that you can always call the tags off any token by

16:14.700 --> 16:20.550
saying POS underscore and tag underscore if you forget the underscore it's just going to show you that

16:20.550 --> 16:27.120
raw number and you should also keep in mind they can also call spaces that explain to explain its token

16:27.120 --> 16:29.020
tags or its token part of speech.

16:29.100 --> 16:35.610
And we also have this table that kind of lines them up all nicely for us then as a quick reminder if

16:35.610 --> 16:40.710
you ever want to do a count of how many parts of speech you have you can say take your document call

16:40.710 --> 16:45.520
count by and then call Spacey the attributes whatever attribute you want it counts.

16:45.570 --> 16:48.830
Coming up next we're going to review how to visualize parts of speech.

16:48.900 --> 16:49.510
We'll see if they're.

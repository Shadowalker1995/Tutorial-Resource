WEBVTT

00:05.230 --> 00:10.960
Welcome back to this lecture on sentence segmentation in space the basics we saw briefly how Doc objects

00:10.990 --> 00:13.780
are divided into sentences in this lecture.

00:13.780 --> 00:18.670
We're going to learn how sentence segmentation works and how to set our own segmentation rules to break

00:18.670 --> 00:21.700
up Dock's into sentences based off our own rules.

00:21.700 --> 00:22.820
Let's go ahead and check this out.

00:22.840 --> 00:24.090
In a Jupiter notebook.

00:24.340 --> 00:29.780
OK here I am at the notebook and right now I imported Spacey I've loaded up the English library set

00:30.220 --> 00:36.280
and also I've created a doc object out of this string right here where I have this is the first sentence.

00:36.300 --> 00:37.400
This is another sentence.

00:37.450 --> 00:38.680
And then this is the last sentence.

00:38.680 --> 00:40.320
So there's three sentences here.

00:40.390 --> 00:50.230
And as we know from space the basics what I can do is say four sent in doc since printout sent where

00:50.260 --> 00:54.550
Doc that sense actually separates everything into sentences.

00:54.550 --> 01:00.310
Keep in mind that this doc dot se A.S. is actually a generator.

01:00.430 --> 01:06.700
So if you were to try to index off of sense it would actually fail.

01:06.850 --> 01:11.890
You'd get an error saying a generator object is not subscript will because this generates the sentences

01:11.980 --> 01:13.990
instead of holding them all in memory.

01:14.020 --> 01:18.240
That's a safe space because as you can imagine this document object can be huge.

01:18.250 --> 01:24.640
So you don't want all of the saved duplicated in memory instead you can iterate through the sentences.

01:24.640 --> 01:30.370
However for the actual document itself since it knows from the language library how to quickly look

01:30.370 --> 01:37.480
up tokens you can easily grab individual tokens from the dock but you can't grab sentences individually

01:38.650 --> 01:40.120
from that sense.

01:40.240 --> 01:46.830
If you really need to do this all you need to do is just pass this into a list so you can say list thok

01:46.840 --> 01:52.390
sense and then you gonna have a list here of the sentences they can then index off of order to grab

01:52.390 --> 01:53.470
the sentences.

01:53.470 --> 01:56.560
However keep in mind that if you actually look at the type here

01:59.380 --> 02:04.630
these are going to be Spacey spand objects not just normal strings.

02:04.630 --> 02:10.570
Now typically Spacey is really good at separating out sentences itself based off things like the start

02:10.570 --> 02:12.950
of a sentence or the end of a sentence.

02:13.000 --> 02:18.700
However you may have a custom dataset for sentence segmentation you want to follow your own particular

02:18.700 --> 02:19.450
set of rules.

02:19.510 --> 02:24.190
Maybe you want to segment's based off a semi-colon which isn't technically the stop of a sentence.

02:24.190 --> 02:30.780
Well let's actually create a new object that is going to be a document object.

02:30.830 --> 02:35.630
It's going to be Unicode and I'm going to use single quotes on the outside because it's going to be

02:35.660 --> 02:38.600
kind of an interesting set of strings.

02:38.620 --> 02:46.290
Well we're going to do is say have a little quote here like management is doing the right things.

02:46.460 --> 02:52.080
Semi-colon leadership is doing the right things.

02:53.090 --> 02:56.220
And then this is itself going to be a quote inside of this text.

02:56.210 --> 02:58.620
I'm going to zoom out so you can see the effect here.

02:58.760 --> 03:05.520
And then I'm going to attribute this for dash to Peter Drucker.

03:05.710 --> 03:06.700
So we're going to run that.

03:06.940 --> 03:09.860
And if we take a look at the actual text of this document.

03:09.880 --> 03:14.450
So if I just say Doc text I can see management is doing the right things.

03:14.500 --> 03:16.990
Semi-colon leadership's doing right things.

03:16.990 --> 03:19.590
This first phrase is inside its own set of double quotes.

03:19.630 --> 03:25.480
And then we see that Peter Drucker first let's see what happens with the default settings of sentence

03:25.480 --> 03:26.980
segmentation.

03:26.980 --> 03:40.410
If I say force sent in doc since Prince the sentence and then Prince a new line if I run that notice

03:40.410 --> 03:46.180
it got separated out into two sentences the first one is the long quote and the second one is this dash

03:46.230 --> 03:48.370
Peter Drucker that may be right.

03:48.390 --> 03:51.810
That may be wrong depending on your particular use case.

03:51.840 --> 03:58.460
So let's show you how to add a new rule to the pipeline that gets created when you wrap this in an LP.

03:58.480 --> 04:01.690
So now we're going to do is show you basically two approaches.

04:01.690 --> 04:10.830
One is to add a segmentation rule basically adding a new thing to segment on and the other is essentially

04:10.830 --> 04:12.220
changed the rules entirely.

04:12.270 --> 04:21.060
So change segmentation rules so by default you can see from previous examples if we scroll up that we

04:21.060 --> 04:25.270
split essentially on periods and white spaces here to see the sentences.

04:25.380 --> 04:30.540
So maybe that's not the case in which case you'll need to replace all the rules but maybe you also want

04:30.540 --> 04:35.700
to separate on the normal separations but add in something like separating on semi-colons.

04:35.700 --> 04:39.660
So we're going to show you first how to add a segmentation rule that will show you how to change all

04:39.660 --> 04:41.090
the segmentation rules.

04:41.100 --> 04:45.520
Let's first show you how to add a segmentation rule and do that we need to create a function.

04:45.570 --> 04:51.800
We'll start off by calling our functions set custom underscore boundaries.

04:51.930 --> 04:57.450
And is going to accept a document object and I want to show you a couple of things that we can do with

04:57.450 --> 04:58.710
a document object.

04:58.710 --> 05:04.280
The first thing is that every token inside a document actually retains its index position.

05:04.290 --> 05:13.200
So for example I can say for token in documents print out token I and if we actually then run set custom

05:13.200 --> 05:20.460
boundaries on our current documents you'll see here that every token contains its index position.

05:20.460 --> 05:27.840
So for example I can print the token and the tokens so we can see the first set of double quotes is

05:27.850 --> 05:28.600
0.

05:28.600 --> 05:32.760
This management is token number one is a token number two and so on.

05:32.770 --> 05:33.510
So we have this.

05:33.590 --> 05:37.440
I for the index position which is going to be useful as you'll see in a second.

05:37.780 --> 05:43.480
So what we're going to do is we'll say for tokin in doc but we're going to do some indexing here on

05:43.480 --> 05:44.560
this doc object.

05:44.620 --> 05:47.330
Say colon negative 1.

05:47.470 --> 05:53.560
And the reason for that is if we actually take a look at document colon negative one this goes all the

05:53.560 --> 05:59.470
way from the beginning up to but not including the very last token and you'll see why we have to do

05:59.470 --> 06:00.360
that in just a second.

06:00.390 --> 06:07.090
We're going to do is we'll go through every tokin up to and including the last one and we'll say if

06:07.090 --> 06:13.180
tokin text is equal to and then we're going to add a new segmentation rule so we'll say if it's equal

06:13.180 --> 06:23.330
to a semi colon Well we're going to do is say that documents at that index position plus one.

06:23.440 --> 06:29.720
So that very next one will say is sentence are sent.

06:29.740 --> 06:33.070
Start is equal to true.

06:33.110 --> 06:38.080
So essentially all we're doing is we're going along through every tokin inside of this document.

06:38.120 --> 06:43.640
And once we see a semi colon we're going to say OK you just saw a semi colon the very next word or token

06:43.640 --> 06:49.200
after that that's going to be the start of a new sentence which is why we're saying token I plus one.

06:49.460 --> 06:55.370
And since we're saying I plus one we only need to go up to Peter because the index of Peter plus one

06:55.370 --> 06:56.820
is going to be his last name trucker.

06:56.990 --> 07:01.460
So that's why we're going from the beginning all the way up to the end minus the very last word because

07:01.460 --> 07:03.460
we're at ease this plus one.

07:03.740 --> 07:09.580
And we want to make sure that we don't actually go past the documents.

07:09.590 --> 07:12.870
Otherwise we'll get some index out of range here.

07:13.040 --> 07:17.420
So again we're going through each of these tokens once we says some I call and we're going to say that

07:17.420 --> 07:21.530
very next word such as leadership that's going to be the start of a sentence.

07:21.530 --> 07:23.980
We'll set that attribute equal to true.

07:24.080 --> 07:30.330
And then at the end once you're done going through for a loop we're going to return the documents so

07:30.330 --> 07:31.420
we know how to run that.

07:31.890 --> 07:38.550
And then you're going to call an LP and you're going to say add underscore pipe and you get a passing

07:38.870 --> 07:49.780
custom boundaries and you're going to say before equals parser and then we can say and O.P. that pipe

07:49.960 --> 07:54.460
underscore names and you can see here we have the tagger.

07:54.460 --> 08:00.160
So that happens first then our custom boundaries happens the pipeline then the parser and then the named

08:00.220 --> 08:01.360
entity recognition.

08:01.540 --> 08:07.290
Notice that tagger parser and the recognition those all happen on the pipeline automatically.

08:07.300 --> 08:10.390
So let's go ahead and rerun the document creation.

08:10.660 --> 08:17.350
So before we actually get to changing segmentation rules we're going to do is create a couple more cells

08:18.550 --> 08:22.050
here and say Doc 4.

08:22.540 --> 08:27.950
So brand new documents in LP and we're going to copy and paste this long quote.

08:29.240 --> 08:33.310
So copy that paste it in.

08:33.420 --> 08:38.900
So I had this new document for and then what I'm going to do is say the following.

08:39.850 --> 08:47.070
Four cents in doc for cents print sent.

08:47.660 --> 08:52.160
So when you run that notice now it's separating on that semi-colon.

08:52.160 --> 08:59.810
When I ran the original documents and asked it to print sentences it printed out the entire quote and

08:59.810 --> 09:01.050
then Peter Drucker.

09:01.250 --> 09:08.210
But now I've created a new pipeline set my custom boundaries inside of the pipeline and then again read

09:08.210 --> 09:13.320
the fine documents and then you can see here it's separating on the semi colon.

09:13.340 --> 09:16.220
So that's how you can add a segmentation rule.

09:17.130 --> 09:20.910
So what happens if you want to change the rules completely.

09:20.910 --> 09:25.500
So in some cases we want to replace spaces default sensitizer with our own set of rules.

09:25.590 --> 09:30.450
So we're going to see now how the default sensitizer breaks on periods and they're going to replace

09:30.450 --> 09:34.310
that behavior of the sense incisor to maybe break on something like line breaks.

09:35.080 --> 09:44.550
So again we're going to say an LP is equal to space e the load and I'm reloading the English library.

09:44.620 --> 09:48.760
So I kind of reset to the original default behavior.

09:48.970 --> 09:52.960
Otherwise my cousin boundaries that I created here is going to be messed up.

09:53.200 --> 09:54.740
So I've reloaded this.

09:54.760 --> 10:02.800
Ran that line again and then we're going to create a string will say my string is a unicode string and

10:02.800 --> 10:06.220
says this is a sentence.

10:06.220 --> 10:12.580
This is another and we're actually going to then add in some line breaks here so I add in some line

10:12.580 --> 10:13.690
breaks.

10:13.780 --> 10:19.400
Add two in a row and then say this is a we'll do another line break.

10:19.850 --> 10:23.010
We'll say third sentence.

10:23.300 --> 10:23.580
OK.

10:23.590 --> 10:29.500
So if I were actually prints my string here it says this is a sentence this is another.

10:29.500 --> 10:36.280
This is a third sentence you can imagine for something like a text dataset of poetry line breaks like

10:36.280 --> 10:40.320
this are actually more important than the periods.

10:40.360 --> 10:47.710
And you may want to the fine line breaks themselves as the actual end of a sentence instead of what

10:47.710 --> 10:50.030
is classically known as the end of a sentence which is a period.

10:50.050 --> 10:52.740
So for something like approach with data set this could be really important.

10:52.750 --> 10:56.790
Now let's see what happens if we check out the default behavior first.

10:56.830 --> 11:03.640
We're going to say Doc is equal to an LP will pass in my string and now take a look at what the default

11:03.670 --> 11:07.710
segmentation is will say for sentence.

11:08.050 --> 11:14.320
And talk about sense print out the sentence.

11:14.610 --> 11:20.140
And when we run this you'll notice that it's actually able to pick up on this double set of new lines

11:20.520 --> 11:22.680
but it automatically separates the first.

11:22.680 --> 11:24.280
This is a sentence from.

11:24.280 --> 11:25.470
This is another.

11:25.650 --> 11:31.820
And that's going to be an issue for us if we only want the new line to be the indicator of a new segment.

11:31.950 --> 11:36.800
So maybe you really want three segments here this one this one and this one.

11:37.080 --> 11:40.720
Instead of separating on standard segments.

11:40.980 --> 11:52.070
So if we want to change the rules we need to call from Spacey thought pipeline import sentence segments

11:52.070 --> 11:59.350
or and then we'll create a function that will basically tell Spacey and its pipeline what it should

11:59.350 --> 12:08.450
be using for segmentation and we'll call this function split on we'll call it split on new lines.

12:08.470 --> 12:12.900
It takes the documents we're going to find start at zero.

12:14.400 --> 12:19.620
And we'll say have I seen a new line or have I seen a new separator or whatever you want your segmentation

12:19.620 --> 12:25.530
to be and we'll start off as false and then we'll say for word in the documents

12:28.180 --> 12:32.370
we've seen a new line then this is going to be a generator.

12:32.380 --> 12:41.980
I will yield doc from start to the current words in the expedition and hear word you could replace it

12:41.980 --> 12:45.550
with token if that makes more sense to you it's just a variable name.

12:45.550 --> 12:52.330
And then we'll say the New START is equal to the word at that are the words in solution there and we'll

12:52.330 --> 13:05.400
reset seen new line equal to false and then we'll say oh if the word text and then we're going to call

13:05.400 --> 13:11.790
the starts with method which is just a normal method available often in Python string will say if the

13:11.790 --> 13:17.580
word text starts with a new line then we've just seen a new line.

13:17.620 --> 13:24.040
Set it to true and then at the end of this for loop we're going to yield

13:27.660 --> 13:31.860
from start all the way to the end.

13:31.860 --> 13:33.340
So do we actually doing here.

13:33.510 --> 13:34.550
We start at zero.

13:34.560 --> 13:38.250
We haven't seen a new line yet or a new segmentation so we'll set that equal to false.

13:38.460 --> 13:43.470
And then for every token or word in that document if we've already seen the new line then we're going

13:43.470 --> 13:50.520
to yield from the start up to that current tokens index will reset that current tokens index as a start

13:50.520 --> 13:53.770
position and we'll set senior line equals to false.

13:53.940 --> 14:01.150
Else if we actually see that that particular tokens text starts with and here backslash and you can

14:01.140 --> 14:07.620
replace that with whatever segmentation or separator you want then we reset the new line equals to true.

14:07.620 --> 14:12.420
So again the things that you would replace for your own segmentation are essentially whatever your indicator

14:12.420 --> 14:13.710
is here which starts with.

14:13.800 --> 14:19.160
And if you want you can change the variable names from senior line to seen my segments.

14:19.220 --> 14:19.750
OK.

14:20.040 --> 14:25.350
And then we're returning back or generating these chunks of the document.

14:26.400 --> 14:34.070
So then what we do is we just say create a variable like a speedy and then we call sentence segment

14:34.070 --> 14:34.550
later.

14:35.770 --> 14:37.340
Pasand the vocab.

14:38.320 --> 14:44.290
And then we say strategy we're basically replacing the default strategy for sentence segmentation and

14:44.290 --> 14:49.840
then we pass in our function split on new lines and then we're going to add to the pipeline or we'll

14:49.840 --> 15:00.650
say an LP at Pipe espie the and if you check out now when we say Doc is equal to an LP strings we need

15:00.650 --> 15:02.160
to redefine that document.

15:02.660 --> 15:12.830
Well now say for sentence in Doc thought sense Prince and there we're going to print out for example

15:13.580 --> 15:15.930
the actual sentence.

15:16.010 --> 15:21.080
So if you run this notice what happens we're saying this is a sentence this is another we are no longer

15:21.080 --> 15:24.990
splitting by default on this is a sentence and then period another line.

15:24.990 --> 15:26.360
This is another instead.

15:26.390 --> 15:30.640
Now this whole thing is being treated as a single segment's.

15:30.890 --> 15:35.900
So notice again the difference here by default Spacey was splitting on the periods.

15:35.900 --> 15:41.540
Now we've completely overwritten the sentence segmentation rules to our own strategy where we're splitting

15:41.540 --> 15:42.980
on new lines.

15:43.340 --> 15:44.230
OK.

15:44.240 --> 15:46.200
And that's it for this lecture.

15:46.250 --> 15:51.290
Hopefully now you realize that you can use the default sentence segmentation.

15:51.290 --> 15:53.640
You can add rules on what to segment on.

15:53.780 --> 15:57.190
Or you can completely replace the rules if you need so.

15:57.500 --> 15:59.180
Thanks and we'll see you at the next lecture.

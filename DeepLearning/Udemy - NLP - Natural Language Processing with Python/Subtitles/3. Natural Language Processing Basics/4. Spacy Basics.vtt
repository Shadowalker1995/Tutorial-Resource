WEBVTT

00:05.370 --> 00:06.300
Welcome back everyone.

00:06.330 --> 00:09.130
This lecture on Spacey basics.

00:09.280 --> 00:11.530
There are a few key steps forward going to space.

00:11.640 --> 00:13.250
We're going to cover in this lecture.

00:13.540 --> 00:15.880
The first step is loading the language library.

00:15.880 --> 00:20.830
Remember you need to have install the language library as shown in the spacey setup lecture.

00:20.830 --> 00:26.050
Then once you've loaded in that language library we build the pipeline object and from that pipeline

00:26.050 --> 00:32.860
object we can use tokens perform parts speech tagging and understand different token attributes.

00:32.870 --> 00:39.230
So as I mentioned Spacey works if a pipeline object and the main idea here is that there is an end of

00:39.310 --> 00:41.560
the function that we're going to create from space.

00:41.720 --> 00:47.480
That is going to automatically take in raw text and perform a series of operations that is going to

00:47.480 --> 00:56.150
tag parse and describe the text data and those operations are known as tokenization parsing piece of

00:56.150 --> 00:57.940
speech recognition and so on.

00:58.010 --> 01:00.980
And we're going to cover each of those and a lot more detail later on.

01:02.150 --> 01:08.300
So our purpose for this lecture is is to discover the pipeline object and various series of operations

01:08.660 --> 01:09.950
in subsequent lectures.

01:09.950 --> 01:14.590
We're going to dive a lot deeper into each of these individual aspects of an LP and spacey.

01:14.630 --> 01:20.510
So we're going to talk about tokenization parts of speech or POS stemming limitation and a lot more

01:20.510 --> 01:21.380
detail.

01:21.410 --> 01:25.330
So keep in mind we're going to introduce those terms right now in this lecture but we're going to Diven

01:25.390 --> 01:28.080
to a lot more greater detail later on.

01:28.170 --> 01:29.500
OK let's get started.

01:29.520 --> 01:31.290
But opening up a Jupiter notebook.

01:31.480 --> 01:32.460
OK here I am.

01:32.510 --> 01:39.000
What is essentially the repo for the course underneath the python basics the very first note book is

01:39.000 --> 01:40.460
called space basics.

01:40.500 --> 01:43.360
That's the notebook we're going to be going over in this lecture.

01:43.410 --> 01:46.690
I highly encourage you to also check out that notebook on your own.

01:46.990 --> 01:51.320
There's lots of useful information including installation and set up information that we cover in the

01:51.320 --> 01:52.580
space the set up lecture.

01:52.680 --> 01:57.540
But there's also a table so if you start scrolling down you'll eventually find tables that talk a little

01:57.540 --> 02:00.510
bit more about tokens and tokenization.

02:00.690 --> 02:03.680
So definitely take a look at this as we go along here.

02:03.750 --> 02:09.510
I'm going to open up a new notebook just create a new untitled notebook here and we're going to do is

02:09.510 --> 02:11.160
essentially go along that lecture.

02:11.160 --> 02:12.160
So let's get started.

02:13.290 --> 02:16.100
The first thing we need to do is actually import Spacey.

02:16.440 --> 02:21.200
So we'll say import Spacey and then we're going to load the language library.

02:21.330 --> 02:30.870
We will say an LP is equal to Spacey load and then we're going to pasan a specific string an underscore

02:30.900 --> 02:37.290
core which stands for core English language and then Web S-M which is essentially the small version

02:37.290 --> 02:38.510
of this language library.

02:38.760 --> 02:40.490
And then we're going to hit enter here.

02:41.100 --> 02:46.670
And don't worry if this takes a long time to load the very first time you run it it usually takes a

02:46.670 --> 02:47.040
while.

02:47.040 --> 02:52.050
It's a fairly large library but it's also part of what makes space so efficient is that a lot of what

02:52.050 --> 02:55.920
it's running on top of it's preloaded into this library.

02:55.920 --> 03:00.930
The next step we're going to do is create a doc object or a document object.

03:00.930 --> 03:07.650
So we'll create a variable called the OSI and we're going to pasan a unicode string.

03:07.650 --> 03:16.110
That means we prefer that string with a U and we're just going to pass in a string that says Tesla is

03:16.110 --> 03:18.270
looking at buying

03:20.910 --> 03:28.340
U.S. startup for dollar sign 6 million.

03:28.830 --> 03:35.170
OK so what's actually going to happen here is using the language library that we just loaded that spacey

03:35.170 --> 03:35.980
developed.

03:36.160 --> 03:42.160
It's going to essentially parse this entire stream into separate components for us and it's going to

03:42.670 --> 03:45.640
parse it into what are known as tokens essentially.

03:45.790 --> 03:48.830
Each of these little words is going to become a token.

03:48.850 --> 03:57.630
So what it can do is say for token in doc so I can actually iterate through this document object.

03:57.830 --> 04:05.210
And then I can print out the token and there's various attributes I can actually grab from each token.

04:05.490 --> 04:12.810
So for example I can grab the token text and the token text is actually the raw text that it grabs notice

04:12.960 --> 04:21.550
that it's smart enough to actually treat you as Dot these capital U and S as a single token Spacey threw

04:21.570 --> 04:25.970
a lot of development is actually smart enough to realize that when we say something like capital you

04:25.970 --> 04:29.070
dot capital S dot we're talking about the country.

04:30.120 --> 04:35.190
It's also smart enough to realize that this dollar sign and the six should probably be separated the

04:35.190 --> 04:41.720
dollar sign stands for US dollars and the stands for an amount and then million stands for another amount.

04:41.850 --> 04:45.030
And later on we'll see that with named entity recognition.

04:45.030 --> 04:50.040
Spacey can actually tell things like Tesla as being part of a company.

04:50.110 --> 04:54.970
So we're going to hear it's a print token that XTi and then we're also going to print out some more

04:54.970 --> 04:55.320
stuff.

04:55.330 --> 05:01.080
Let's go ahead and print out token POS which stands for part of speech.

05:01.510 --> 05:07.480
So when we run that we now see this ninety five ninety nine ninety nine.

05:07.570 --> 05:11.920
And later on we're going to see that each of these numbers actually corresponds to the parts of speech

05:11.950 --> 05:20.560
like an adverb a verb a noun conjugation etc. if you actually want the Ronayne used in is say POS underscore

05:21.590 --> 05:24.320
Bruhn that and it will tell you what part of speech it is.

05:24.340 --> 05:31.470
So is it smart enough to know that Tesla is a proper noun is a verb looking a verb proper noun for us

05:31.480 --> 05:33.330
start up as a noun and so on.

05:33.470 --> 05:38.510
Still smart enough to realize that million here is a number and so is six.

05:38.560 --> 05:40.610
And then the star sign is a symbol.

05:40.810 --> 05:45.880
So already right off the bat Spacey knows a lot of information about each of these tokens.

05:47.290 --> 05:57.790
And then you can also do things like print tokens dot and if you do deeply underscore it's going to

05:57.790 --> 05:59.900
give you even more information.

06:01.040 --> 06:07.310
Anti-peace stands for syntactic dependency and we'll talk about that in more detail when we talk about

06:07.310 --> 06:08.720
specific tokens.

06:08.720 --> 06:14.810
But hopefully now you can see that it's really incredible the capabilities of spacey along of natural

06:14.810 --> 06:19.330
language processing to grab a lot of information just from a simple string.

06:19.520 --> 06:23.970
It recognize that Tesla is a proper noun not just a word at the start of a sentence.

06:23.990 --> 06:28.430
So Tesla here is capitalized not just because it's the start of a sentence but also because it's a proper

06:28.430 --> 06:28.770
noun.

06:28.940 --> 06:34.380
And it was able to tell that it's also understood that us these dots don't separate it.

06:34.430 --> 06:35.440
It's a single entity.

06:35.450 --> 06:36.780
It's a single token.

06:36.850 --> 06:41.210
And as we dive deeper into Spacey we're going to see where each of these abbreviations mean and how

06:41.210 --> 06:42.080
did arrives.

06:42.350 --> 06:48.380
We'll also see how Spacey can interpret the last three tokens combine to six million dollar sign that

06:48.380 --> 06:54.720
six million it's going to be able to understand that all of this is some sort of quantifier of money.

06:55.170 --> 06:59.760
But let's go ahead and move on and discuss the pipeline object.

06:59.780 --> 07:05.780
So after I'm putting the space module in the cell above we loaded a model and named an LP.

07:05.990 --> 07:11.930
So this actual line of code is known as loading a model and we called it an LP.

07:11.940 --> 07:19.520
Next we created a dock or a document object by applying this model in LP to our text.

07:19.560 --> 07:26.780
Spacey also builds a companion vocab object for vocabulary and we'll cover that in later lectures.

07:27.050 --> 07:33.430
This doc object that we created holds the process text and that's really the focus of this lecture.

07:33.650 --> 07:43.070
So off of this and help the object Well we can do is call DOT pipeline.

07:43.160 --> 07:49.400
So when we run an LP R-Texas entering a processing pipeline the first breaks down the text and then

07:49.400 --> 07:55.490
performs that series of operations of tagging parsing and describing that data so we can see here the

07:55.490 --> 08:02.540
basic a.p pipeline is a tagger a parser and then any R which stands for named entity recognizer and

08:02.540 --> 08:05.720
we'll talk about each of these in a lot more detail later on.

08:05.990 --> 08:14.030
And you can also just get the basic names by saying a.p LP pipe underscore names and you can play around

08:14.840 --> 08:20.260
with the various attributes here which we're going to talk about as we continue on throughout the course.

08:20.270 --> 08:24.010
The first one I want to discuss just quickly is tokenization.

08:24.010 --> 08:29.560
So the very first step in processing any text is to split it up all the component parts.

08:29.690 --> 08:36.410
That is the words and punctuation into tokens and these tokens are annotated inside the doc object to

08:36.440 --> 08:38.980
contain descriptive information.

08:39.050 --> 08:40.870
So let's go ahead and see an example.

08:41.750 --> 08:48.260
I'm going to create an other document talk to an We're going to Pessin a unicode string means it starts

08:48.260 --> 08:56.690
a few and we'll say Tesla isn't looking into startups anymore.

08:58.580 --> 08:59.290
We run that.

08:59.510 --> 09:06.640
And then let's go ahead and print out the tokens for token doc to and we're going to print out same

09:06.640 --> 09:15.930
information we printed out last time which was the text to speech and then the syntactic dependency.

09:15.940 --> 09:21.790
So if you're on that you can see again Tesla it's a proper noun noun subject and then it's actually

09:21.790 --> 09:29.890
able to understand that is and an apostrophe t is actually a conjunction and it's going to be able to

09:29.890 --> 09:34.870
keep them separate as well as know the relationship between these two parts of these tokens.

09:34.870 --> 09:37.650
So it's really advance what space is doing here.

09:37.660 --> 09:40.560
So again notice that isn't it really.

09:40.630 --> 09:47.530
Actually it's been split to two tokens and spatially recognizes both the root verb is and the negation

09:47.620 --> 09:48.320
attached to it.

09:48.370 --> 09:54.960
So it understands is the root verb and that this is a negation this an apostrophe T.

09:55.060 --> 10:00.970
Notice also that both the extended whitespace and the period at the end of the sentence are assigned

10:00.970 --> 10:02.090
their own tokens.

10:02.110 --> 10:09.790
So if I were to put in a lot of extended whitespace here and run this again this space would actually

10:09.790 --> 10:11.760
become a token with Speccy.

10:11.800 --> 10:14.080
And then the stop is a punctuation.

10:14.080 --> 10:19.410
Now I want to point out that here we're iterating for every token inside of this document object.

10:19.600 --> 10:24.130
But we can also use indexing taxi grab tokens individually.

10:24.190 --> 10:30.130
So if I take my document object I can use indexing to grab the very first token off of this and by default

10:30.160 --> 10:31.740
it returns that token text.

10:31.840 --> 10:38.440
But just as before or offer that token object I can ask for attributes like part of speech and it returns

10:38.440 --> 10:40.840
back prop and which stands for proper noun.

10:40.990 --> 10:46.930
And if you Cheka are spacey BASIX notebook and scroll to the section we're at right now which is learning

10:46.930 --> 10:50.570
about parts of speech tacking dependencies.

10:50.620 --> 10:55.960
Additional token attributes and so on we have a couple of useful links here for understanding the different

10:56.020 --> 10:56.890
notations.

10:56.890 --> 11:04.840
So here we saw that there's a part of speech like Propp and for proper noun verb space noun and so on.

11:05.020 --> 11:09.390
You can check out the link in the note book that we provide here for annotations specific.

11:09.610 --> 11:12.700
And there's also a variety of languages here English German et cetera.

11:13.060 --> 11:19.840
You can just expand English I can tell you the part of speech the morphology and description so you

11:19.840 --> 11:23.370
can take a look at all of this to see what all of this stands for.

11:23.380 --> 11:28.330
So if you see pro and that stands for a pronoun personal and then so on.

11:28.330 --> 11:32.700
You can keep going on in more detail and it does this for a wide variety of languages.

11:33.130 --> 11:37.660
So there's Parsa speech tagging that's available to us and we'll cover parts of speech in a lot more

11:37.660 --> 11:39.430
detail later on on its own.

11:39.430 --> 11:46.490
There's also syntactic dependencies for each token and that as I mention is just going to be the call.

11:46.510 --> 11:53.320
For example if we grab the first item here we can say the IP underscore this is the syntactic dependency.

11:53.370 --> 11:58.750
Again there's a link inside the Speccy notebooks you can go here dependencies get a full list of syntactic

11:58.750 --> 11:59.860
dependencies.

11:59.860 --> 12:06.010
So right now those are only available for English and German so you can expand English and see the label

12:06.280 --> 12:07.950
as well as description.

12:07.960 --> 12:13.390
We've also provided in Sodor Spacey notebook a good explanation of what these dependencies actually

12:13.390 --> 12:14.620
stand for.

12:14.620 --> 12:18.670
Again this has more to do if a really good understanding of the way the English language works which

12:18.670 --> 12:19.980
can be really complicated.

12:20.000 --> 12:24.880
But this isn't really an English course so we're not good dive too much into the details but this link

12:24.880 --> 12:29.740
here that we provide for you is the full paper explaining the various dependencies and what they stand

12:29.740 --> 12:37.300
for things like a causal subject a dependent a causal passive subject and so on this sort of level of

12:37.300 --> 12:41.830
detail is sometimes really important if you're trying to build a chat bot that can understand things

12:41.830 --> 12:44.590
like present versus past tense and so on.

12:44.650 --> 12:49.500
Keep mindspace basically doing this all this work for us under the hood.

12:49.580 --> 12:54.650
So we'll come back to our untitled notebook and the last thing I want to mention is that there's lots

12:54.650 --> 12:56.810
of other additional token attributes.

12:56.810 --> 13:01.020
So far we've seen parts of speech dependencies and a couple of more.

13:01.310 --> 13:06.680
If you come back to the Speccy basics notebook and scroll down here we have an entire list of the different

13:06.680 --> 13:08.680
tags and description.

13:08.990 --> 13:13.970
So for example if we call it that text that's is the original word text which in this case would be

13:13.970 --> 13:14.780
Tesla.

13:15.080 --> 13:20.660
If you want the lemma that's going to give you the limitation or the base form of the word and later

13:20.660 --> 13:23.200
on we're going to talk about limitation and a lot more detail.

13:23.330 --> 13:26.460
Lotusphere that essentially the result is just lower case thingness.

13:26.760 --> 13:32.840
There's the part of speech a simple speech tag if you tag underscore that actually gives you a detailed

13:32.840 --> 13:33.970
part of speech tag.

13:34.190 --> 13:39.500
So beyond just proper noun they'll tell you proper singular so understands that Tesla is actually a

13:39.740 --> 13:42.860
singular object then you can ask for the shape.

13:42.860 --> 13:46.130
So things like capitalization punctuation digits.

13:46.130 --> 13:48.640
So the way it shows that is it shows that capital X..

13:48.680 --> 13:53.540
So in the content the first letter is capitalized and then lowercase X for everything else being lowercase

13:53.770 --> 13:56.600
and you can also ask is Alpha or is stop.

13:56.600 --> 14:01.800
So is Alpha answer the question is the token alpha character so is alphanumeric.

14:01.840 --> 14:07.250
It's actually just an alphabetical character excuse me and that is stop asks Who is the token part of

14:07.250 --> 14:08.010
the stop list.

14:08.030 --> 14:11.350
So one of the most common words of whatever language you're working with.

14:11.400 --> 14:14.660
So Tesla is definitely not one of the most common words in English language.

14:14.810 --> 14:15.900
So it's false.

14:15.920 --> 14:19.380
So you have all these variety of tags you can call off of any of these tokens.

14:19.550 --> 14:25.440
And Spacey did this all automatically for us the very minute that he passed it into this an LP.

14:25.480 --> 14:30.200
And again it's doing this all based off the fact that we loaded in this library which is why it took

14:30.200 --> 14:32.880
some time but that's what makes Spacey so efficient.

14:32.900 --> 14:41.680
It loads up the library first for us now large stock objects can be hard to work if sometimes a span

14:41.770 --> 14:47.330
is a slice of a dark object in the form Some start versus a stop.

14:47.410 --> 14:50.980
So I'm going to copy and paste because it's quite a large string.

14:50.980 --> 15:01.320
If we scroll down here to span's and I'm going to copy paste this cell that way we don't need to write

15:01.320 --> 15:02.570
it all again.

15:02.730 --> 15:05.390
And essentially we're just passing in a really long string.

15:05.400 --> 15:12.380
So although comly actually to John Lennon from his song beautiful boy and so on so I have this really

15:12.380 --> 15:20.900
large documents and what I may want to do is actually just grab a span of it so I can say life quote

15:22.600 --> 15:24.960
so notice here there's a quote here.

15:24.960 --> 15:30.120
Life is what happens to us while we are busy making other plans and we're going to say is life quote

15:30.180 --> 15:39.990
is equal to Doc 3 and starting at index 16 up to but not including index 30 and then I can print out

15:41.120 --> 15:46.050
the life quote and it gives me this quote Life is what happens.

15:46.050 --> 15:51.250
That's why we are making other plans so this is now a span of that overall document because it's documents

15:51.260 --> 15:51.870
quite large.

15:51.890 --> 15:56.280
And maybe we're only interested in this particular quote inside of that document.

15:56.330 --> 16:03.260
So what's really interesting is that even though we're only grabbing a section of this document Spacey

16:03.260 --> 16:06.800
is smart enough to know that this life is a spam.

16:06.860 --> 16:13.580
So if you check out the type of this life quote space is doing a lot of work under the hood to understand

16:13.580 --> 16:21.510
that this is a particular span unlike the entire document which we checked type of doc three understands

16:21.510 --> 16:21.630
it.

16:21.640 --> 16:23.230
That's the entire document.

16:23.270 --> 16:27.320
So when you take a slice out of this space it's going to be smart enough to understand that it's the

16:27.320 --> 16:34.700
span of a larger document and then certain tokens inside a document or Doc object may also receive a

16:34.700 --> 16:36.800
start a sentence tag.

16:36.800 --> 16:42.110
While this doesn't immediately build the list of sentences these tags enable the generation of sentence

16:42.110 --> 16:44.480
segments through doc thought.

16:44.530 --> 16:47.890
E.A. Yes and later we're going to write our own segmentation rules.

16:47.990 --> 16:49.740
But let me just show you a simple example.

16:51.450 --> 17:00.090
Well say talk for is equal to an LP create a unicode string and say this is the first sentence.

17:00.120 --> 17:01.920
Period space.

17:02.000 --> 17:04.000
This is an other sentence.

17:06.550 --> 17:07.600
Period space.

17:07.600 --> 17:10.940
This is the last sentence.

17:12.090 --> 17:16.410
So here we have three sentences inside her documents.

17:16.860 --> 17:19.500
And again Spacey doing a lot of work for us.

17:19.560 --> 17:25.050
It actually understands and is going to separate each of these sentences throughout the documents so

17:25.050 --> 17:36.270
I can say for sentence in doc for and if I just sit duck for those to go through every token but I can

17:36.270 --> 17:46.260
call the SE A.S. or sense attribute in order to print out each particular sentence so spacey automatically

17:46.260 --> 17:54.950
understands that this period and the space is a sentence and then I can say something like Doc for at

17:56.090 --> 17:57.950
index position 6.

17:58.430 --> 18:00.840
So that's this right here.

18:00.940 --> 18:01.800
That's this word right here.

18:01.810 --> 18:03.500
This room or these are tokens.

18:03.620 --> 18:06.670
So we have various tokens here I can call.

18:06.680 --> 18:10.540
Thought is underscore sent underscore.

18:10.550 --> 18:14.050
Start asking Is this the start of a sentence.

18:14.210 --> 18:16.120
If you're in that will return true.

18:16.850 --> 18:21.100
If I check it out for another token it's good to check it out for tokens 7.

18:21.140 --> 18:24.030
That's is this is right here.

18:24.160 --> 18:27.270
Make it more obvious by token 8 the word another.

18:27.440 --> 18:32.660
I'm going to ask K is this word another the start of a sentence.

18:32.660 --> 18:37.040
And if you run this it's not going to return anything it's going to return none because it is not the

18:37.040 --> 18:42.180
start of a sentence unlike when he passed and six it returns back true.

18:42.230 --> 18:48.110
So it's really incredible the capabilities a Speccy to take in a raw string and completely understand

18:48.110 --> 18:50.000
things like parts of speech.

18:50.030 --> 18:56.060
Name recognition token attributes where the sentence starts and ends and a lot more.

18:56.120 --> 18:58.240
So we just touch the tip of the iceberg.

18:58.400 --> 19:03.150
But I really hope this gets you excited about the potential capabilities of spacey.

19:03.230 --> 19:07.840
Coming up next we're going to dive in and a lot more detail to each of these particular components.

19:07.850 --> 19:08.810
I'll see you at the next lecture.

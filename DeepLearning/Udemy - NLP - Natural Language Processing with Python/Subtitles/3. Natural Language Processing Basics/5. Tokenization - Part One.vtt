WEBVTT

00:05.360 --> 00:11.780
Welcome to this lecture on tokenization tokenization is the process of breaking up the original raw

00:11.780 --> 00:15.580
text into component pieces or otherwise known as tokens.

00:15.590 --> 00:21.190
This was the very first step that occurred when we created our document objects using the pipeline was

00:21.210 --> 00:23.780
spacey.

00:23.890 --> 00:29.350
You can imagine that we have some original text and the very first step is to split on whitespace.

00:29.350 --> 00:35.680
So from where moving to L.A. We split on the white space then Spacey is smart enough to understand that

00:35.680 --> 00:39.250
it's going to separate prefix that is characters at the beginning.

00:39.460 --> 00:41.980
It's also going to two separate exceptions.

00:42.070 --> 00:46.620
Those are concatenations and it's also going to be able to separate a suffix.

00:46.630 --> 00:48.930
So those are characters towards the end.

00:49.240 --> 00:55.450
And then we can also realize that we have exceptions such as an exclamation point and then we're done.

00:55.450 --> 01:00.410
So there's lots of steps involved but luckily Spacey is taking care of all of that for us.

01:00.430 --> 01:05.440
We have prefix suffix infix and exceptions.

01:05.440 --> 01:08.760
So again notice that the tokens are just pieces of the original text.

01:08.800 --> 01:13.280
We don't see any conversion to words stems or lemmas the base forms of words.

01:13.390 --> 01:18.100
And we haven't seen anything yet about organizations places money etc. that's going to be named and

01:18.190 --> 01:23.890
the recognition we we're going to talk about later on for now you should understand that tokens are

01:23.890 --> 01:28.690
the basic building blocks of a dark object everything to help us understand the meaning of the text

01:28.990 --> 01:32.440
is derived from tokens and the relationship to one another.

01:34.290 --> 01:38.730
As I mentioned we had the prefix those were the characters at the beginning.

01:38.730 --> 01:41.910
We had the suffix characters at the end.

01:41.960 --> 01:47.660
We have infix which are the characters in between the prefix and the suffix and then we have the exception

01:47.750 --> 01:53.000
which is a special case rule to split a string into several tokens or prevent a token from being split

01:53.270 --> 01:55.190
when punctuation rules are applied.

01:56.710 --> 01:58.860
So we can see a few examples here such as.

01:58.900 --> 02:03.120
A prefix is a dollar sign prefixing an amount of money.

02:03.490 --> 02:10.880
A suffix can be something like K.M. suffix seeing at the end the kilometers travelled in unfix is just

02:10.880 --> 02:17.900
something between like a dash or a forward slash and the exceptions can be basically things are a special

02:17.900 --> 02:23.210
case rules to make sure when you're doing the split as the token you understand that it's actually part

02:23.210 --> 02:28.500
of a joint punctuation like you dot dot.

02:28.700 --> 02:31.490
Now tokens have a variety of useful attributes and methods.

02:31.580 --> 02:34.060
We're going to explore tokens of Spacey in further detail.

02:34.070 --> 02:35.770
But jumping over to Jupiter notebook.

02:35.900 --> 02:37.300
Let's get started.

02:37.370 --> 02:44.510
The first two are going to do in our note book is import Spacey and then load up our Spacey library

02:45.580 --> 02:56.940
will say Spacey load and then call it an underscore co core underscore Wehbe underscore S-M that load's

02:56.940 --> 02:58.340
or spacey library.

02:58.620 --> 03:03.940
Next let's go and create a string that includes opening and closing quotation marks.

03:04.230 --> 03:15.870
So say my string is equal to will have single strings on the outside and we're going to say where moving

03:16.530 --> 03:30.450
to a exclamation point and then we're actually going to have this in its own set of quotes.

03:30.480 --> 03:33.990
So those are the quotes I'm using on the very outside.

03:33.990 --> 03:39.450
I have this single set of quotes and then the reason I'm using a backslash here is the so that the single

03:39.450 --> 03:44.970
quote in between we and our ear isn't actually going to break up the string and then we have these double

03:44.970 --> 03:53.060
quotes which are going to be intact so if I took a look at my string I see we're moving to L.A. But

03:53.060 --> 03:58.500
if I actually print this out I will see we're moving to L.A..

03:58.880 --> 04:03.710
So again just including some information in there such as the double quotes and then using this backslash

04:03.740 --> 04:09.120
as an escape character for the single quote in order to not stop the string too early.

04:10.310 --> 04:20.650
Let's go ahead and say Doc is equal to an LP and will pass in my string and then for a token and doc.

04:20.920 --> 04:30.810
Let's go ahead and print the text of the token and we can see here we were able to grab the double quotes.

04:30.810 --> 04:35.150
We're able to separate we an apostrophe are we moving to.

04:35.280 --> 04:39.110
We understood the LS should be kept together and then we have an exclamation mark.

04:39.570 --> 04:45.400
So Spacey is going to isolate punctuation that does not form an integral part of a word.

04:45.450 --> 04:50.010
Things like quotation marks commas and punctuation at the end of the sentence will be assigned their

04:50.010 --> 04:50.930
own token.

04:51.150 --> 04:57.390
However punctuation that exists as part of the email address website or numerical value will be kept

04:57.450 --> 04:58.710
as part of that token.

04:58.710 --> 05:00.560
Let me show you an example.

05:00.730 --> 05:06.390
Let's create another string and pass it into a dock.

05:06.860 --> 05:18.050
We're going to create a unicode string that says something like We're here to help send snail mail email

05:18.100 --> 05:25.260
support at our site dot com or visit us at each.

05:25.270 --> 05:33.430
And it's going to make up a Web site here that E.W. our site dot com exclamation mark.

05:33.950 --> 05:38.790
So we run that and run if we can see this is actually a really complex string to analyze.

05:38.810 --> 05:42.090
We have a lot of punctuation here such as these exclamation points.

05:42.100 --> 05:45.080
This dash or hyphens separating snail mail.

05:45.230 --> 05:48.350
We have dots here which isn't really of a sentence.

05:48.360 --> 05:52.970
Instead it's part of this email address and then we also have these dots right here which are part of

05:52.970 --> 05:54.060
a Web site.

05:54.080 --> 05:56.320
Spacey is smart enough to understand that.

05:56.360 --> 06:08.010
So if we see 40 in doc to print see here we can see that the Web site and email are intact.

06:08.050 --> 06:12.850
Space is smart enough to understand that this particular period isn't going to end the sentence.

06:12.870 --> 06:18.210
What is interesting though is that the hyphen in between snail and mail is actually assigned its own

06:18.210 --> 06:24.370
token and spacially has its own internal rules for deciding what kind of token gets assigned where.

06:24.420 --> 06:25.940
Let's create another document.

06:26.030 --> 06:39.860
Well say Doc three is equal to an LP and here we're going to pass in a five kilometer NYC cab ride cab

06:39.860 --> 06:47.050
ride it costs $10 and 30 cents.

06:47.050 --> 06:51.750
Run that and then we'll save 40 in Doc 3.

06:51.880 --> 06:54.230
Go ahead and print t.

06:54.790 --> 07:00.390
And he Rissi a five K.M. being separated from five NYC cab ride costs.

07:00.390 --> 07:03.830
Dollar Sign and again it's smart enough to keep ten thirty.

07:03.840 --> 07:04.720
Together.

07:04.930 --> 07:08.720
So here that distance unit and the dollar sign are assigned their own tokens.

07:08.770 --> 07:13.820
Yet the dollar amount itself here and $10 30 cents is preserved.

07:14.800 --> 07:21.340
Punctuation that exists as part of a known creation will also be kept as part of the token.

07:21.340 --> 07:33.730
So for example will create no document and LP and will say something like let's visit St. Louis in the

07:34.390 --> 07:37.210
US next year.

07:37.210 --> 07:37.970
We run that.

07:38.100 --> 07:40.310
Well check out the tokens that spacey figured out.

07:42.210 --> 07:44.610
And I'll say let's visit.

07:44.620 --> 07:50.460
Notice it keeps St together Louis and it keeps us together again.

07:50.470 --> 07:53.260
Everything is happening under the hood for your convenience.

07:53.290 --> 07:56.150
You don't actually have to tell Spacey what rules to expect.

07:56.170 --> 07:59.160
It's smart enough to understand that on its own.

07:59.230 --> 08:04.360
Now if we ever want to count the number of tokens all we need to do is check the length of a document

08:04.390 --> 08:05.040
object.

08:05.040 --> 08:07.660
So if I want to know how many tokens doc 4 has.

08:07.660 --> 08:11.400
I just checked the length and it reports back that it has 11.

08:11.410 --> 08:20.590
Now we can actually also count vocab entry vocab objects contain a fool library of items so off of a

08:20.590 --> 08:25.900
document object you can call the vocab.

08:26.230 --> 08:31.870
And it's this large vocabulary object and this number is going to change based on the language library

08:31.870 --> 08:34.240
loaded at the start.

08:34.270 --> 08:40.990
So if I check the length of Doc for that vocab notice right now it's fifty seven thousand eight hundred

08:41.110 --> 08:42.250
fifty two.

08:42.250 --> 08:51.250
That means that when we loaded up this English core web as M for small that has a vocabulary of fifty

08:51.250 --> 08:57.640
seven thousand eight hundred fifty two different types of tokens and there is an LG or a much larger

08:57.640 --> 09:00.820
language library that you can use for more complex tasks.

09:00.880 --> 09:05.990
But for most of our use cases this language library is going to service fine.

09:06.180 --> 09:10.980
Now tokens can also be retrieved by index position and slice and we've actually shown this before.

09:11.030 --> 09:11.990
But let's reiterate.

09:12.090 --> 09:23.780
It will say duck 5 is equal to an LP pass and a unicode string that says it is better to give than receive

09:26.420 --> 09:30.140
and if I have docked 5 here it can simply index.

09:30.730 --> 09:38.650
And here I can grab one single token or I can grab a slice of tokens maybe 5 say give me everything

09:38.650 --> 09:45.830
from position 2 to 5 and I can see better to give and we kind of have our own little span there.

09:45.870 --> 09:52.080
Keep in mind tokens cannot be reassigned although document objects can be considered lists of tokens.

09:52.080 --> 09:54.470
They do not support item re-assignment.

09:54.630 --> 09:57.150
So once you created that document it's going to be fixed.

09:57.150 --> 10:03.340
You can't do something like re-assign something like that it's going to say object does not support

10:03.340 --> 10:03.790
assignment.

10:03.790 --> 10:07.960
So this document object you're not allowed to do this sort of reassignment which makes sense there's

10:07.960 --> 10:11.010
a lot of information here that spacey figured out from these tokens.

10:11.200 --> 10:14.970
It shouldn't make sense to be able to simply replace it for a simple python string.

10:16.950 --> 10:24.300
Now Spacey can actually go a step beyond simple tokens it can actually understand named entities named

10:24.360 --> 10:24.890
entities.

10:24.900 --> 10:27.000
Add another layer of context.

10:27.120 --> 10:32.880
The language model that you load in at the very top recognizes that certain words or organization names

10:32.970 --> 10:39.450
while others are locations and still other combinations relate to things like money or dates named entities

10:39.480 --> 10:43.970
are accessible through the E and T S property of a document object.

10:43.980 --> 10:52.300
I want to create a new document to show this will say Doc let's say eight is equal to an LP and we'll

10:52.300 --> 11:05.860
say the following string Apple to build a Hong Kong factory for 6 million dollars want to just say six

11:05.860 --> 11:08.150
million dollars.

11:08.260 --> 11:11.920
So we run this right now and we already know that we have token information.

11:11.920 --> 11:21.850
We can say for tokin and Doc date print out the token text and in fact I can print it all out along

11:21.850 --> 11:26.890
the same line by saying and is equal to some sort of separator.

11:26.890 --> 11:28.740
So I can do that to print out.

11:28.750 --> 11:30.460
Let's add a little space there.

11:31.030 --> 11:33.010
So here I can see all the various tokens.

11:33.070 --> 11:38.270
Apple to build a Hong Kong factory for the dollar sign than six and then million.

11:38.560 --> 11:40.620
But here's what's really cool.

11:40.960 --> 11:52.240
I can go ahead and say for entity in dock 8 but instead of now going through dock I'm going to say dock

11:52.270 --> 12:00.800
eight dot and Ts and we're not going to do is say print the entity.

12:01.220 --> 12:07.190
So off of this string space is smart enough to figure out that there's something special about Apple.

12:07.340 --> 12:13.130
Hong Kong and six million dollars and is able to figure this out without you telling it more information.

12:13.130 --> 12:19.430
It recognized that Apple Hong Kong and six million dollars are named entities kind of similar to proper

12:19.430 --> 12:20.600
nouns in a sense.

12:20.650 --> 12:24.860
Basically there's more context to these particular words.

12:24.860 --> 12:29.850
And in fact it's smart enough to know the falling off of each of these entities.

12:30.020 --> 12:36.650
We can actually print out a label so you can say label underscore and then we're going to print a new

12:36.650 --> 12:38.330
line here.

12:39.200 --> 12:40.890
And you'll notice that it has these labels.

12:40.880 --> 12:48.830
They'll say Apple is R.G. or organization or $6000 is money and you can check out the named entities

12:48.830 --> 12:55.040
list that we have inside of our notebook we have a little link here that will show you a table of what

12:55.040 --> 13:00.700
names and cities look like and what these actual codes stand for and then check out the link in our

13:00.730 --> 13:03.330
notebook for link the documentation.

13:03.350 --> 13:09.370
Now you can ask for more explanation on these labels directly by simply saying the following.

13:09.440 --> 13:14.540
Print the string representation of a Speccy.

13:14.730 --> 13:20.410
It has this really useful explain function and you can actually ask it to explain that label.

13:20.690 --> 13:26.480
So I know a little bit nested here but all I'm saying is explain that label pasand into a string and

13:26.480 --> 13:27.410
then print it out.

13:29.140 --> 13:29.700
Whoops.

13:30.050 --> 13:30.580
This should be.

13:30.600 --> 13:32.230
And to see.

13:33.120 --> 13:33.770
OK.

13:33.780 --> 13:38.850
So again I'm just going through each entity printing out the entity print printing out the label and

13:38.850 --> 13:41.800
then printing out the builtin explanation for that label.

13:41.820 --> 13:45.440
So orgy's stands for companies agencies institutions.

13:45.570 --> 13:52.110
GP is for countries cities and states 6 million are recognized as money which is monetary values including

13:52.110 --> 13:52.690
unit.

13:53.010 --> 13:55.710
So all of this is what space is doing for you.

13:55.710 --> 14:00.060
Lots of awesome work though we don't really have to be aware of.

14:00.190 --> 14:07.320
And the last thing I want to mention here is that Doc E.A. s or entities similar to that we have now

14:07.320 --> 14:11.340
and Chunk's which are another object property nown chunks.

14:11.410 --> 14:17.120
Our base noun phrases essentially flat phrases that have a noun as their head.

14:17.320 --> 14:22.210
So you can think of noun chunks as noun plus the words describing a particular noun.

14:22.210 --> 14:23.710
Let me show you what I mean by that.

14:23.990 --> 14:32.850
We're going to say Doc nine is equal to an LP and we'll create a sentence here called Autonomy's cars

14:36.530 --> 14:43.820
shift insurance liability toward manufacturers.

14:44.120 --> 14:44.540
OK.

14:44.540 --> 14:49.820
So kind of a complex sentence here and we have various chunks the chunks are going to be autonomous

14:49.820 --> 14:53.700
cars because we have a known cars and this word describing that.

14:53.830 --> 15:00.770
Now we also have insurance liability as another chunk and then manufactures itself is a now chunk is

15:00.800 --> 15:01.420
just a noun.

15:01.460 --> 15:03.860
But it doesn't really have the Scripter here.

15:04.010 --> 15:13.730
So again I can say for chunk in and I'll say Doc nine and I can call noun chunks off of this and then

15:13.730 --> 15:21.030
actually print out each chunk autonomous cars insurance liability and manufacturers.

15:21.300 --> 15:26.880
And I would encourage you to again check out the link that we provide in the notebook for a list of

15:26.880 --> 15:29.720
all the different types of noun chunks that are available.

15:29.760 --> 15:34.920
Coming up next for part two we're going to talk about visualizing with builtin visualizes that's basically

15:34.920 --> 15:36.120
also provides.

15:36.150 --> 15:37.170
We'll see you at the next lecture.

WEBVTT

00:05.300 --> 00:06.240
Welcome back everyone.

00:06.270 --> 00:11.090
Or you can continue right where we left off last time by showing you how to use T.F. idea with sikat

00:11.090 --> 00:11.600
learn.

00:11.870 --> 00:14.510
Let's head back to the notebook in the previous lecture.

00:14.510 --> 00:20.060
We went ahead and performed count vectorization as we discussed during the theory lecture.

00:20.060 --> 00:25.910
We can actually use T.F. idea or term frequency inversed document frequency in order to actually give

00:25.910 --> 00:32.720
words that are more important more weight one way to do this is by simply passing in our count vectorization

00:33.140 --> 00:35.490
into ATF IDF transformer.

00:35.650 --> 00:45.980
We can say from S-K learn that feature extraction thought text import T.F. IDF transformer.

00:45.980 --> 00:49.240
We'll talk about TAF IDF vectorize or object and just a second.

00:50.510 --> 00:59.240
First let's create an instance of the T.F. idea of transformer will say T.F. IDF transformer is equal

00:59.240 --> 01:08.690
to and here we'll call an instance of it and then we can simply take X train underscore T.F. IDF and

01:08.690 --> 01:18.010
set equal to the T.F. IDF transformer and then call transform on X train counts the result of the count

01:18.020 --> 01:19.310
vectorization.

01:19.310 --> 01:24.970
Now don't worry if you see a future warning show up that excess do of sikat learn and they will update

01:25.030 --> 01:27.080
and fix that in a future release.

01:27.170 --> 01:28.300
So you don't need to worry.

01:28.310 --> 01:31.150
It still works even if you get this warning.

01:31.160 --> 01:34.540
Next we're going to do is just quickly check the shape of this object.

01:34.610 --> 01:36.600
The extraneous idea.

01:37.750 --> 01:39.020
And here we can check the shape.

01:39.020 --> 01:42.350
It's still the same shape but it's no longer just counts.

01:42.350 --> 01:47.690
Instead we've taken in the term frequency and multiplied it by its inverse document frequency.

01:47.840 --> 01:55.340
Now because it's so common to perform a count vectorization followed by T.F. IDF transformation that

01:55.400 --> 02:02.090
sikat learn provides a T.F. IDF vectorization and that actually combines the two previous steps of Count

02:02.090 --> 02:06.510
vectorization A.F. IDF transformation into one single step.

02:06.890 --> 02:18.480
So we can do in the future is just say from S-K learn the feature extraction text import T.F. IDF victimiser

02:19.220 --> 02:22.360
and then you can create an instance of this vector laser.

02:22.390 --> 02:32.540
I was going to call vectorize or T If IDF victimiser and then we can say X train t If IDF is equal to

02:33.470 --> 02:38.840
and we call her vectorize her and then we can call that transform directly on X train.

02:38.840 --> 02:45.440
Remember this is our original x training data set and this essentially combines the process of both

02:45.470 --> 02:49.040
count vectorization and T.F. idea of transformation.

02:49.250 --> 02:52.130
So for convenience there's this T.F. idea of vectorize.

02:52.520 --> 02:55.070
So we can run that and we would get back the same result.

02:55.070 --> 02:58.520
Notice again I get a warning here at the Pentagon what version you are using.

02:58.550 --> 02:59.990
You may or may not get a warning.

02:59.990 --> 03:02.400
It's OK if it's just the warning.

03:02.420 --> 03:11.540
Next we're going to do is we're going to train a classifier So we will say from S-K learn SVM import

03:12.320 --> 03:19.250
and we'll be importing a linear support vector classifier will create an instance by saying CNF is equal

03:19.250 --> 03:30.830
to linear s v c and then we will say our classifier and we're going to fit it to the vectorized training

03:30.830 --> 03:31.340
data.

03:31.580 --> 03:38.000
So I'll say X underscore train t f f and then we passen the corresponding y.

03:38.000 --> 03:43.460
Training labels you run this and I'll report back the model that was used.

03:43.530 --> 03:49.710
Now remember only our training set has been vectorize into a full vocabulary in order to perform an

03:49.710 --> 03:51.520
analysis on our test set.

03:51.540 --> 03:54.990
We would actually have to then repeat all the same procedures.

03:54.990 --> 03:59.770
Now that can actually get a bit tiresome especially if you have a very long process.

03:59.880 --> 04:06.040
What sikat learn provides is a pipeline class that essentially behaves like a compound classifier.

04:06.090 --> 04:10.140
It can perform both the vectorization and the classification.

04:10.140 --> 04:16.740
So instead of having to do this sort of thing transform and count vectorization on your test data in

04:16.740 --> 04:22.830
order to predict what we can do is actually combine everything we just saw into one single pipelined

04:22.830 --> 04:32.370
step so the way we do this is we say from as Kaylor that pipeline import and we import this pipeline

04:32.430 --> 04:37.020
object and then we say text or whatever variable you want.

04:37.020 --> 04:42.750
We're going to call this my text classifier and you create an instance of the pipeline object and the

04:42.750 --> 04:47.460
pipeline object is going to take a tuple or a list of tuples excuse me.

04:47.460 --> 04:52.300
So you're going to pass in a list of tuples each tuple.

04:52.350 --> 04:57.060
What it's going to have is going to have a string name that either side or what to call this step in

04:57.060 --> 04:58.130
the pipeline.

04:58.170 --> 05:02.580
So I'm going to say T.F. IDF is my first step of the pipeline.

05:02.790 --> 05:04.890
And then you actually call what you want to occur.

05:04.920 --> 05:10.640
So I can pass in my vectorize or object there as the first item in my tuple.

05:10.650 --> 05:20.550
Then I can passen in other tuple pair so I can say actually perform classification using a linear as

05:20.550 --> 05:25.770
we see those means of open and close parentheses here and there.

05:25.790 --> 05:28.340
We can actually create our pipeline.

05:28.340 --> 05:35.000
So this pipeline object behaves just like a normal classifier of sikat learn our normal machine learning

05:35.000 --> 05:35.570
model.

05:35.810 --> 05:41.840
However what's convenient about this pipeline object is that it actually can perform all these steps

05:41.840 --> 05:48.050
for you in a single call then that means you can directly provide the original extraneous data and have

05:48.050 --> 05:52.780
it be both vectorized and run the classifier on it in a single step.

05:52.970 --> 05:57.080
So this is what you see often use especially in natural language processing where you can have a really

05:57.080 --> 06:04.090
long pipeline of things like text feature extraction or moving stop words tokenization limits ization

06:04.080 --> 06:05.160
et cetera.

06:05.240 --> 06:07.690
So the pipeline is a really convenient way to do this.

06:07.760 --> 06:09.590
Right now we have a very kind of short pipeline.

06:09.620 --> 06:14.580
It's just performing vectorization and the support or the support vector classifier you can imagine

06:14.580 --> 06:18.310
that we can provide a lot more steps inside this list of tuples.

06:18.320 --> 06:24.350
So the way to use the pipeline is exactly the same way you would using normal model you simply call

06:24.350 --> 06:27.410
your pipeline object and then say that fit.

06:27.680 --> 06:33.010
But now I can pass in my raw treating data X train and y train.

06:33.110 --> 06:35.640
And this is a lot more convenient than what we did before.

06:35.750 --> 06:42.230
Previously I had to call the vectorize or create an instance of the vectorize or do a fit transform

06:42.230 --> 06:48.560
on my original x train dataset then import the classifier create an instance of the classifier and then

06:48.560 --> 06:53.760
fit on the transformed or vectorize version of the ex-Marines dataset pipeline.

06:53.940 --> 06:55.180
Does all of this for us.

06:55.310 --> 06:59.790
It creates the pipeline and then you get to decide what steps to take.

06:59.870 --> 07:05.250
Do some vectorization first then perform the classification next step so that I can simply say that

07:05.270 --> 07:07.700
fit on the original x train.

07:07.730 --> 07:08.930
Why train data.

07:09.230 --> 07:14.930
So I run that fit here and again or if you get this warning you should eventually see something that

07:14.930 --> 07:17.640
says pipeline and then report back the steps it took.

07:17.840 --> 07:21.110
And then we can test the classifier and display results.

07:21.110 --> 07:27.830
So I can say my predictions is equal to and here I'm just going to call my pipeline object and then

07:27.860 --> 07:34.580
call predict just like I would with any other machine learning model and passing in the raw test data.

07:34.580 --> 07:38.740
So X test remember if we actually take a look at this.

07:38.750 --> 07:43.410
So for instance go below and take a look at what X test looks like.

07:43.420 --> 07:46.200
Remember these are the raw text messages.

07:46.370 --> 07:51.260
And what I'm doing is I'm going to pass in the raw text messages straight into this predictive function.

07:51.260 --> 08:00.170
So now I have these predictions and then I can say from S-K learn that metrics import and I can import

08:00.260 --> 08:07.270
a confusion matrix and a classification report and then see how well I performed.

08:07.460 --> 08:14.800
So I can say Prince the confusion matrix of why test versus my predictions

08:17.460 --> 08:22.710
and here I can see my confusion matrix Lotus's or performing much much better than our first attempt

08:22.710 --> 08:24.300
on this dataset.

08:24.600 --> 08:27.310
And then I can actually print out the classification report as well.

08:28.630 --> 08:33.550
So I can say why test predictions run that.

08:33.620 --> 08:36.070
And here I could see the classification report.

08:36.080 --> 08:38.380
So again notice what's happening here.

08:38.390 --> 08:44.540
I'm calling this all off of this text classifier which was this pipeline object for convenience and

08:44.540 --> 08:51.440
now we're able to see that we get a much better performing classifier when actually vectorize and taken

08:51.440 --> 08:56.450
the message to go ahead and compare these results for the confusion matrix and classification port to

08:56.450 --> 08:57.800
our previous results.

08:57.800 --> 09:00.100
We can also do is check out accuracy.

09:00.380 --> 09:11.420
So I can say from S-K learn import metrics and then I can call metrics accuracy score and then pasan.

09:11.510 --> 09:20.650
My lips are lower case why my tests and predictions and I can see I am achieving almost 90 percent accuracy.

09:20.900 --> 09:26.550
So if you ever want to predict on a new message you just use the same predict method here though he

09:26.550 --> 09:28.630
is on the X test data set.

09:29.120 --> 09:30.940
Let's quickly show you how you can do that.

09:31.070 --> 09:37.910
So I'm going to grab my classifier object text underscore C L F and let's have a predicate on a new

09:37.910 --> 09:40.820
message that I make up whether or not it's spam.

09:40.820 --> 09:45.220
So I'm going to say hi and pass this as a list.

09:45.530 --> 09:46.400
Just a single message.

09:46.400 --> 09:50.270
Hi how are you doing today.

09:51.470 --> 09:57.260
I'm going to run that and it's predicting that it's him that is a legitimate message which makes sense.

09:57.260 --> 09:59.460
This does look like a legitimate message string.

09:59.510 --> 10:01.170
Notice I'm passing it inside of a list.

10:01.460 --> 10:04.070
Let's try predicting on something that is more Spammy.

10:04.130 --> 10:07.150
So you can say congratulations

10:09.280 --> 10:14.760
you've been selected as a winner.

10:14.760 --> 10:20.720
Text 1 2 4 4 or whatever some sort of number.

10:20.910 --> 10:22.350
Congratulations.

10:22.350 --> 10:25.450
Free entry to contest.

10:25.500 --> 10:25.840
Let's see.

10:25.890 --> 10:26.990
Let's see if that's spammy enough.

10:27.000 --> 10:31.530
We run that and it looks like it does predict that is a spam text message.

10:31.530 --> 10:33.500
Keep mine depending on what string you pass.

10:33.510 --> 10:38.280
It may or may not predict that it's spam and spam is kind of harder to make a spam message.

10:38.280 --> 10:39.270
It looks bad enough.

10:39.510 --> 10:43.350
But if you pass in an oral message it should hopefully predict ham.

10:43.470 --> 10:43.970
OK.

10:44.160 --> 10:49.230
So we just learn how to actually build out a pipeline train a classifier display the results and predict

10:49.440 --> 10:55.000
on our X test dataset as well as predicting on single new messages coming up next.

10:55.020 --> 10:58.680
We're going to go through an entire code along Tech's classification project.

10:58.710 --> 10:59.330
We'll see if there.

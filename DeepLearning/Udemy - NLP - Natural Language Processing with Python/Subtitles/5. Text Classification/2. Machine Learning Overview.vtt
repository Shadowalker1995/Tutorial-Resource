WEBVTT

00:05.430 --> 00:09.040
Welcome back everyone to this lecture on an overview of machine learning.

00:10.630 --> 00:15.570
Before we dive into text classification let's work on understanding the general machine learning process

00:15.570 --> 00:16.770
we're going to be using.

00:17.050 --> 00:20.700
The specific case of machine learning we're going to be conducting in this particular section of the

00:20.710 --> 00:23.520
course is known as supervised learning.

00:23.530 --> 00:26.570
So we're going to be focusing on that supervised learning process.

00:27.290 --> 00:30.050
Because different students have different backgrounds and math.

00:30.050 --> 00:34.570
We're going to be keeping the mathematics behind the machine learning algorithms on the lighter side.

00:36.070 --> 00:40.740
A great textbook on General machine learning especially if you are interested in the mathematics is

00:40.750 --> 00:44.800
called an introduction to statistical learning by Gareth James's companion book.

00:45.040 --> 00:50.070
It's freely available online as a PDA and it's released for free by the authors.

00:50.200 --> 00:53.590
You can simply Google search the title of the book to find the link.

00:53.590 --> 00:58.360
So again just google search I guess L.R. or introduction to statistical learning and you'll be able

00:58.360 --> 01:00.390
to easily find this book for free online.

01:02.390 --> 01:07.580
So let's begin by answering the question what is machine learning machine learning as a method of data

01:07.580 --> 01:14.270
analysis that automates analytical model building using algorithms that it's really learned from data

01:14.270 --> 01:15.330
itself.

01:15.350 --> 01:20.570
Machine learning allows computers to find hidden insights without being explicitly programmed where

01:20.570 --> 01:21.420
to look.

01:21.440 --> 01:26.000
So the machine learning algorithm on its own is going to figure out what patterns and what features

01:26.000 --> 01:26.990
are important.

01:28.740 --> 01:30.840
Now what does machine learning actually used for.

01:30.840 --> 01:35.070
As you may already know it's used across a variety of tasks and industries.

01:35.220 --> 01:41.310
Anything from fraud detection to credit scoring loan applications or the things we're going to be focusing

01:41.310 --> 01:45.770
on things like tech sentiment analysis or e-mail spam filtering.

01:45.780 --> 01:50.510
So this is a small subset here of what machine learning applications can do for us.

01:52.080 --> 01:58.960
Most talk about supervised learning supervised learning algorithms are trained using labeled examples.

01:58.990 --> 01:59.720
That's a key word.

01:59.720 --> 02:07.610
They're labeled so a labeled example is an example where the input and desired output is known.

02:07.620 --> 02:14.610
For example if you have a data point as a segment a text it could have a categorical label such as the

02:14.610 --> 02:21.390
text of an email being labelled as spam versus legitimate or the text of a movie review being labeled

02:21.420 --> 02:25.040
as a positive movie review or a negative movie review.

02:26.880 --> 02:32.160
The learning algorithm or machine learning algorithm is going to receive a set of inputs along with

02:32.160 --> 02:37.650
the corresponding correct outputs and the algorithm is going to learn by comparing its actual output

02:37.920 --> 02:40.470
with the correct outputs in order to find an error.

02:40.590 --> 02:45.300
And then it's going to modify the internal parameters of the machine learning algorithm accordingly

02:45.600 --> 02:51.100
to achieve the best results on its own without you explicitly programming it what it should change.

02:52.760 --> 02:58.280
Supervised learning is commonly used applications where historical data points predict likely future

02:58.280 --> 03:04.550
events such as historical information of previous emails you've received that someone has labeled as

03:04.820 --> 03:07.830
legitimate or illegitimate or spam.

03:07.910 --> 03:13.650
The historical data can be used in order to provide a future spam filter for future emails.

03:15.360 --> 03:20.160
So let's talk about the supervised machine learning process and this is probably the most fundamental

03:20.160 --> 03:22.750
thing to get out of this particular lecture.

03:23.040 --> 03:25.890
So we're going to go through all these steps individually.

03:25.890 --> 03:28.800
Now the first step is to actually quiet your data.

03:28.800 --> 03:30.570
And there's many different ways to do this.

03:30.630 --> 03:33.450
In this particular course the acquisition is pretty simple.

03:33.450 --> 03:38.280
We actually just provide the data for you but depending on where you're actually working or what you're

03:38.280 --> 03:43.480
doing for your personal machine learning projects you can acquire that data through a variety of sources.

03:43.500 --> 03:48.240
You can check out the resource links in this lecture for lots of free data sources or if you're working

03:48.240 --> 03:53.160
on a company you can work internally to acquire data but your customers or even from like things like

03:53.160 --> 03:54.870
physical sensors et cetera.

03:54.930 --> 03:59.790
So we're going to assume then that you've acquired that data then the next step is to clean and format

03:59.790 --> 04:00.480
your data.

04:00.570 --> 04:05.490
And in particular if you're working with text data for natural language processing you'll have to perform

04:05.490 --> 04:11.000
things like vectorization on that data which are going to cover in the lecture text feature extraction.

04:11.310 --> 04:16.590
But you get to do things like remove missing data points from your data and then convert the raw text

04:16.770 --> 04:20.340
into numerical vectors that machine learning models can understand.

04:20.580 --> 04:24.410
So that's a whole process that we're really going to focus on on in another lecture on the future.

04:24.580 --> 04:27.920
But you've acquired that data and you've cleaned in a format of your data.

04:29.800 --> 04:34.990
Then once you have the clean and formatted data you're going to split the data into training data or

04:34.990 --> 04:40.410
each training set and then test data or a test set and know what you're going to do is on the training

04:40.410 --> 04:47.320
data you're going to grab your machine learning model and have it learn or train off that training data.

04:47.370 --> 04:50.730
And that's also known as fitting the model to the training data.

04:50.790 --> 04:55.070
So the machinery model is going to attempt to learn as best it can.

04:55.440 --> 04:59.820
The different application that you're looking for so if you're trying to build an email spam filter

04:59.940 --> 05:05.820
right now it's working on that training set of data and the training data is commonly about 70 percent

05:05.880 --> 05:07.380
of the total data.

05:07.380 --> 05:10.950
So your training your model and other model has learned on your training data.

05:10.980 --> 05:15.060
Now it's time to evaluate your model's performance and for evaluation.

05:15.060 --> 05:19.370
It makes sense to use data that the model has never seen before that you're not cheating.

05:19.530 --> 05:21.720
That's why we have the separate test data set.

05:21.840 --> 05:28.500
Around 30 percent of our data so we take the model we just trade on and then we test it against that

05:28.500 --> 05:33.870
test dataset and then we can evaluate how well the model actually performed using different evaluation

05:33.870 --> 05:34.490
metrics.

05:34.530 --> 05:38.350
And we're going to talk about evaluation metrics in a lecture in that section.

05:38.370 --> 05:44.400
So after evaluating our model we can decide to improve the model maybe edits some parameters of the

05:44.400 --> 05:50.040
original machine learning algorithm so we can go back to the model and training and building phase adjust

05:50.040 --> 05:55.800
some parameters and essentially repeat this process until we're satisfied for a modest performance and

05:55.800 --> 05:56.910
then we can deploy the model

05:59.600 --> 06:05.150
so for tech's classification and recognition this is really common and widely applicable use of machine

06:05.150 --> 06:05.740
learning.

06:06.110 --> 06:10.340
And later on in this section of the course we're going to be learning about the sikat learn library

06:10.460 --> 06:16.850
in order to use Python to conduct machine learning text classification.

06:16.850 --> 06:22.100
Now let's take a quick moment to focus on that idea of the train to split that I just mentioned and

06:22.100 --> 06:25.070
that we just saw her and we're going to learn a few key terms.

06:25.070 --> 06:31.730
So let's imagine a full data set of what is known as ham versus spam text messages and ham is just another

06:31.730 --> 06:35.080
word for legit or correct text messages.

06:35.240 --> 06:41.710
Basically the opposite of spam so we're going to imagine a full dataset and our dataset is going to

06:41.710 --> 06:42.790
look something like this.

06:42.790 --> 06:46.390
Notice that we have a label and the actual raw text message.

06:46.390 --> 06:51.160
So you can see the one in the middle there that's a spam text message offering some sort of bogus free

06:51.160 --> 06:52.950
entry and some weekly competition.

06:53.170 --> 06:56.980
The other text messages are him or legitimate text messages.

06:56.980 --> 07:02.560
So our idea here is that we're going to build a machine learning model that can detect given a new text

07:02.560 --> 07:07.200
message whether or not something is ham or spam.

07:07.310 --> 07:10.400
Now before the split we're going to have labels and features.

07:10.400 --> 07:15.540
And in this case the label is again the historical label ham or spam.

07:15.590 --> 07:22.310
So that means that someone in or someone manually had to go in and read the message and then label it

07:22.430 --> 07:25.250
as a legitimate message or spam message.

07:25.250 --> 07:31.140
So that's a key part of supervised learning is they have to have historical labeled data and often labeling

07:31.140 --> 07:33.260
your data as a manual process.

07:33.260 --> 07:37.510
Now in this course you're not going have to label any data we provide label data for you.

07:37.910 --> 07:44.330
So we have what's known as the y label and the X features features is the actual data points.

07:44.420 --> 07:46.970
They're going to use to predict the white label.

07:47.180 --> 07:52.730
And the reason we have y and x is because in linear algebra notation the way the mathematics work out

07:52.790 --> 07:56.630
usually have an X matrix of features and white label.

07:56.630 --> 07:58.770
And often you'll see why lower cased.

07:58.820 --> 08:00.870
So this is right before the split.

08:00.890 --> 08:04.760
We still have labels and features we haven't actually than a train test split yet.

08:05.000 --> 08:09.710
What we want to do is probably take about 70 percent of these points both the labels and the corresponding

08:09.710 --> 08:15.200
messages and save those for the training data and then take the other 30 percent and have that be our

08:15.200 --> 08:15.980
test data.

08:17.600 --> 08:25.240
So once we have our white labels next features before we fit the model we'll split the data so we're

08:25.230 --> 08:27.270
at take this data and we're going to split.

08:27.270 --> 08:31.260
So this is a really small example of a dataset just five points that we've already shown that we're

08:31.260 --> 08:32.050
splitting it.

08:32.160 --> 08:37.500
So you notice here a total data or splitting it into a test set and a training set.

08:37.500 --> 08:40.920
Remember that we still have a white label on X features.

08:40.920 --> 08:46.790
So we just split the data and we have now our test set in our training set.

08:48.040 --> 08:53.560
And now we can do is we can individually label the subsets of y s and x.

08:53.570 --> 08:58.880
So what you're going to see when you do the actual train test split of python and sikat learn is for

08:58.910 --> 08:59.710
outputs.

08:59.900 --> 09:05.930
You have your y labels your y tests in y train and then you have your X features your X features for

09:05.930 --> 09:08.890
the test set and your X features for the training set.

09:09.080 --> 09:15.860
So don't be confused when you run the train to split and see for outputs why test X test Y train extreme.

09:16.070 --> 09:19.580
All it is is the data set being split into two pieces.

09:19.610 --> 09:26.340
Your tests and your training set and then those two pieces being split along the y labels and X features.

09:26.390 --> 09:30.620
So that's all that is those four terms mean.

09:30.660 --> 09:34.170
So again we have those four components and you're going to see this components often throughout the

09:34.170 --> 09:36.820
course.

09:36.960 --> 09:41.220
So these four components are simply the result of the train to split groups being separated between

09:41.220 --> 09:42.840
features and labels.

09:42.840 --> 09:48.090
Coming up next we're to continue to understand the classification process in more detail and importantly

09:48.390 --> 09:51.860
metrics in order to evaluate a classification process.

09:51.870 --> 09:52.980
We'll see you at the next lecture.

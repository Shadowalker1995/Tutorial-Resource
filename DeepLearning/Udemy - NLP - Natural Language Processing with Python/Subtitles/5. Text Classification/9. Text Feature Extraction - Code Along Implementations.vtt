WEBVTT

00:05.300 --> 00:09.880
Welcome back everyone to feature extraction from text in this lecture.

00:09.890 --> 00:14.660
We're going to be learning about a basic manual implementation of building out a vocabulary just to

00:14.660 --> 00:19.690
give you a little bit of more intuition behind building out a vocabulary across multiple documents.

00:19.850 --> 00:25.130
Then we'll learn how to use the sikat learn library for Victoria Station as well as how to build pipelines

00:25.130 --> 00:27.640
for natural language processing with sikat Hitler.

00:27.740 --> 00:29.770
Let's open up the notebook and get started.

00:29.780 --> 00:30.350
All right.

00:30.400 --> 00:35.600
If you check out the text classification folder there is a notebook labeled feature extraction from

00:35.600 --> 00:41.050
text and if you open up that feature extraction from text notebook it's divided into two sections.

00:41.210 --> 00:46.400
The first section is basically showing you how to build out a natural language processor from scratch

00:46.640 --> 00:53.510
particularly how to build out a vocabulary giving each word a unique ID will briefly walk through that.

00:53.540 --> 00:57.810
We're not actually going to live code anything there and then we'll actually show you how I perform

00:57.800 --> 01:00.940
these steps using the real psychic learned tools.

01:00.960 --> 01:05.910
So just to give you a little bit more intuition of what's happening at the very base level behind every

01:05.910 --> 01:10.650
natural language processor is the fact they need to build out a vocabulary.

01:10.810 --> 01:15.400
So we're going to start off with some documents and keep my you don't really need to memorize this code.

01:15.400 --> 01:17.540
You never actually implement this manually in real life.

01:17.630 --> 01:23.530
Again just to build out an intuition here we're going to write two text files and keep mine.

01:23.530 --> 01:28.390
These two text files if you're not using Jupiter they're available right here as to x files inside the

01:28.390 --> 01:30.060
text classification folder.

01:30.770 --> 01:32.700
And then we can build out a vocabulary.

01:32.900 --> 01:37.970
So this is essentially the first step in natural language processing regardless of what method you're

01:37.970 --> 01:39.020
going to be using.

01:39.020 --> 01:45.800
You do have to assign some sort of vocabulary across all your documents and the way this works is we're

01:45.800 --> 01:53.270
going to build out a dictionary and then set an ID counter then we're going to open each text file and

01:53.270 --> 01:59.300
we're going to read in lowercase sing all the words so that we have a list of all the words then for

01:59.300 --> 02:05.070
every word in that list of words if that word is already in the vocabulary dictionary will just continue

02:05.630 --> 02:08.590
if that word is not present a cross over cavalry.

02:08.690 --> 02:14.380
We'll go ahead and assign it a unique ID number and then we'll add one to the IDs tracker.

02:14.420 --> 02:19.570
That way we go ahead and make sure that each next word gets a unique ID.

02:19.580 --> 02:22.480
Again we're not really counting the number of occurrences yet.

02:22.550 --> 02:29.440
We're just giving each word a unique ID so we can print out the vocabulary dictionary and we see this

02:29.480 --> 02:33.820
is one is is two is three stories four and so on.

02:33.830 --> 02:41.180
So each of these strings or words is getting a unique ID then we do the same thing for the second textfile

02:41.450 --> 02:44.300
using the same vocabulary dictionary as before.

02:44.300 --> 02:48.230
And notice we're again checking if that word is already present in the vocabulary.

02:48.230 --> 02:55.970
So for a word like story shows up in both documents we know that it has a unique ID across all the documents

02:56.030 --> 03:03.470
so story ID is for or in this location for and then we can store it that way and know that even though

03:03.680 --> 03:10.370
2.6 C has 15 words we ended up only adding 7 new words to that vocabulary dictionary.

03:10.470 --> 03:16.200
And so we've essentially encapsulated our entire language across all the documents in a dictionary so

03:16.200 --> 03:20.940
we can then begin to perform feature extraction on each of our original documents.

03:20.970 --> 03:27.120
So what we could do is for each of these documents we can create an empty vector space for each word

03:27.120 --> 03:28.380
in the vocabulary.

03:28.410 --> 03:33.600
So all I'm doing here is I'm saying one that ticks he has a string and then I'm adding a bunch of zeros

03:33.690 --> 03:38.230
for each vocabulary word that's present in this dictionary right here.

03:38.290 --> 03:43.360
That one is going to do is going to map the frequencies of each word across a text document.

03:43.620 --> 03:50.520
So again reading in all those words are now actually going to count the words and then I'm going to

03:50.520 --> 03:56.820
count them based off their exposition so we can see here that these particular words at these positions

03:57.210 --> 04:01.980
end up showing one time that I have a word here that shows up twice and then the rest of these words

04:02.010 --> 04:05.750
don't show up and the times they're only showing up in the other documents.

04:05.760 --> 04:10.930
So most of the time you're going to get a bunch of zeros and we explain this before.

04:10.950 --> 04:15.210
That means we're going to create a sparse matrix as we end up doing this for the other documents.

04:15.450 --> 04:18.690
So if we do this the same for the second document you get something that looks like this.

04:18.780 --> 04:22.400
Again a bunch of zeros for words that weren't used in the second document.

04:22.410 --> 04:27.320
So now we can begin to compare the vectors and we see that some words are common to both.

04:27.360 --> 04:29.120
Some appear only one that ticks.

04:29.220 --> 04:31.200
Others appear only in two that ticks T.

04:31.350 --> 04:36.120
And we're essentially going to extend this logic to tens of thousands of documents and we begin to see

04:36.120 --> 04:41.190
the vocabulary Dictionnaire grow to hundreds of thousands of words and the vectors again were mostly

04:41.190 --> 04:44.400
contain zero values making them sparse matrices.

04:44.400 --> 04:50.600
So now let's explore how we can learn about bag of words and T.F. IDF as well as using stop words Bortz

04:50.620 --> 04:51.140
them.

04:51.150 --> 04:53.600
Tokenization and tagging.

04:53.790 --> 04:57.110
So you can go ahead and read these descriptions here.

04:57.120 --> 04:59.290
A lot of this what we went over in that previous lecture.

04:59.610 --> 05:02.730
But we're going to now begin feature extraction from text.

05:02.730 --> 05:07.160
So for this I'm going to be again working with this messaging dataset.

05:07.290 --> 05:11.280
But now we're working directly with the message data.

05:11.280 --> 05:19.290
So I'm going to open up a brand new notebook and then follow along with that actual data set and using

05:19.290 --> 05:22.150
sikat learn to perform this feature extraction.

05:22.170 --> 05:30.930
The first thing to do is I'm going to import non-players MP and import panderers as PD and then we'll

05:30.930 --> 05:38.780
read in that file by saying read underscore CXXVI and then underneath the text files I have the Esam

05:38.790 --> 05:40.240
as spam collection.

05:40.410 --> 05:45.250
It's a TSB file meaning it's separated by tabs.

05:46.260 --> 05:51.710
And we can confirm this by checking out the head of that document.

05:51.780 --> 05:57.000
Previously we learned about sikat learn and how we can build a machine learning classifier given numerical

05:57.000 --> 05:57.910
information.

05:58.080 --> 06:03.690
What we want to do now is take this raw text information and vectorize it now is always a good idea

06:03.690 --> 06:05.070
to check for missing values.

06:05.070 --> 06:14.690
So one way to do that is saying DMF is null and taking some of that and they'll return back zeros if

06:14.690 --> 06:16.310
there are no missing values.

06:16.310 --> 06:21.990
Keep in mind you may also want to be a little more clever in this and check for strings that are empty.

06:22.130 --> 06:26.000
Maybe a missing value is not actually a completely missing value.

06:26.000 --> 06:29.870
It could just be a string of one space or something like that.

06:29.870 --> 06:31.690
Now we'll talk about that more in your assessment.

06:31.700 --> 06:33.230
But keep that in mind.

06:34.300 --> 06:34.840
OK.

06:34.970 --> 06:43.170
So we're going to do is take again a quick look at the label column and then call value counts here.

06:43.400 --> 06:50.830
You can see we have seven hundred forty seven instances of spam and 4000 825 instances of him.

06:50.850 --> 06:55.140
Next we're going to split the data into a training set and a test set.

06:55.200 --> 07:00.330
So we will say from S-K learn the model selection

07:02.930 --> 07:11.070
import train test split and then we're going to actually just look at the text.

07:11.080 --> 07:15.830
So I'm not even going to bother with the length of the message or the punctuation.

07:15.850 --> 07:19.390
I'm only going to be using the raw text as the feature itself.

07:19.600 --> 07:32.720
So I will say x is equal to the F message and Y is equal to if label and just by convention X is capitalized

07:32.810 --> 07:39.320
and Y is lower case that has to do with the fact that X kind of represents a larger matrix and Y is

07:39.320 --> 07:45.110
essentially a one dimensional array of just a bunch of labels Hamers spam.

07:45.110 --> 07:49.840
So an often question I get is why is X capitalized and why is lowercase.

07:49.850 --> 07:52.070
So the reason why just convention.

07:52.130 --> 07:58.820
Next we're going to actually perform the train split so when called Train to split do shift tab on this

07:59.000 --> 08:05.240
after you've imported it and you should be able to scroll down here and copy and paste right here.

08:05.480 --> 08:12.250
This command in order to save yourself a little bit of typing and we've already covered how the train

08:12.250 --> 08:13.200
to split works.

08:13.210 --> 08:20.630
So just going to pass in our X and r y we will choose a test size that's appropriate so we can say test

08:20.660 --> 08:22.410
size is equal to.

08:22.450 --> 08:27.400
And again there's no right or wrong answer but 30 percent or 33 percent a third of your data is pretty

08:27.400 --> 08:28.660
common.

08:28.660 --> 08:34.060
And then we will say a random state to make sure you're trained to split is randomized the same way

08:34.060 --> 08:35.280
minus or following on.

08:35.290 --> 08:36.310
Exactly.

08:36.310 --> 08:41.430
And you can just choose an arbitrary number so I'll choose 42 and you can choose 42 as well.

08:41.440 --> 08:44.330
So you get the same split I do next.

08:44.330 --> 08:47.720
It's time to actually perform count vectorization.

08:47.720 --> 08:50.510
So Tex pre-processing tokenizer.

08:50.540 --> 08:56.030
And the ability to filter out Stop words are all included in count victimiser which builds a dictionary

08:56.060 --> 09:01.500
of features and transforms documents to feature vectors and to use Skillern for this.

09:01.550 --> 09:13.130
We simply say from Learn the feature extraction text import and if hit tab here you can essentially

09:13.130 --> 09:18.110
scroll down and see the various things that are available like T.F. idea of transformer vectorize or

09:18.410 --> 09:20.690
vectorize or hashing vectorize or et cetera.

09:20.930 --> 09:28.280
Right now we're just going to use the Count vector Isar then we create an instance of the count vector

09:28.280 --> 09:37.120
Isar So I'd say count underscore that is equal to count vectorize or.

09:37.360 --> 09:39.790
So now you have this instance of this count vector Isar.

09:39.790 --> 09:46.300
So what I need to do is if I take a look at X it's right now still in raw text form what I want to do

09:46.420 --> 09:51.710
is pass it into the count vector Isar and then transform this.

09:51.730 --> 09:53.710
So there's two ways of doing this.

09:53.770 --> 10:01.950
The first thing I need to do is actually fit the vector Roser to the data and what that does is it does

10:01.950 --> 10:12.710
things like build a vocabulary and then it also does things like count the number of words and so on

10:13.400 --> 10:24.240
then the next step I need to do is transform the original text message to the vector.

10:24.240 --> 10:29.200
So this is known as a fit transform and there's two ways of doing this.

10:29.220 --> 10:32.970
One way is to first do your fitting and then do your transform.

10:33.240 --> 10:41.810
So you can do something like this or you can say count vectorize or fit to the training data and then

10:41.810 --> 10:52.250
say that my vectorized X train counts is equal to count vectorize victimiser and then call transform

10:53.820 --> 10:55.470
on your X training data.

10:55.470 --> 11:00.810
So that's one way to do it to fit essentially building on your vocabulary counting the words and then

11:00.960 --> 11:02.790
actually doing the transformation.

11:02.820 --> 11:06.990
So fitting by itself doesn't do the transformation you have to actually call that transform.

11:06.990 --> 11:15.020
So because almost always you do a fit and the transform together there's a convenience method that sikat

11:15.020 --> 11:21.410
learn has which is called if you call counter-act It's called Fit transform and essentially does these

11:21.410 --> 11:22.830
two steps for you.

11:23.090 --> 11:28.220
So you may see it both ways when you read the documentation you may see it as two separate steps fitting

11:28.430 --> 11:33.710
and then transforming or you may see it done in one line with this transform because it is so common

11:34.070 --> 11:36.500
to do these operations in conjunction.

11:36.680 --> 11:39.170
I'm going to comment this out and do it in one step.

11:39.200 --> 11:44.720
Since that's a little more efficient and we're just going to pass in our extremely data and this is

11:44.720 --> 11:50.420
going to fit essentially learn the vocabulary cut the number of words and then transform X train.

11:50.420 --> 11:59.230
So that's actually a vector numerical vectors so say X train counts and so that equal to the count vector

11:59.230 --> 12:02.110
Isar fit transform on the extreme data.

12:02.710 --> 12:08.590
So I run that and now I'm extremely data victory's or has fit and train on it and then transformed the

12:08.590 --> 12:09.630
original data.

12:09.670 --> 12:17.450
So if I take a look at x train counts notice that I actually can't view it because it's a huge sparse

12:17.540 --> 12:18.320
matrix.

12:18.530 --> 12:26.420
So sikat learn is actually smart enough to compress this into a compressed sparse Rove format.

12:26.420 --> 12:31.490
The way this works is you can imagine that if you have a bunch of zeros in a row it would make sense

12:31.490 --> 12:36.200
to actually compress this information and just count the number of rows you have or count the number

12:36.200 --> 12:37.530
of zeros you have in a row.

12:37.820 --> 12:39.300
So extreme counts.

12:39.320 --> 12:44.050
Notice it's three thousand seven hundred thirty three by seven thousand eighty two.

12:44.240 --> 12:51.320
So if you actually take a look at the original x train data and check the shape of it it has three thousand

12:51.630 --> 12:53.530
733 messages.

12:53.570 --> 12:59.910
So across all those messages there was seven thousand eighty two unique words.

12:59.930 --> 13:02.990
So a lot of those words are not going to show up in most of the text messages.

13:03.010 --> 13:09.170
You have a ton of zeros because of this sikat learn a PI are able to compress using the information

13:09.170 --> 13:12.890
that there's so many zeros there in order to save space in them or in your computer.

13:13.070 --> 13:18.050
So we're not going be able to directly see the sparse matrix unless we particularly call for it but

13:18.050 --> 13:23.340
that's usually not a good idea because you don't want to use that space in memory because it's not needed.

13:23.390 --> 13:25.660
We can let some PI be smarter about this.

13:25.670 --> 13:26.250
OK.

13:26.510 --> 13:31.240
So we have our original X-Trace shape those messages.

13:31.460 --> 13:38.270
And now if we Cecco X counts and asked for the shape of that we now see all those messages and then

13:38.420 --> 13:41.590
the vocab count that remember the vocab count we saw earlier.

13:41.630 --> 13:46.760
We come back to this feature extraction from text as well as the previous slide that right now is just

13:46.760 --> 13:48.110
looking something like this.

13:48.140 --> 13:54.440
Essentially all those counts across all those text messages where most of them are going to be zeros.

13:54.660 --> 14:00.870
Next we want to do is transform the counts to frequencies with T.F. IDF then we'll combine the steps

14:00.960 --> 14:05.280
with ATF IDF vectorize or we'll train the classifier and build the pipeline.

14:05.490 --> 14:08.770
We'll go ahead and continue right where we left off in the next lecture.

14:08.790 --> 14:16.440
But as a quick recap so far we've just read in our data we've imported and ran a train test split on

14:16.440 --> 14:17.400
this.

14:17.430 --> 14:22.620
We did a count of vectorization and then fit transform on the training data.

14:22.620 --> 14:23.780
I'll see you in the next lecture.

14:23.790 --> 14:29.160
We continue by combining these steps with term frequency inversed document frequency analysis.

WEBVTT

00:05.230 --> 00:09.460
Welcome back to part two of late in Thursley allocation with Python.

00:09.460 --> 00:12.180
Let's continue where we left off in the previous lecture.

00:12.190 --> 00:16.870
All right here we are at the notebook where we left off last time we defined three steps last time we

00:16.870 --> 00:22.240
wanted to be able to grow the vocabulary of words grab the topics and then grab the highest probability

00:22.240 --> 00:23.510
words per topic.

00:23.770 --> 00:27.730
Let's begin by showing you how to grab the vocabulary of words I'm doing.

00:27.760 --> 00:33.790
Enter to put themselves below the cell and what I want to show you is that the count vectorize or object

00:33.820 --> 00:36.760
we created for the document matrix.

00:36.760 --> 00:42.220
You can actually do a get underscore a feature underscore names method and if you check the length of

00:42.220 --> 00:47.540
what's returned here it's fifty four thousand seven hundred and seventy seven.

00:47.650 --> 00:51.880
And if we take a look at our previous sparse matrix that's actually the other dimension.

00:51.880 --> 00:59.380
In fact this is documents by words or a document matrix so get feature names is actually just holding

00:59.410 --> 01:02.850
everything or pulling an instance of every single word.

01:03.070 --> 01:10.750
In fact if you check the type of CV docket feature names it's just a list of all the words that were

01:10.780 --> 01:18.050
in all your documents so we can randomly grab some words off of this so I can say Get feature names

01:21.490 --> 01:28.410
and then Passons some sort of index position off of this list so I can grab word number index location

01:28.480 --> 01:31.680
50000 and that happens to be the word transcribe.

01:31.680 --> 01:37.880
Or I can get the word at location 41000 and there happens to be reproductive.

01:37.890 --> 01:45.030
So just to make this clear I could print out a bunch of random words from my list of words so I could

01:45.030 --> 01:57.910
say import random and then say random word ID is equal to random random integer and then say grab a

01:57.910 --> 02:08.230
random integer from 0 up to 54000 777 and make sure we still ran the word ID correctly and then call

02:08.230 --> 02:13.600
see the get feature names and then pasan that random word ID.

02:13.600 --> 02:18.460
So as you keep running the cell you should see some different random words show up.

02:18.460 --> 02:22.890
So these are all words that are in our actual documents.

02:22.930 --> 02:28.840
So now we know I can simply call Seabee that get feature names and then pass in some ID number like

02:28.840 --> 02:33.190
10 and I will get some sort of string word that happened to be in the document.

02:33.220 --> 02:36.120
So now I understand how to grab the vocabulary of the words.

02:36.130 --> 02:38.890
Now it's time to actually grab the topics.

02:38.890 --> 02:46.820
So the way we do this is we actually now grab this information directly off the trained LDA so I can

02:46.820 --> 02:52.730
say LDA or whatever happened to call your model and call component's underscore.

02:53.150 --> 03:02.180
Then you can check the length here of these components and you notice there's 7 components and LDA components

03:02.240 --> 03:03.410
underscore.

03:03.410 --> 03:12.130
If we check the type of this it actually is just a name PI array containing probabilities for each word

03:12.940 --> 03:16.450
so that I can say grabbed the first item.

03:16.450 --> 03:19.350
Actually you can even describe the shape of the array.

03:19.600 --> 03:25.330
You'll notice it's seven topics by fifty four thousand seven hundred and seventy seven words and we

03:25.330 --> 03:28.440
take a look at the actual components.

03:28.450 --> 03:31.350
You just notice it's this giant array.

03:31.420 --> 03:37.510
So what we're going to be able to do is combine this information with our ability to grab vocabulary

03:37.930 --> 03:42.220
in order to show you the highest probability words per topic.

03:42.220 --> 03:48.370
So we're going to do here is we're going to start off by just grabbing a single topic off of these components.

03:48.370 --> 03:54.520
So going to create some new cells and let's grab one of these topics we'll say single topic is equal

03:54.520 --> 03:58.800
to LDA components underscore and going to index at zero.

03:59.080 --> 04:04.750
So it's the very first topic and we still again don't know what this topic actually represents.

04:05.990 --> 04:15.080
So I'm going to take single topic and I'm going to ask for ARG sort and Arg sort does it returns the

04:15.140 --> 04:18.000
index positions that would sort the story.

04:18.050 --> 04:21.010
So let me show you with a very simple example first.

04:21.020 --> 04:24.530
So this is the output of our sort.

04:24.650 --> 04:29.920
But I'm going to create a very simple array just to make sure you understand this sort process.

04:29.990 --> 04:41.760
So I'm going say a r r so the variable name and P array and I get a pass in 10 and then 200 and let's

04:41.760 --> 04:49.030
say 1 and if I take a look at my array I have 10 201.

04:49.140 --> 04:56.160
If I take a look at array and then call sore on it it returns to 0 1.

04:56.160 --> 05:02.790
So what that actually means is this is the index positions that would sort the array from the lowest

05:02.940 --> 05:08.860
value to the highest value because the index to that was 1 index 0 that was 10.

05:08.940 --> 05:10.830
And then our largest value was the index 1.

05:10.830 --> 05:11.930
So it was two hundred.

05:12.180 --> 05:17.350
So what we're doing is we're going to take these single topics and then figure out what index positions

05:17.370 --> 05:22.280
we should be looking at for high probability words for this particular single topic.

05:22.320 --> 05:28.560
So it's all our is doing is essentially showing us the locations of these larger values.

05:28.560 --> 05:30.400
So that's where we're using our sort here.

05:30.480 --> 05:32.340
So I can then use these in expedition's.

05:32.370 --> 05:39.420
So it's actually going to be directly correlated with the index positions of this CV docket feature

05:39.420 --> 05:44.140
names since LTA that we're actually trained on the result of this compact razor.

05:44.610 --> 05:45.180
OK.

05:45.510 --> 05:51.770
So what we're going to do then is now I understand that I can call single topic the arc sort of index

05:51.770 --> 05:55.870
positions for the high probability words for that particular topic.

05:55.970 --> 06:07.300
What I'm going to do then is the following I will say single topic and then I'm going to call our sort

06:10.780 --> 06:15.220
and let's go ahead and grab the top 10 words for this topic.

06:15.370 --> 06:19.630
So off of our sort I'm going to say negative 10 colon.

06:19.930 --> 06:24.070
So this line is a little tricky to understand at first but it's actually not so bad.

06:24.070 --> 06:37.320
All we're doing here is remember that our source returns back index positions sorted from least to greatest.

06:37.330 --> 06:43.220
And what we're looking for are the top 10 values.

06:43.300 --> 06:47.310
In other words the 10 greatest values.

06:47.500 --> 06:55.520
So that means I want the last 10 values of our sort.

06:55.930 --> 07:01.180
And so what this does right here is after you call Ark's sort it says starting next mission negative

07:01.180 --> 07:01.880
10.

07:02.260 --> 07:04.060
Colin go all the way to the end.

07:04.180 --> 07:14.470
All this is doing is saying grab the last 10 values of the result of our sort which returns the index

07:14.470 --> 07:20.530
positions of the 10 highest probability words for this particular single topic.

07:20.530 --> 07:33.090
So what we're going to do is say top 10 words is equal to single topic arc.

07:35.190 --> 07:43.040
Negative 10 all the way to the end and then those are the positions of my top 10 words and I'll say

07:43.040 --> 07:56.570
for index in top 10 words Prince CV get feature names and then I'm going to call that index.

07:56.630 --> 08:01.970
So when you're done this may take a little time but hopefully you should see words that tend to have

08:01.970 --> 08:04.460
some sort of generalized topic feeling.

08:04.460 --> 08:11.330
So this first single topic the 10 highest probability words show up in that particular topic are new

08:11.660 --> 08:17.620
percent's government company million care people health said and says.

08:17.930 --> 08:23.580
So here we can maybe tell that these look maybe like business articles government articles or health

08:23.580 --> 08:25.430
care issue articles.

08:25.430 --> 08:31.750
So what it can also do is start asking for more words like the top 20 words here.

08:32.120 --> 08:36.680
So let's go ahead and call this top 20.

08:36.850 --> 08:43.540
We'll run this and we'll make sure we call top 20 here.

08:43.550 --> 08:44.230
There we go.

08:44.420 --> 08:49.460
And now I get to see the top 20 words for this particular single topic and now it definitely starts

08:49.460 --> 08:54.230
to clear up that maybe has to do more with health insurance and politics so I can see President state

08:54.230 --> 08:57.920
tax insurance federal government per cent and more.

08:58.070 --> 09:05.060
But what's really incredible here is that LDA is now showing us some sort of underlying latent topic.

09:05.090 --> 09:11.780
It's trying to show us that these particular words have high probabilities of showing up in this particular

09:11.930 --> 09:13.310
single topic.

09:13.310 --> 09:20.840
Let's go ahead and now set up a little loop that prints out the top 15 words for each of the seven topics.

09:21.970 --> 09:29.530
So we're going to do here is the following we'll say for in the next column a topic a little bit a tuple

09:29.530 --> 09:37.870
and packing because we're going to call enumerates off L.D. components.

09:37.940 --> 09:49.370
I'm going to use some string formatting here to say the top 15 words for Topic number and then I'm going

09:49.370 --> 09:50.710
to insert here.

09:50.990 --> 09:53.700
Index essentially is just the count.

09:53.710 --> 09:54.560
In fact was going to call it.

09:54.590 --> 09:57.020
I sort of get confused here.

09:59.610 --> 10:08.240
And then what I'm going to do is say Prince and I'm going to create a list of C-v get feature names

10:10.100 --> 10:22.900
index for index and topic Arc's sorts and then grab the last 15 words after you done are sort and then

10:22.900 --> 10:29.480
we'll go ahead and print we'll print some new lines here.

10:29.640 --> 10:35.430
All right so what is is doing is essentially saying for each topic and LDA components are LDA components

10:35.490 --> 10:36.390
was an array.

10:36.420 --> 10:42.810
We come back up here of seven so seven topics by fifty four thousand seven hundred and seventy seven

10:42.810 --> 10:45.570
words and has a value associated to each of these words.

10:45.660 --> 10:50.180
The greater the value the higher probability has a value to that particular topic.

10:50.190 --> 10:54.800
So what we're doing is for each of those topics we're just grabbing the top 15 words and then we're

10:54.810 --> 11:01.270
using miscomprehension to basically grab those index actions off our Get feature names words.

11:01.290 --> 11:07.380
So if you run this loop you should end up seeing the falling top 15 words for topic zero companies money

11:07.380 --> 11:13.530
year federal etc. So it looks as if healthcare in the United States top 15 words for topic number one

11:14.010 --> 11:20.760
military how security Russia government Trump $250 for topping number two World Family Home day time

11:20.760 --> 11:21.690
water city.

11:21.690 --> 11:25.440
So maybe some infrastructure stuff here we can see medical disease patients health.

11:25.440 --> 11:27.600
So maybe just general health.

11:27.600 --> 11:29.700
Topic number for the election.

11:29.760 --> 11:32.700
Obama Clinton so looks like stuff do with the election.

11:33.690 --> 11:37.620
Number five music maybe lifestyle articles etc..

11:37.740 --> 11:42.380
And here we can see data science university students school science education topics.

11:42.390 --> 11:45.850
So here we only printed out seven topics zero through six.

11:46.020 --> 11:48.940
It's up to you to decide how many topics start beforehand.

11:49.080 --> 11:54.000
If some of these topics maybe aren't clear enough or maybe some of these topics seem too similar maybe

11:54.000 --> 11:59.370
you go for less topics or if you want to get some clarity and subdivide these into further topics you

11:59.370 --> 12:02.300
would just ask for more topics right off the top.

12:03.200 --> 12:09.730
So the last thing left to do is to attach these topic numbers to the original articles.

12:09.830 --> 12:11.780
And that's actually pretty straightforward.

12:11.780 --> 12:19.520
All we need to do is recall that we have our original document turned matrix as well as our original

12:19.850 --> 12:26.390
NPR data frame but we would like to do is essentially create a new column here that has a topic number

12:26.990 --> 12:35.700
and to do that all we really need to do is create a list of the actual topics off this document matrix

12:35.940 --> 12:51.970
and what we can do is call LDA dot transform on our original DTM so say topic results and go ahead and

12:51.970 --> 12:52.410
run that.

12:52.450 --> 12:54.380
Again this may take a little bit of time.

12:54.400 --> 12:59.980
Technically we've already done this but we call the fit transform instead of just a dot transform.

13:00.060 --> 13:05.830
And now if I take a look at topic results what this is is an array.

13:06.000 --> 13:11.600
And if you check the shape it's in array where we have the original eleven thousand nine hundred ninety

13:11.600 --> 13:13.030
two articles presented.

13:13.320 --> 13:15.780
And now we have by 7.

13:15.780 --> 13:21.020
So what this is if I take a look at the very first instance of topic results.

13:21.150 --> 13:25.410
These are essentially probabilities of belonging to a particular topic.

13:25.410 --> 13:29.910
Remember we previously saw probabilities of words belonging to a particular topic.

13:29.910 --> 13:36.000
Now we're seeing the probability of a document belonging to a particular topic where this topic is topic

13:36.000 --> 13:37.970
number 0 indexed 0.

13:37.980 --> 13:41.640
This is the probability of topic number one index 1.

13:41.640 --> 13:45.900
Notice here how these are essentially just values in percentage form.

13:45.900 --> 13:51.720
So somewhere between 0 and 1 and if it's a little hard for you to read that you can call that round

13:51.720 --> 13:55.110
off of this and you can see the actual percentages.

13:55.320 --> 14:00.600
So it looks like the very first article at index 0.

14:00.810 --> 14:07.410
It has a high its highest probability is around 68 percent belonging to topic number 1.

14:07.410 --> 14:08.610
So let's see if that makes sense.

14:08.610 --> 14:13.980
What was topic number one topic number one tended to include words like military House security Russia

14:14.010 --> 14:16.690
government President Trump police etc..

14:16.770 --> 14:21.120
So let's take a look back at the original documents.

14:21.120 --> 14:27.540
Remember we can print out any article we want so let's go ahead and print out the first article and

14:27.570 --> 14:29.040
it looks like it does have to do with that.

14:29.040 --> 14:35.400
So Washington of 2016 even pulsing the bipartisan politics cannot etc. so it does look like it has to

14:35.400 --> 14:36.260
do with politics.

14:36.270 --> 14:42.750
It has to do if Russia it has to do security Moscow etc. so that seems like a pretty fair topic.

14:42.750 --> 14:49.530
Now keep in mind because of recent news and the way NPR works and some of these articles particularly

14:49.530 --> 14:52.520
the way we scrape these articles a lot of them actually do of politics.

14:52.590 --> 14:58.680
So you may see feel that some of these topics overlap which OK we're going to do now is show you how

14:58.680 --> 15:01.670
to easily connect this with our original data frame.

15:01.700 --> 15:06.480
So what we're going to do is I'm actually not interested in these probabilities.

15:06.480 --> 15:10.720
I'm just interested in the index position of the highest probability.

15:10.980 --> 15:19.250
And I can grab that by calling RMX and RMX returns back the index position of the highest probability.

15:19.290 --> 15:22.310
So again RMX says index 1.

15:22.320 --> 15:24.340
That's where you had your highest probability.

15:24.340 --> 15:38.140
So going to do is say NPR topic is equal to topic results and then I'm going to call RMX across.

15:38.160 --> 15:42.190
Access is equal to one.

15:42.470 --> 15:44.280
And then I can see the assigned topics.

15:44.300 --> 15:50.030
So note that these first four looks like their topic one then from photography Illustration A video

15:50.030 --> 15:54.960
looks like topic to topic 3 2 3 2 5 5 and so on.

15:55.160 --> 15:59.600
And then if you ever want to figure out well what is topic 5 represent says who's the YouTube star of

15:59.600 --> 16:00.630
2016.

16:00.710 --> 16:03.970
We go ahead and see will or the high probability words for topic 5.

16:04.190 --> 16:09.290
We simply scroll up see topic 5 has to do with music people life etc..

16:09.530 --> 16:15.200
So looks like it makes sense that this story about YouTube stars 2016 and Adele singing has to do with

16:15.200 --> 16:16.240
top five.

16:16.610 --> 16:19.970
All right I hope you found this as fascinating as I did.

16:20.000 --> 16:25.370
If you have any questions feel free to ask in the Q and A forums and I would highly encourage you to

16:25.370 --> 16:29.810
check out our original site and Thursley allocation notebook so you can play around with those values

16:29.810 --> 16:30.460
as well.

16:30.700 --> 16:31.270
OK.

16:31.490 --> 16:33.400
Thanks and I'll see you at the next lecture.

WEBVTT

00:05.370 --> 00:10.560
Welcome back everyone in this lecture we're going to discuss non-negative matrix factorization and how

00:10.560 --> 00:12.390
we can apply it to topic modeling.

00:13.830 --> 00:19.890
Non-negative matrix factorization is an unsupervised algorithm that simultaneously performs dimensionality

00:19.920 --> 00:22.230
reduction and clustering.

00:22.230 --> 00:27.300
We can use it in conjunction with T.F. idea to model topic's across various documents.

00:28.610 --> 00:33.840
So because the general mathematics behind non-negative matrix factorization are an image.

00:34.120 --> 00:38.380
And that we'll show you how you can apply it with a document matrix.

00:38.480 --> 00:43.910
So first you're given a non-negative matrix A which is what we're going to be able to create using T.F.

00:43.940 --> 00:44.570
idea.

00:44.900 --> 00:50.540
And then we want to find a K that image an approximation in terms of two other non-negative factors

00:50.640 --> 00:52.080
w an h.

00:52.130 --> 00:58.340
So you can have some matrix A which has and by M some rows features and columns or objects and we want

00:58.340 --> 01:06.820
to perform factorization to essentially approximate a with the matrix multiplication of W and H where

01:06.830 --> 01:14.820
we have and by K for W and K by M for H and again we are going to approximate each object.

01:14.860 --> 01:23.550
That is the columns of a by a linear combination of K reduce them mentions or basis vectors in W. each

01:23.550 --> 01:29.100
Bass's vector can then be interpreted as a cluster and the membership's of objects in these clusters

01:29.190 --> 01:30.770
are encoded by H.

01:32.580 --> 01:36.810
So the way this is going to work again is we have an input that we're providing a non-negative data

01:36.810 --> 01:38.570
matrix which is a.

01:38.730 --> 01:40.910
And in this case that's going to be your T.F. idea.

01:41.010 --> 01:47.310
So your term frequency inverse document frequency for the documents versus the actual words and those

01:47.400 --> 01:52.920
across all the documents then we're going to have a number of Bass's vectors or k.

01:53.190 --> 01:57.830
In our case for topic modeling the k is going to be the number of topics we choose.

01:57.840 --> 02:04.420
So just like before with LDA we actually have to decide on how many topics we want to try to discover.

02:04.440 --> 02:10.290
So we get to choose the value OK and then we're going to initialize some values for the factors W H

02:10.740 --> 02:17.460
and these start off as random matrices then it's our job to essentially First get some sort of measurement

02:17.580 --> 02:21.080
of the reconstruction error between a and the approximation.

02:21.320 --> 02:25.900
H and then we're going to do is optimize.

02:26.140 --> 02:32.890
So we have an expectation maximisation optimization to essentially refine W and H in order to minimize

02:32.890 --> 02:39.310
that objective function essentially keep updating those coefficients until the approximation of W. H

02:39.550 --> 02:41.590
is actually going to make sense for a.

02:41.860 --> 02:47.170
So a common approach is to essentially iterate between two multiplicative update rules until convergence

02:47.350 --> 02:53.330
updating H then updated w and then repeating that process and to Convergys.

02:53.340 --> 02:59.550
So the way this works for our particular use case is we first construct a vector space model for documents

02:59.880 --> 03:02.160
after we do things like filter out Stop words.

03:02.190 --> 03:08.420
And that results in a term document matrix for a or a document termit tricks DTM.

03:08.610 --> 03:14.940
Then we apply T.F. IDF term weight normalization to a and then normalized T.F. idea of vectors to unit

03:14.940 --> 03:15.920
length.

03:15.930 --> 03:19.750
Keep in mind those first three steps even though it sounds like a lot.

03:19.890 --> 03:24.740
Sikat learn is essentially doing all of that for us when we call it T.F. idea of vector Isar.

03:24.960 --> 03:29.820
So we don't have to do things like remove the Stoppard's ourselves and call count vectorizing and then

03:29.820 --> 03:34.220
call the term frequency and then figure out inverse document frequency.

03:34.290 --> 03:40.710
Recall that all of that from text feature extraction is essentially done under the hood for us by calling

03:41.070 --> 03:44.410
the T.F. idea of vector Isar circular.

03:44.520 --> 03:50.540
So once we've done that or we're going to do is we're going to initialize the factors using an ant and

03:50.710 --> 03:58.510
the SVT or non-negative double single singular value decomposition on the A matrix.

03:58.650 --> 04:06.660
And the we're going to apply a projected gradient non-negative matrix factorization to a.

04:06.670 --> 04:10.080
So then what we end up discovering are the basis vectors.

04:10.180 --> 04:15.550
Those are the topics or clusters in the data and then we have a coefficient matrix which is the membership

04:15.550 --> 04:18.590
weights for documents relative to each topic.

04:18.790 --> 04:21.880
In this case each cluster.

04:21.940 --> 04:27.160
So again all we're doing is we're going to create a documentary matrix with T.F. idea factorization.

04:27.190 --> 04:28.270
So we've done that before.

04:28.270 --> 04:32.860
It looks something like this here instead of numerical values we're just representing the various values

04:32.890 --> 04:35.100
by how dark blue there is.

04:36.120 --> 04:39.170
And then we're going to have a resulting w an H.

04:39.210 --> 04:40.630
And so the basics are vectors.

04:40.770 --> 04:42.160
Those columns in W..

04:42.180 --> 04:47.790
Those are your topics and we can see how words relate to particular topics.

04:47.790 --> 04:51.230
So looks like Topic one has TV shows and actors.

04:51.300 --> 04:57.140
Topic 2 has to do a sports clubs footballs and then topic three tenths of do business banks money and

04:57.150 --> 05:04.230
finance and then h is going to have coefficients and that's going to be the membership's for each document.

05:04.260 --> 05:08.770
So we have co-efficient values for each document and how they relate to each topic.

05:09.030 --> 05:15.480
So the results are pretty similar as far as interpretation to what we saw before we get some sort of

05:15.480 --> 05:20.470
relationship between the documents and the topics and the words and the topics.

05:21.400 --> 05:28.360
Now keep in mind just like LDK were still going to need to select the number of expected topics beforehand.

05:28.510 --> 05:33.550
So we need to actually again choose that value of k and say something like I believe there are 12 topics

05:33.580 --> 05:36.640
across all these articles or I believe there are seven topics.

05:36.640 --> 05:43.060
So you have to initialize with some sort of value of k and also just like with L.D. a we all have to

05:43.060 --> 05:48.440
interpret these topics based on the coefficient values of the words per topic.

05:48.640 --> 05:54.550
Keep in mind these matrix coefficient values are not probabilities that can be interpreted like we did

05:54.550 --> 05:55.750
with LDK.

05:55.750 --> 06:01.670
We're still going to be able to interpret a relationship between a word to a topic of the coefficient

06:01.750 --> 06:07.650
and between a document to a topic with the coefficient but it's not like a probability value.

06:07.690 --> 06:15.600
Lta gave us it's just the coefficient value that the non negative matrix factorization solved for now.

06:15.610 --> 06:22.270
Luckily due to sikat learns uniform syntax switching out LDK for MF is actually very simple.

06:22.270 --> 06:28.480
In fact we're going to be able to kind of run through the last notebook and very easily adapt it for

06:28.540 --> 06:34.840
an F so we're going to repeat our analysis of the article dataset and show you how you can do topic

06:34.840 --> 06:37.170
modeling and discover topics.

06:37.450 --> 06:43.120
And we're going to be able to go through it a lot faster because we actually just have to swap out a

06:43.120 --> 06:45.530
couple of things in order to run an MF.

06:45.640 --> 06:49.630
So we're going to use the same notebook as we did before and then just show you what you need to swap

06:49.630 --> 06:52.010
to perform non-negative matrix factorization.

06:52.300 --> 06:54.170
OK we'll see you at the next lecture.

WEBVTT

00:05.310 --> 00:07.440
Welcome back everyone in this lecture.

00:07.440 --> 00:11.590
We're going to show you how to actually perform latent Thursley application with Python.

00:11.820 --> 00:13.790
Let's open up a new notebook and get started.

00:15.150 --> 00:21.360
As a quick note underneath the topic modeling folder you'll notice that there is an NPR that CXVII.

00:21.530 --> 00:25.660
We're going to be using that and pure that CSFB data file to actually load up some articles.

00:25.790 --> 00:28.950
So make sure you actually look at it underneath the topic modeling folder.

00:29.180 --> 00:32.750
And if you want to follow any of the code that we show there in this lecture you can run it from the

00:32.870 --> 00:34.350
latent location.

00:34.550 --> 00:39.290
I've gone ahead and created a new notebook in the same folder so I can easily access this NPR that see

00:39.290 --> 00:40.090
every file.

00:40.370 --> 00:45.080
Let's go ahead and go to that untitled notebook now and the first we're going to do is actually load

00:45.080 --> 00:46.430
up that data.

00:46.670 --> 00:52.990
So we'll say import pand this is Preedy and then check out what that data actually holds by saying NPR

00:52.990 --> 01:01.680
is equal to PD that read underscore as we and NPR see as we.

01:01.700 --> 01:06.620
So let's take a look at the head of this data frame so we can understand what this actually is what

01:06.620 --> 01:07.850
this dataset is.

01:07.850 --> 01:14.450
It's a couple of thousand articles and each of these rules is the full text of one of the articles.

01:14.450 --> 01:18.320
Notice that right now we only have information on the article text.

01:18.320 --> 01:22.700
We don't actually have some sort of label column indicating what topic this particular article belongs

01:22.700 --> 01:23.540
to.

01:23.540 --> 01:30.460
So for example if we actually want to view one of the articles we simply say NPR at the article call

01:30.460 --> 01:32.700
them and then passen whatever role we want.

01:32.840 --> 01:37.580
So if all of the very first article I could index at zero and it return back the text of that entire

01:37.580 --> 01:38.310
article.

01:38.330 --> 01:40.360
This is essentially the first row.

01:40.610 --> 01:48.770
And if you look up the length of the actual dataset self-check of length NPR there's almost 12000 articles

01:48.770 --> 01:49.280
here.

01:49.370 --> 01:55.370
So you can have fun exploring just any basically any number between 1 and eleven thousand nine hundred

01:55.370 --> 01:55.960
ninety two.

01:55.970 --> 02:03.230
So article number 4000 we can run that was like the comment at that link line and here we can see an

02:03.230 --> 02:04.800
article on this.

02:04.800 --> 02:09.770
So what we don't know yet is what actual topics each of these articles belong to.

02:09.770 --> 02:14.110
So we're going to try to attempt to figure out and assign a topic to each of these articles.

02:14.120 --> 02:16.180
Let's go ahead and continue on.

02:16.310 --> 02:21.470
The first thing I need to do before I actually run LDA is perform a little bit of pre-processing and

02:21.490 --> 02:30.910
we can do that by saying from Eski learn the feature extraction text and we're going to do vectorization

02:31.090 --> 02:33.910
something we've already seen in the text classification lecture.

02:34.080 --> 02:41.150
So we're going to import vectorize or elysÃ©e C-v is equal to an instance of Count victimiser.

02:41.250 --> 02:45.050
Now we're going to pasan three parameters that are really useful.

02:45.060 --> 02:49.620
The first one is Max D F and Max the F.

02:49.620 --> 02:54.780
What it does is when you're building up the vocabulary we're going to ignore certain terms that have

02:54.780 --> 02:56.810
really high document frequency.

02:56.850 --> 03:03.030
So this essentially gets rid of terms that are really common across a lot of the documents.

03:03.150 --> 03:07.100
So you can pass in a number between 0 and 1 here.

03:07.110 --> 03:10.450
So such as 90 or 0.9.

03:10.680 --> 03:16.800
And what this is going to do is it's going to discard words that show up in 90 percent of the documents

03:17.280 --> 03:18.650
and you can play around this value.

03:18.780 --> 03:23.460
So if you want to be a little looser of what words get thrown out or not throw away as many words you

03:23.460 --> 03:28.220
can say something like Ok only words that show up in 95 percent of all the documents.

03:28.470 --> 03:36.820
So you can play around that ratio and it's counterpart is mindif which is essentially a minimum.

03:36.870 --> 03:39.560
So what's the minimum document frequency.

03:39.630 --> 03:46.350
So words that show up a minimum amount of times and you can actually passen for both Max T.F. and mindif

03:46.710 --> 03:52.980
either a ratio between 0 and 1 or if you pass an integer it will actually take that into account as

03:53.190 --> 03:55.080
a raw number of documents.

03:55.080 --> 04:02.420
So for example if I passen to here this says that the minimum frequency for a word to be counted into

04:02.450 --> 04:06.450
this count vectorize her it has to show up in at least two documents.

04:06.450 --> 04:09.900
So it can't be totally unique to a single article.

04:10.260 --> 04:14.880
So that's usually a good idea to throw out some really common words and also throw out random words

04:14.910 --> 04:16.450
maybe even typos or misspelling.

04:16.590 --> 04:19.870
So you have to guarantee that they show up at least in two documents.

04:20.130 --> 04:20.850
OK.

04:20.910 --> 04:26.400
And then also you're going to do is we can automatically remove Stoppard's simply by saying Stop words

04:26.400 --> 04:33.530
here and pass in the string English and vectorize or can actually automatically remove those Stoppard's.

04:33.750 --> 04:38.320
We have other methods of doing this as we know if we wanted to we could tokenize all of these Ramoo

04:38.320 --> 04:43.680
stoppers ourselves with spacey but we could also let victimiser take care of all that for us as well

04:43.680 --> 04:49.760
as take out really common words and make sure words are meeting some sort of minimum threshold.

04:50.150 --> 04:50.690
OK.

04:51.030 --> 04:52.480
So there's our count vectorize.

04:52.650 --> 04:57.090
Now it's time to actually apply this and transform to the article.

04:57.090 --> 05:01.590
Again notice here I'm not going to do any sort of train to split because this is unsupervised learning.

05:01.710 --> 05:06.110
I don't have any label to test against So this really makes sense to test train split and said what

05:06.110 --> 05:08.170
I'm doing is essentially unsupervised learning.

05:08.270 --> 05:12.800
We're going to transform to the entire dataset.

05:12.810 --> 05:21.730
OK so we're going to do now is we're going to say a document matrix is equal to C-v thought that transform

05:22.960 --> 05:26.560
and I'm going to do this on the article call them

05:29.830 --> 05:36.310
and then if we check out our document matrix then you should have a sparse matrix.

05:36.310 --> 05:39.970
Keep in mind depending on what computer you have this fit transform may take a while.

05:39.970 --> 05:41.530
This is a pretty big dataset.

05:41.740 --> 05:48.570
But right now it looks like we have almost about 55000 terms and here the number of articles.

05:48.730 --> 05:50.860
OK so there's our turn matrix.

05:50.860 --> 05:55.020
Up next what we need to do is actually perform the latent a.

05:55.390 --> 06:02.080
And this is built in to sikat learn all you need to say is from a scalar the composition

06:04.560 --> 06:05.370
import.

06:05.880 --> 06:07.480
And then you can actually just directly import.

06:07.490 --> 06:11.020
You could have autocomplete this latent there are allocation.

06:11.200 --> 06:15.510
Now we're going to say LDA as our variable name here.

06:15.510 --> 06:20.190
Typically you don't actually have variable names are capitalized but I like this because it kind of

06:20.190 --> 06:21.740
makes it sound like an acronym.

06:22.020 --> 06:28.830
And also it doesn't make the l be confused of a one sort of say LDA and then we're going to call instance

06:28.900 --> 06:33.460
a late since there is only allocation and we can explore some of the parameters.

06:33.480 --> 06:39.450
There's lots of different parameters here such as learning the K learning method et cetera number of

06:39.450 --> 06:46.110
iterations the most important ones that we actually want to go through are number of components and

06:46.110 --> 06:50.910
we also want to make sure we set a random state because there is some random this because remember we

06:50.910 --> 06:53.080
randomly assigned words in the beginning.

06:53.130 --> 06:57.210
So we want to make sure that our random state is a sign that we are going to get the same topics that

06:57.270 --> 06:57.910
I do.

06:58.940 --> 07:03.760
So we're going to do here is first decide on the number of components.

07:03.920 --> 07:06.700
So we're going to say number of components.

07:07.220 --> 07:10.270
And this is where there is no right or wrong answer.

07:10.280 --> 07:16.060
It really depends on your domain experience and your familiarity with the domain that data is from.

07:16.130 --> 07:19.990
I'm going to just say I want 7 general topics returned.

07:20.000 --> 07:25.640
If you want to get into more detail such as maybe subcategories of politics like international politics

07:25.970 --> 07:27.500
versus national politics.

07:27.500 --> 07:32.210
If you're hoping for that sort of level of information in subtopics you might want to make that number

07:32.210 --> 07:33.980
of components much larger.

07:34.070 --> 07:35.940
Again it's totally up to you.

07:36.470 --> 07:42.230
So then we're going to just choose subcomponents as seven essentially looking for seven topics and then

07:42.230 --> 07:47.820
I'm also going to say random state is equal to 42 and I would encourage you to do the same.

07:47.870 --> 07:53.990
That way the words return per topic are the same for you as they are for me.

07:53.990 --> 08:01.190
OK so we have LDA and now it's time to take LDA and fit it to our document turn matrix.

08:01.190 --> 08:04.820
So we'll go ahead and run that and I'm going to fast forward in time.

08:04.820 --> 08:05.870
This should take awhile.

08:05.870 --> 08:07.360
Again it depends on your hardware.

08:07.490 --> 08:12.730
But we are dealing with a large number of documents and remember LDA is an iterative process.

08:12.770 --> 08:19.190
So it's going to keep updating those weights per word per topic over and over again until they've stabilized.

08:19.190 --> 08:22.360
So I'm going to go and hop forward in time until that's done fitting.

08:22.930 --> 08:23.550
OK.

08:23.690 --> 08:25.180
We have finished fitting the model.

08:25.370 --> 08:27.800
Now there's only three steps left.

08:27.800 --> 08:34.480
The first step is to actually grab the vocabulary of words.

08:35.610 --> 08:46.980
The second step is to grab the topics and the first step is to grab the highest probability words per

08:46.980 --> 08:47.710
topic.

08:47.760 --> 08:50.690
That way we can actually interpret the topics themselves.

08:50.760 --> 08:54.000
So we're going to continue right off of this in the very next lecture.

08:54.000 --> 08:54.590
We'll see if there.

WEBVTT

00:00.240 --> 00:01.360
Welcome back everyone.

00:01.440 --> 00:06.780
In this lecture we're going to show you how to use voter sentiment analysis with Python and L2.

00:07.240 --> 00:10.030
OK let's begin by importing an el T.K..

00:10.740 --> 00:15.910
And then what you need to do is download the Vader lexicon and you only need to do this once.

00:16.200 --> 00:25.580
So we'll say an 80 k download and then pass in Vader underscore lexicon.

00:25.760 --> 00:26.950
Go ahead and run that.

00:27.140 --> 00:30.690
And then if you've already downloaded it it should be almost instantaneous.

00:30.800 --> 00:34.100
If not it may take a little bit of time to download the data from the Internet.

00:34.100 --> 00:38.020
Make sure you check your firewall and make sure that's not blocking your download.

00:38.030 --> 00:43.600
So once you've downloaded the Vader lexicon you should be able to import it using from an A.K..

00:43.610 --> 00:55.260
That sentiment Darth Vader import and then it's sentiment intensity analyzer and then create an instance

00:55.260 --> 00:55.780
of it.

00:56.010 --> 00:56.880
We call it S.A.T.

01:00.020 --> 01:05.480
and what Vader's sentiment intensity analyzer does is it simply takes in the string and returns a dictionary

01:05.480 --> 01:12.200
of scores in four categories negative neutral positive and then a compound score which is computed by

01:12.200 --> 01:15.190
normalizing the negative neutral and positive scores.

01:15.260 --> 01:17.310
So it's create a really simple string.

01:17.390 --> 01:27.000
We'll say this is a good movie and then we'll say essay The Dot and off of this we will call polarity

01:27.030 --> 01:33.840
underscore scores and then pass in the string and you get back this dictionary which has some negative

01:33.840 --> 01:39.570
value a neutral value a positive value and then a compound value which essentially normalizing these

01:39.570 --> 01:40.880
three values here.

01:40.890 --> 01:44.870
So as we expect there is no negative value since it's this is a good movie.

01:44.970 --> 01:49.290
It has some neutral words or tones in it and then it has also some positive tones.

01:49.380 --> 01:53.220
And the max value for any of these is one point zero.

01:53.220 --> 01:56.490
So now let's try a more complicated string.

01:56.610 --> 02:07.140
We'll say this was the best comma most awesome movie and then we're going to capitalize ever made and

02:07.140 --> 02:08.690
have three exclamation points.

02:08.790 --> 02:14.070
And as we previously mentioned Vader is smart enough to understand things like repeated punctuation

02:14.610 --> 02:16.230
and capitalization.

02:16.230 --> 02:17.580
So go ahead and run that.

02:17.580 --> 02:21.280
And then again call aside the polarity scores.

02:21.390 --> 02:23.040
Make sure we spell that right.

02:23.220 --> 02:24.340
Pass on the string.

02:24.540 --> 02:28.910
And here we can see it's again more positive than the previous one.

02:29.220 --> 02:34.420
And we can see here that the compound score is much more positive because neutral also dropped.

02:34.500 --> 02:37.430
Finally let's go ahead and have a very negative string.

02:37.440 --> 02:47.560
So we'll say this was the worst movie I have ever or that has ever disgraces seen.

02:47.600 --> 02:56.120
Let's say that has ever disgraced disgraced the screen OK.

02:56.160 --> 02:57.900
So quite a negative review.

02:57.900 --> 03:04.040
Let's see if the Vader picks it up we'll say S.A.T. polarity scores pass an A.

03:04.240 --> 03:09.430
And here we can see that now there is no positive it's just neutral and negative and so happens is the

03:09.430 --> 03:15.400
compound score then becomes negative so we can see here a compound score of zero would be completely

03:15.400 --> 03:15.960
neutral.

03:16.270 --> 03:22.420
A compound score above zero indicates some sort of positive score and then a compound score below zero

03:22.540 --> 03:24.870
indicates some sort of negative score.

03:25.030 --> 03:30.160
So that we're going to do is show you how you can use Vader to analyze Amazon reviews.

03:30.190 --> 03:35.260
So in our text files folder we have an amazon reviews the TSC file.

03:35.260 --> 03:38.560
I went ahead and moved it to the same location as my notebook here.

03:38.860 --> 03:45.470
And we're going to read it in using pandas will say import pandas as PD and they will say ADF is equal

03:45.470 --> 03:50.440
to PD read CSB and already moved the file here.

03:50.450 --> 03:53.500
So it's simply called Amazon reviews that TSB.

03:53.510 --> 03:58.870
It's again located under text files folder and it's tab separated.

03:58.940 --> 04:04.140
So you need to also indicate that the separator is backslash T for Tab separation.

04:04.190 --> 04:08.400
Once you read that in you should be able to view it by simply calling the head of that data frame.

04:08.570 --> 04:10.560
And essentially what we have here are labels.

04:10.600 --> 04:15.260
They see their process for positive or energy for a negative and then we have the actual text of the

04:15.260 --> 04:16.100
review.

04:16.100 --> 04:22.400
So if we wanted to get an idea of how many positive or negative labels we want we have we can say the

04:22.400 --> 04:29.630
F pass passing the label column and then simply call value underscore counts and we can see here we

04:29.630 --> 04:35.250
have slightly more negative reviews than positive reviews but overall it looks like we have around 10000

04:35.660 --> 04:36.520
reviews.

04:36.680 --> 04:42.110
So what we're gonna do now is do a little bit of cleaning of the data just to double check that we have

04:42.110 --> 04:47.150
no empty records and then we're going to run a first review through Vader.

04:47.150 --> 04:52.460
So quite simply what we're gonna do here is simply say the F that drop N A.

04:52.580 --> 04:55.960
And we'll say in place is equal to true.

04:56.060 --> 05:02.060
That's going to drop anything that's missing and then we're going to do is drop anything that has a

05:02.150 --> 05:04.250
empty whitespace value.

05:04.250 --> 05:08.600
Now for your data sets depending where you get them you may or may not have this but it's always a good

05:08.600 --> 05:08.950
idea.

05:08.960 --> 05:12.480
So I'm just saying for index for label and for review.

05:12.500 --> 05:20.510
So those are kind of place holders there and Def dots and then I'm going to call it or tuples so essentially

05:20.510 --> 05:27.140
here everything is just going to be returned as a tuple where I have the index the label and then the

05:27.140 --> 05:28.820
review text.

05:28.820 --> 05:39.910
So for I label and review I'm going to say if the type of the review is equal to the string type then

05:39.910 --> 05:47.480
I'm going to check that the review is space essentially checking whether or not it's a space there.

05:47.860 --> 05:55.410
And if it's true I'm going to take a list of blanks which will go ahead and define it will say blanks

05:56.260 --> 06:01.510
and we've already seen this kind of thing before but I'll simply say blanks append and then we'll plan

06:01.570 --> 06:03.330
that index position.

06:03.430 --> 06:06.000
So if we run this let's go ahead and check on blanks.

06:06.010 --> 06:07.340
See if we had any blanks.

06:07.450 --> 06:08.140
It looks like we did it.

06:08.230 --> 06:09.010
This list is empty.

06:09.010 --> 06:14.860
So we don't need to drop anything but if we did have some index positions that were brand blanks we

06:14.860 --> 06:21.590
simply need to say DLF drop blanks and then we could say in place is equal to true.

06:21.730 --> 06:25.360
But again since we don't have any we don't actually need to run that line.

06:25.360 --> 06:30.490
So now we're going to do is continue on and run a first review through Vader.

06:30.670 --> 06:38.630
So say SD called polarity scores and we're going to just run the first review on it so we'll say DFT

06:38.630 --> 06:46.170
dot and we'll call I 0 and then call that particular review.

06:46.250 --> 06:51.620
So all I'm doing is if we were to check out this one line here that's essentially just grabbing the

06:51.620 --> 06:54.620
text of the first review so we can see here it's quite positive.

06:54.620 --> 07:01.040
The soundtrack was beautiful game music exclamation points best music etc. so shook up the polarity

07:01.040 --> 07:06.620
score here and then we get back this dictionary and it looks like it has a very small amount of negativity

07:06.980 --> 07:13.040
that Vader picked up and it could be small phrases that get confusing for Vader things like anyone who

07:13.040 --> 07:18.200
cares to listen may be kind of negative in a slight sense but it's actually a very small negativity.

07:18.320 --> 07:23.690
In fact most of it is neutral or slightly positive which means a compound score is extremely positive

07:23.990 --> 07:28.650
which if we take a look at that first label was positive so the first few reviews are positive.

07:28.730 --> 07:31.520
So looks like Vader is actually able to select that.

07:32.090 --> 07:35.740
So now let's go ahead and as scores and labels to the data frame.

07:35.900 --> 07:46.860
So we're going to say the F scores is equal to def review and then we're going to call in apply method

07:47.310 --> 07:54.000
in order to essentially apply SD polarity scores to every single review in our data frame so we'll say

07:54.720 --> 08:02.630
lambda take that review and then apply SD polarity scores to that particular review.

08:02.980 --> 08:07.270
So we run that and this may take a little bit of time because it is running this whole polarity of course

08:07.270 --> 08:08.830
function on every single review.

08:09.010 --> 08:13.300
But once you have that you can go ahead and check out the ahead of the data frame and then you'll get

08:13.300 --> 08:18.730
back in your column scores that contains this dictionary and often you really just want to be of the

08:18.730 --> 08:19.780
compound score.

08:19.810 --> 08:25.240
So let's go ahead and create a new compound column we'll say the F compound and then we'll set that

08:25.240 --> 08:30.100
equal to the F scores and similar idea here.

08:30.100 --> 08:37.270
We're simply going to apply a lambda and we're gonna pass in that dictionary and then just gram compound

08:37.350 --> 08:38.410
off of that dictionary.

08:38.410 --> 08:43.050
So every item in scores is a dictionary source since just reading the score for compound.

08:43.180 --> 08:45.370
So we'll go ahead and run that.

08:45.910 --> 08:49.350
That's going to be much faster since the dictionary is just existing there.

08:49.690 --> 08:54.260
And then we can get the compound score so notice these first five labels they're all positive and it

08:54.270 --> 08:56.650
looks like the compound score is also all positive.

08:57.040 --> 09:02.620
So let's go ahead and based off this compound score do a little bit of logic and say if it's greater

09:02.620 --> 09:04.450
than zero then it's positive.

09:04.450 --> 09:10.090
If it's less than zero it's negative and then we'll compare these compound scores to the true labels

09:10.090 --> 09:18.670
that we already know so we're going to say one last column of our creation comp score and say it's equal

09:18.670 --> 09:27.760
to D F compound we're going to apply essentially a little bit of logic here.

09:27.810 --> 09:35.400
We'll take in that score and we'll say return back the string peo us for positive.

09:35.660 --> 09:43.760
If the score is greater than or equal to zero else return the string negative.

09:44.090 --> 09:48.950
Essentially changing the score into a string that matches our current label.

09:48.950 --> 09:53.010
So if we run this and then again check out the head of our data frame.

09:53.030 --> 09:54.670
Now we have a label.

09:54.770 --> 10:01.520
The review the full scores dictionary the compound offer that scores dictionary and our converted compound

10:01.850 --> 10:03.860
to a score or a label.

10:03.920 --> 10:06.980
P.S. so looks like we're matching up on the first five.

10:07.110 --> 10:13.520
But let's go ahead and have an overall report on the accuracy comparing the Vader compound score labels

10:13.880 --> 10:23.240
to the manual labels from this dataset and we can do that simply by saying from SDK learn that metrics

10:23.630 --> 10:31.820
import and we'll do an accuracy score a classification report as well as a confusion matrix.

10:31.910 --> 10:33.680
So we've already seen quite a few of these already.

10:33.680 --> 10:42.760
Let's first just get the accuracy score and we can do that by simply saying the F label sets the correct

10:42.820 --> 10:46.630
value compared to the F comp score.

10:46.630 --> 10:52.590
So essentially we're comparing how well that Vader perform against what was manually labeled.

10:52.600 --> 10:57.280
So this first label was manual label essentially a person read these reviews and decided whether or

10:57.280 --> 10:59.430
not they're positive or negative.

10:59.500 --> 11:04.270
So if we run their accuracy score we get an accuracy of zero point seven.

11:04.300 --> 11:09.400
If we were to randomly choose positives and negatives we'd be probably getting an accuracy score of

11:09.400 --> 11:10.720
around zero point five.

11:10.840 --> 11:15.460
So we can see we're doing better than random guessing which is quite good given the fact that we're

11:15.460 --> 11:20.130
essentially just running one line of code appear of inside the polarity scores.

11:20.140 --> 11:25.420
So it's definitely not bad considering how simple it is to run this process.

11:25.420 --> 11:30.650
Let's go ahead and print the classification report we'll say print classification report.

11:30.650 --> 11:38.480
And just like before we'll pass in the true label that we know and then our calculated comp score so

11:38.480 --> 11:45.500
we'll run this and then we can see our precision recall an F1 score and we can also compare it to versus

11:45.500 --> 11:47.000
negative versus positive.

11:47.000 --> 11:54.030
So it looks like the Vader has a little bit of trouble with negative reviews versus positive reviews.

11:54.110 --> 12:00.250
And if you take a look at some of these Amazon reviews some of these strings and some of the text is

12:00.260 --> 12:05.600
sometimes a bit hard to read and sometimes it's also sarcastic which means it's really hard to detect

12:05.740 --> 12:09.830
so sarcasm is almost impossible to detect for something like Vader.

12:09.830 --> 12:17.490
And then finally let's print out a confusion matrix so say confusion matrix and then D F label and pass

12:17.490 --> 12:19.880
in what Vader said.

12:19.910 --> 12:26.090
So the comp score here and here we can see our confusion matrix where we have the correctly classified

12:26.090 --> 12:30.160
instance a positive and negative versus the incorrectly classified.

12:30.290 --> 12:33.640
So you can see here again it's not performing well with the negative Varese.

12:34.120 --> 12:34.760
OK.

12:34.940 --> 12:40.520
So this does tell us that Vader correctly identified in Amazon review as positive or negative roughly

12:40.520 --> 12:43.020
around 71 percent of the time.

12:43.040 --> 12:46.450
That's definitely not bad considering how simple the process is.

12:46.550 --> 12:52.460
But it's also not excellent compared to maybe some state of the art deep learning methods for sentiment

12:52.550 --> 12:58.110
analysis but overall for simple import they can quickly apply to any text dataset.

12:58.130 --> 12:59.590
It's quite good.

12:59.600 --> 13:00.270
OK.

13:00.290 --> 13:03.890
Coming up next we'll go ahead and run a sentiment analysis project.

13:03.890 --> 13:04.460
We'll see you there.

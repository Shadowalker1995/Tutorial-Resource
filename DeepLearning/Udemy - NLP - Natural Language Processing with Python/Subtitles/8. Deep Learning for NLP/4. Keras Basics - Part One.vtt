WEBVTT

00:05.250 --> 00:09.280
Welcome back everyone to this lecture on Keres basics.

00:09.310 --> 00:13.660
We're going to learn how to create a very simple neural network for classifying the famous ires.

00:13.660 --> 00:19.830
They set the goal of this particular lecture is to just get you comfortable with the syntax with.

00:20.170 --> 00:25.930
The data itself contains measurements of flower pedals and steeples and corresponding labels to one

00:25.930 --> 00:27.060
of three classes.

00:27.160 --> 00:29.360
That is one of three flower species.

00:29.380 --> 00:30.610
It's a really simple data set.

00:30.620 --> 00:33.310
It's super famous has its own Wikipedia page.

00:33.430 --> 00:37.090
So we're going to be working on this just to learn the syntax basics.

00:37.090 --> 00:38.900
Let's open up a notebook and get started.

00:39.680 --> 00:41.090
OK here I am at a notebook.

00:41.140 --> 00:47.030
I want to start off by importing some pie as N.P. because we may need to use some pie later and then

00:47.030 --> 00:48.730
I'm going to read in the data set.

00:48.770 --> 00:54.020
The state is famous enough that it's actually built into sikat learn so I can simply say from S-K learn

00:54.920 --> 01:00.910
that data sets import and there is a load Irus function.

01:01.100 --> 01:11.020
So load underscore Irus and then I'm going to create a variable called Irus and then run the load Irus

01:11.030 --> 01:11.740
function.

01:11.810 --> 01:17.660
And if you take a look at what type of object is returned here it's actually a specialized Bunche object

01:17.660 --> 01:18.780
from sikat learn.

01:19.070 --> 01:24.650
And if you check its attributes it has feature names Target Target names and the data.

01:24.710 --> 01:30.110
So it's actually already kind of restructured for us and you can print out the description attribute

01:30.800 --> 01:33.640
and it prints out an entire description of what this is.

01:33.650 --> 01:38.120
So is essentially just as I mentioned simple links and widths and centimeters and then pedal lengths

01:38.150 --> 01:44.720
and pedal with in centimeters and the corresponding class there's 150 instances and there's four attributes

01:45.170 --> 01:51.560
and the flowers belong to a species of Iris it's either iris a Tosa Iris versicolor or Iris virginica

01:51.970 --> 01:55.480
and some some are sticks and then famous Iris dataset.

01:55.520 --> 01:56.800
Lots more info here.

01:57.170 --> 02:02.210
OK so let's actually now grab the feature information and the label information.

02:02.210 --> 02:07.780
It's actually already organized for us with attribute calls from the spunge object.

02:07.890 --> 02:15.040
So I will say my X features is equal to Irus data and if we take a look at X it's essentially a name

02:15.060 --> 02:18.530
pyrite with the four measurements lined up per instance.

02:18.690 --> 02:22.430
So people length's people with Pedda length and pedal with.

02:22.470 --> 02:24.150
So we have X ready to go.

02:24.150 --> 02:27.130
Now what we need to do is grab our labels.

02:27.210 --> 02:32.390
So that's also organized for us in the form of a target attribute.

02:32.520 --> 02:35.890
Essentially the target variable or label target.

02:35.950 --> 02:39.210
If you check out what Y looks like it's sorted.

02:39.210 --> 02:43.100
So we can see we have class 0 class 1 and class 2.

02:43.380 --> 02:47.790
Now because of the way carriers and your own that works work in general we're actually going to want

02:47.790 --> 02:52.080
to convert this to a categorical labeling system.

02:52.080 --> 02:53.730
Essentially what encoding.

02:53.970 --> 03:03.000
So what that means is I want a vector that's going to be zero for everywhere that this class doesn't

03:03.000 --> 03:03.840
match up.

03:03.840 --> 03:11.190
So for example class zero is going to be one index zero and then zero everywhere else for the other

03:11.190 --> 03:11.990
two classes.

03:12.150 --> 03:15.850
So essentially these become indexed additions 0 1 and 2.

03:15.870 --> 03:18.670
So a class one instance.

03:18.690 --> 03:24.580
So if we had class actually this would be class 0 it would look like this.

03:24.790 --> 03:31.250
And then class of one would end up getting a one at that that exhibition.

03:32.590 --> 03:41.800
And then a class of two would end up getting a one or one hand-coded at this and exposition.

03:41.820 --> 03:43.750
So this is known as one encoding.

03:43.920 --> 03:48.390
And we're going to be able to do this quite easily using some built in tools of course.

03:48.390 --> 03:53.480
So all we're doing is transforming each of these into its own little vector of three.

03:53.550 --> 04:02.320
And the way we do this is we say From cares that you tildes import and we import to underscore categorical.

04:02.640 --> 04:07.830
So we can say from Keres import to underscore categorical.

04:08.160 --> 04:08.910
There we go.

04:08.910 --> 04:10.790
And then we simply just cast it.

04:10.790 --> 04:15.450
We say why is not equal to two categorical Y.

04:15.690 --> 04:19.460
And if you check out the shape of y now it's 150 instances.

04:19.500 --> 04:22.190
Now it has three values per label.

04:22.410 --> 04:26.940
So if we actually take a look at what it looks like now you can see it's essentially the one high encoding

04:27.030 --> 04:28.320
so ones are 0.

04:28.380 --> 04:30.990
Then we move onto the next last 0 or 1 0.

04:30.990 --> 04:33.650
Move on to the next last and then 0 0 1.

04:33.720 --> 04:39.630
So now that we have set up for cares what we can do is continue on to split the data into a training

04:39.630 --> 04:41.390
set and a test set.

04:41.400 --> 04:50.270
So we're going to do is just psychic learn from S-K learn that model selection import train test split.

04:50.480 --> 04:56.630
Then we simply call train test split do shift tab as I previously mentioned that we can quickly copy

04:56.960 --> 05:01.480
the tuple that gets created essentially unpacking the values.

05:01.760 --> 05:10.340
So we will copy and paste that X train x test Y train Y test and then just pass it my X features my

05:10.340 --> 05:13.330
y label the sign on a test size.

05:13.400 --> 05:18.760
I'll go ahead and say 33 percent of the data will be testing data and then random state.

05:18.860 --> 05:21.250
If you want the exact same split as I will get.

05:21.290 --> 05:23.630
Go ahead and use 42.

05:23.670 --> 05:31.930
So we run that and now we have the four components the training data the testing features as well as

05:31.930 --> 05:34.490
the labels for the training data.

05:34.720 --> 05:37.220
And then the labels for the test data.

05:37.270 --> 05:41.100
Notice now everything's been shuffled and randomized which is exactly what you want.

05:41.230 --> 05:45.910
You don't want to train on things in an order otherwise you'll get really good at classifying zeros

05:45.910 --> 05:48.950
first and then when it comes to classifying ones you'll get really bad.

05:49.030 --> 05:54.040
So we always want to shuffle that data which is done automatically for us with a train test split which

05:54.040 --> 05:57.310
is why we have a random state because it is randomly shuffled.

05:57.410 --> 06:01.320
So we want to make sure you get the same shuffling I do now for neural networks.

06:01.330 --> 06:05.420
It's usually a really good idea to scale or standardized your data.

06:05.710 --> 06:11.020
So for this particular instance we kind of don't need to but it's a good idea anyways so we're going

06:11.020 --> 06:14.010
to show you how to do it occasionally do it for your own problems.

06:15.400 --> 06:22.950
Simply say for him as he learned that pre-processing and we import min max scaler and what this does

06:23.010 --> 06:29.160
is it simply makes all the values fit between a range like zero and one or negative to 1 to 1.

06:29.250 --> 06:34.260
And what this does is essentially divided by the max value to make sure everything fits between 0 and

06:34.260 --> 06:34.810
1.

06:35.040 --> 06:40.830
So what I mean by that is let's imagine you had a list of values like five 10 15 20.

06:40.860 --> 06:47.010
So if you took this list and then divided it by the max value so this would actually have to be an array.

06:47.070 --> 06:52.440
So if you took this array of values and then divided it by the max value which is 20 when you run that

06:52.620 --> 06:57.220
you'll notice now everything is between 0 and 1 because you divided by the max value.

06:57.240 --> 06:59.540
That's essentially what minimax scalar does.

06:59.550 --> 07:02.100
There's lots of other scaling methods but this one is pretty simple.

07:02.100 --> 07:08.540
So we're going to kind of run on this ideology we just simply create a scalar object just like you would

07:08.540 --> 07:11.270
for anything else with sikat learn.

07:11.870 --> 07:13.680
And then we're going to take her scalar object.

07:13.700 --> 07:17.470
Now we're going to fit it and we're going to fit it to only the training data.

07:17.720 --> 07:21.080
We don't fit it on all the data because that would essentially be cheating.

07:21.080 --> 07:24.560
We don't want to assume prior knowledge of the test data.

07:24.620 --> 07:30.740
So we're only going to fit it to the training features and then we're going to create skilled versions

07:30.740 --> 07:31.650
of our trained data.

07:31.940 --> 07:36.850
We will say scaled X train is equal to the scalar object.

07:37.100 --> 07:45.950
And then we will transform our X training data and we'll do the same thing for the test data we'll call

07:45.950 --> 07:51.760
scalar object and then we will transform our test data.

07:52.930 --> 07:55.470
So if we take a quick peek at the training data.

07:55.470 --> 07:58.990
Now you'll notice all the values are between 0 and 1.

07:59.110 --> 08:00.670
So that converts everything.

08:00.730 --> 08:07.060
And typically that helps neural networks helps the weights and biases from growing to large.

08:07.090 --> 08:09.590
OK so we have the skilled versions of our data.

08:09.780 --> 08:15.060
What we're going to do now is actually build a network of course which is actually a pretty simple process.

08:15.060 --> 08:24.240
We simply say from Karris that models import sequential essentially a bunch of sequences of layers and

08:24.240 --> 08:29.180
then we'll say from cares that layers and we're doing a really simple model here.

08:29.200 --> 08:34.680
So we're just going to import a dense layer later on when we actually perform things like text generation.

08:34.690 --> 08:36.630
These models are going to get a lot more complicated.

08:36.730 --> 08:41.860
We'll import different types of units like Ellis T.M. units but keep things simple just sequential and

08:41.860 --> 08:43.320
dense.

08:43.460 --> 08:46.140
And this is really the syntax we want to get familiar with.

08:46.250 --> 08:51.650
We first create the sequential model and then we simply add layers to the model.

08:51.650 --> 08:52.410
So we say model.

08:52.460 --> 08:58.700
And in this case only important one kind of led to an added layer and each particular layer has different

08:58.700 --> 09:05.680
parameters so dense we want to describe how many neurons or units go into that sense layer.

09:05.810 --> 09:07.700
What should the activation function be.

09:07.790 --> 09:11.680
And then you can do other things like have a bias initialiser et cetera.

09:11.690 --> 09:17.150
A kernel initialiser often those sort of initializers it's fine if the default value is the only thing

09:17.150 --> 09:19.610
you should probably mention is the input dimensions.

09:19.730 --> 09:20.910
So we'll do that as well.

09:21.820 --> 09:25.370
So I'll say dance let's go ahead and use 8 neurons.

09:25.510 --> 09:30.650
I'm just using double the amount of features whether or not that's incorrect or correct.

09:30.740 --> 09:32.460
Literally no right answer for that.

09:32.560 --> 09:37.390
Often people have some sort of domain knowledge that helps them understand what's a good choice for

09:37.390 --> 09:38.080
neurons.

09:38.140 --> 09:42.430
It should probably be something based off the number of features like some multiple of the features.

09:42.430 --> 09:48.870
But again there really is no right or wrong answer for that and they will say input them insuance is

09:48.870 --> 09:56.130
for since we expect for features and then we'll say the activation function is r e l you for a rectified

09:56.220 --> 10:00.550
linear unit and let's go ahead and add in one more then Slayer

10:03.380 --> 10:10.860
so we have that input layer another set of eight neurons and then finally for output layer we're going

10:10.860 --> 10:17.880
to add in three neurons and it's going to have the soft Max activation.

10:18.410 --> 10:24.260
And the reason for that is essentially that three neurons what's going to happen is each neuron is going

10:24.260 --> 10:30.170
to have what looks like essentially a probability for belying in any particular class which is why we

10:30.170 --> 10:31.340
want encoded.

10:31.430 --> 10:33.270
So if we remember what we looked like.

10:33.560 --> 10:34.820
It looks something like this.

10:34.820 --> 10:40.370
So at the end the neurons get to return maybe like 30 percent chance for this exploration a 20 percent

10:40.370 --> 10:45.430
chance for this exploration and then 50 percent chance for this actually correct an expedition.

10:45.440 --> 10:51.380
So that's why at the end we have these three neurons and that's what we choose soft max.

10:51.380 --> 10:57.020
So kind of get some output that kind of looks something like this.

10:57.020 --> 10:58.370
It won't look exactly like this.

10:58.400 --> 11:02.900
Such nice probabilities but this is the general idea is going to tell you what it's leaning towards

11:02.900 --> 11:04.390
based off the expedition.

11:04.550 --> 11:06.250
And that's why we use the soft max.

11:06.530 --> 11:12.910
So at the end of that we simply compile our model.

11:12.990 --> 11:14.630
We choose some sort of loss.

11:14.640 --> 11:18.870
This loss is going to depend on what you're actually performing.

11:19.080 --> 11:29.600
So here we are performing categorical process so we want categories to say categorical underscore cross

11:30.230 --> 11:31.000
entropy.

11:31.250 --> 11:38.800
So that's how we're actually going to determine loss the optimizer will use the atom optimizer.

11:39.030 --> 11:45.240
And then for the metrics we're going to decide this on accuracy.

11:45.620 --> 11:49.030
So that's what we'll be training on and that's we'll be reporting back.

11:49.190 --> 11:54.030
Now if everything worked well if your model you should be able to then ask for a summary of the model

11:54.570 --> 12:00.180
which reports back where each layer is doing what the output shape and the number of parameters created.

12:00.180 --> 12:00.630
All right.

12:00.780 --> 12:06.330
So if you get anything for this lecture it really should be these few lines you import sequential models

12:06.450 --> 12:11.180
and then import the layers you want create your model and then add in the layers and the Pentagon what

12:11.180 --> 12:17.340
kind of import you may have to kind of tune these hyper parameters or the side more hyper parameters.

12:17.580 --> 12:20.690
And then at the end you compile it with some sort of choice of loss.

12:20.820 --> 12:24.780
And this last depends on exactly what kind of problem you're trying to solve.

12:25.730 --> 12:31.950
Then once you have that just like a psychic learn model you simply call models that fit on your training

12:31.950 --> 12:32.710
data.

12:32.790 --> 12:38.470
And remember we scale their training data so Passons scaled X train and then Y train.

12:38.730 --> 12:43.840
We don't need to scale train because it's labels that are already zeros and ones.

12:43.940 --> 12:50.170
So we're going to say pocks and pocks one epoch is running through the entire training data set.

12:50.390 --> 12:55.080
Well go ahead and run through 150 epochs and then there's a verbose.

12:55.080 --> 12:58.740
Verbose is just basically how much output you want information.

12:58.740 --> 13:01.620
So if you put in zero it's not going to report back anything.

13:01.770 --> 13:05.070
If you put in one ill actually show you like a little progress bar.

13:05.220 --> 13:10.200
So two it shows you the pockets on how much loss and what its current accuracy is.

13:10.200 --> 13:14.130
So it's go ahead and run this for 150 POCs this should be relatively fast.

13:14.130 --> 13:15.240
I'm on a really fast computer.

13:15.260 --> 13:17.210
But again the data sets are pretty small.

13:17.280 --> 13:20.090
So at the end of this you should get something around.

13:20.280 --> 13:21.820
I would say 90 percent.

13:21.830 --> 13:26.070
So it may not be exactly this because remember there is some random initialization to the weights and

13:26.070 --> 13:29.410
biases but you should be getting around 90 percent.

13:29.580 --> 13:32.820
You may get slightly higher slightly lower after 150 pucks.

13:32.820 --> 13:33.270
OK.

13:33.570 --> 13:39.920
So another question is how do we predict on new unseen data and how do we evaluate the model performance.

13:39.990 --> 13:42.820
We're going be covering that in the very next lecture.

13:42.840 --> 13:43.350
We'll see if there.

WEBVTT

00:00.720 --> 00:07.380
Welcome back everyone to this lecture on Ellis T.M. and G are you so a common issue that recurrent neural

00:07.380 --> 00:13.260
networks face is that after a while especially if you're training the network on a really large sequence

00:13.650 --> 00:19.920
then the recurring neural network is going to begin to forget those very first inputs the later and

00:19.920 --> 00:24.900
later training batches of later inputs things that come towards the end of the text document that's

00:24.900 --> 00:30.240
going to start kind of overwriting the weights of the very beginning inputs of the beginning of the

00:30.240 --> 00:31.200
text.

00:31.200 --> 00:36.690
So we want to make sure is that we're not beginning to forget those first inputs as information is lost

00:36.900 --> 00:39.460
that each step going through their current neural network.

00:39.480 --> 00:43.730
So what we really need is we need some sort of long term memory for our networks.

00:43.720 --> 00:48.990
We've balanced both the short term memory of the networks the data that it was recently trained on versus

00:48.990 --> 00:54.510
the long term memory of the networks all the data and the very first data that was being trained on.

00:54.820 --> 01:00.240
So the Ellis team or long short term memory cell was created to help address these recurrent neural

01:00.240 --> 01:01.410
network issues.

01:01.410 --> 01:07.830
And while long short term memory cells aren't exactly state of the art they're still quite new considering

01:07.830 --> 01:12.420
that they've really only been around in Paris for just the past few years.

01:12.480 --> 01:15.960
So we're gonna do is go through how an Elysium cell works.

01:15.960 --> 01:18.380
Keep in mind there's actually a lot of mathematics here.

01:18.390 --> 01:22.530
There's really just a mathematical theory lecture where we're actually dealing with Caris it's gonna

01:22.560 --> 01:24.510
be a simple import call.

01:24.660 --> 01:27.180
So check out the resource link for a fool.

01:27.210 --> 01:32.250
Really great breakdown in fact that's a super common link when discussing Ellis Thames and then we'll

01:32.250 --> 01:35.110
also discuss the related G are you sell.

01:35.220 --> 01:40.380
But for our use case I really want you to focus on Ellis Tam because that's what's most commonly used

01:40.680 --> 01:41.860
for tech generation.

01:41.940 --> 01:47.980
So let's get started learning how this Ellis Tam cell actually works but quickly recall that a typical

01:47.980 --> 01:53.250
recurrent neuron basically has this sort of structure where we have some input at T minus one.

01:53.380 --> 02:00.930
Then we have the output a T minus one and that output is also fed back in along the input at T so as

02:00.930 --> 02:06.630
a quick note these outputs are often called Hidden so we can have instead of saying output at T minus

02:06.630 --> 02:06.960
one.

02:06.950 --> 02:12.420
We can say h of T minus one and then that also gets fitted with the input at t that gets passed through

02:12.420 --> 02:15.300
some sort of activation function like the hyperbolic tangent.

02:15.450 --> 02:21.210
Then that gives the output h of T etc. So when you see h of t just kind of think of that as the typical

02:21.240 --> 02:23.700
output of a recurrent neuron cell.

02:23.790 --> 02:29.430
All right so here we can see the entire model of a long short term memory cell and I know it can look

02:29.430 --> 02:34.140
really complicated if you see it in this format but it's actually not so bad when you break it down

02:34.170 --> 02:35.470
in parts.

02:35.610 --> 02:40.650
So let's go ahead and first take a look at what's being input and what's being output it here.

02:40.680 --> 02:44.120
We still have those original inputs from a normal recurrent neuron.

02:44.130 --> 02:47.160
Those h of T minus one and exit T.

02:47.160 --> 02:51.260
But note here we have this third input and we'll call that the cell state.

02:51.330 --> 02:57.360
So right now we're receiving the cell state at T minus one and then the outputs we again output h of

02:57.360 --> 03:00.120
T and then we also have the cell states.

03:00.150 --> 03:02.490
That's the current cell state C of T.

03:02.490 --> 03:07.830
So what we're going to end up doing is we're gonna take in h of T minus one and exit T as well as the

03:07.830 --> 03:14.670
previous cell state C of T minus one and we'll output h of T as well as the current cell state C of

03:14.670 --> 03:15.330
T.

03:15.330 --> 03:22.890
So we're gonna do this step by step so the very first step is called the forgets Gates layer and this

03:22.890 --> 03:29.010
is going to be the first step or we decide what information are we going to forget or throw away from

03:29.010 --> 03:30.450
the cell state.

03:30.450 --> 03:38.040
So what we end up doing here is you pass an h of T minus one and x of T and we pass that in after performing

03:38.040 --> 03:42.980
a linear transformation with some weights and biased terms into a sigmoid function.

03:42.990 --> 03:48.120
And remember because this is essentially a sigmoid layer it's always going to output a number between

03:48.180 --> 03:55.420
0 and 1 and a 1 is going to represent to keep it and a 0 is going to represent to forget about it or

03:55.440 --> 03:56.660
get rid of it.

03:56.670 --> 04:02.430
So if we think back to maybe a language model where we're trying to predict the very next word based

04:02.430 --> 04:08.550
on previous ones a cell state might include the gender of the present subject so that you in picking

04:08.550 --> 04:13.830
the correct pronoun so when you end up seeing a new subject you want to forget about the gender of the

04:13.890 --> 04:14.870
old subject.

04:14.910 --> 04:22.720
So that may be a use for a forget gate layer when working with a natural language as a sequence.

04:22.800 --> 04:28.640
Now the very next step is to decide what new information are you going to store in the cell state.

04:28.740 --> 04:30.580
So we order the what we're going to forget.

04:30.630 --> 04:34.300
Now we need to decide what are we actually going to store in the cell state.

04:34.320 --> 04:36.330
Remember that's C of T.

04:36.450 --> 04:38.970
So the first part is a sigmoid layer.

04:39.000 --> 04:41.480
And the second part is a hyperbolic tangent layer.

04:41.670 --> 04:45.120
So let's go ahead and tackle that first part that sigmoid layer.

04:45.330 --> 04:48.420
So let's sigmoid layer is called the input gate layer.

04:48.450 --> 04:52.230
So we'll say that's equal to I F T for input gate layer.

04:52.230 --> 04:57.180
And again we take an h of T minus 1 and x of t do a linear transformation on it.

04:57.180 --> 05:03.150
With they'll be a vi plus b a vi we pass that into a sigmoid function and again now we have a bunch

05:03.150 --> 05:06.250
of values between zero and ones.

05:06.270 --> 05:11.820
Then the second part of this is the hyperbolic tangent layer and that's again going to take h of T minus

05:11.820 --> 05:17.520
one and exit t do that linear transformation and then pass it through a hyperbolic tangent so that ends

05:17.520 --> 05:21.330
up creating a vector of what we call new candidate values.

05:21.330 --> 05:24.150
So that's that C of T with a little tilt above it.

05:24.600 --> 05:29.060
So these are candidate values that could be added to the state in the next step.

05:29.070 --> 05:33.900
We're going to combine these two to create an update to the cell state.

05:33.900 --> 05:38.910
So if we think back to an example of a language model we essentially want to add the gender of the new

05:38.910 --> 05:45.920
subject to the cell state and replace the old one that we've already decided we're forgetting so now

05:45.920 --> 05:48.560
it's time to update the old cell state.

05:48.590 --> 05:54.110
Remember the old cell state is C of T minus one and we eventually want to update that to the new cell

05:54.110 --> 06:01.080
state C of T so we can pass that on to the T plus one state of the cell.

06:01.460 --> 06:06.050
So in the previous steps we already decided what we're going to forget and we also already decided what

06:06.050 --> 06:07.190
we're going to store.

06:07.310 --> 06:11.670
So now what we need to do is just perform or execute these actions.

06:11.720 --> 06:19.100
So what we end up doing is we multiply the old state C of T minus one by F of T so we end up forgetting

06:19.100 --> 06:21.910
the things that we essentially decided we're going to forget.

06:22.010 --> 06:29.000
Based off that first sigmoid layer then we're going to add I have t the input gate layer times those

06:29.000 --> 06:31.400
candidate values C of t with tilled on top.

06:31.940 --> 06:37.130
So these are the new candidate values and now they're being scaled by how much we decided to up they

06:37.220 --> 06:39.050
each state value.

06:39.050 --> 06:43.790
So if we think back to a case of a language model this is where we would actually drop the information

06:44.090 --> 06:50.330
about that old subject's gender and add in new information based off what we decided in the previous

06:50.330 --> 06:51.850
steps.

06:51.860 --> 06:56.590
Now our final decision is going to be what do we output for h of T.

06:56.840 --> 06:59.500
So this output is gonna be based off your cell state.

06:59.540 --> 07:01.450
That is just a filter diversion.

07:01.550 --> 07:03.080
So it's actually pretty straightforward now.

07:03.080 --> 07:10.310
We just using h of T minus 1 and x of t we pass that after a linear transformation into the sigmoid

07:10.370 --> 07:15.720
layer and that's going to decide what parts of the cell state we're going to be outputting.

07:15.730 --> 07:21.560
Then we put the cell state through a hyperbolic tangent so that's gonna push the values to be between

07:21.650 --> 07:27.890
negative 1 and the 1 and we're going to then multiply it by the output of that sigmoid gate so that

07:27.890 --> 07:30.520
we only output the parts we decided to.

07:30.530 --> 07:33.180
So that's what's occurring here in this red line.

07:33.190 --> 07:38.390
Again we're taking h of T minus one and exit t doing a linear transformation on it passing it through

07:38.390 --> 07:44.090
the sigmoid and then once we have that output so we're going to say that's o of T output of T We're

07:44.090 --> 07:51.020
then going to multiply by the hyperbolic tangent of the C of T or that current cell state and that gives

07:51.020 --> 07:52.480
us h of T.

07:52.670 --> 07:56.610
So that's how an Ellis team cell works.

07:56.740 --> 07:59.360
Now there's different variants on an Alice T.M. cell.

07:59.380 --> 08:02.950
So there's a variance called the peephole variance.

08:03.040 --> 08:05.440
So this is actually quite popular.

08:05.530 --> 08:09.970
And the reason for that is because it adds peepholes to all the gates.

08:09.970 --> 08:20.200
So basically it allows these FTI of T and o of t to be able to see the previous cell state or C of T

08:20.200 --> 08:21.000
minus one.

08:21.040 --> 08:26.950
So you can see here now instead of being a function of just h of T minus one an X of T we're also passing

08:26.950 --> 08:32.530
in C of T minus one and then for the output we're also passing in a sea of T directly.

08:32.650 --> 08:39.340
So that's called an Ellis team with peoples another variation of the Ellis T.M. cell is called the gated

08:39.340 --> 08:41.260
recurrent unit or G are you.

08:41.350 --> 08:46.540
And this was actually introduced quite recently around 2014 and what it ends up doing is it actually

08:46.540 --> 08:53.260
simplifies things a bit by combining the forget and input gates into a single what they call update

08:53.260 --> 08:53.880
gates.

08:53.950 --> 08:58.110
And it also merges the cell state and hidden state and it makes a few other changes.

08:58.120 --> 09:04.450
So this resulting model is actually simpler than standard Ellis models and because of that it's been

09:04.450 --> 09:08.550
growing increasingly popular over the last few years that it's been around.

09:08.650 --> 09:13.600
So there's actually lots of other slight variations off of this and they're being introduced all the

09:13.600 --> 09:19.630
time there was another one called depth gated recurrent neural networks and that was introduced in 2015.

09:19.780 --> 09:24.310
And I mean who knows maybe in a few years we'll see another variation that further improves on this

09:24.310 --> 09:25.060
model.

09:25.060 --> 09:31.300
But the main idea is to understand how Ellis teams work and that allows you to quickly learn how these

09:31.300 --> 09:32.680
variations work off of it.

09:32.770 --> 09:33.140
All right.

09:33.190 --> 09:39.350
Now that we understand Ellis teams and G use we can now use them with Caris.

09:39.370 --> 09:44.870
Now fortunately Caris has a super nice API that makes Ellis Tam and RNA and really easy to work with.

09:45.040 --> 09:50.130
And typically from the research I've done protects generation Ellis teams work best.

09:50.140 --> 09:51.860
So that's what we're going to be using.

09:51.940 --> 09:56.960
Coming up next we're gonna take a several part process in order to learn how to format our data.

09:56.980 --> 09:59.430
That is our text data for recurrent neural networks.

09:59.440 --> 10:04.060
And again Caris has a lot of convenient built in tools for this and then we'll see how to use Ellis

10:04.060 --> 10:05.940
TAM for tech's generation.

10:05.980 --> 10:08.830
This is a really awesome project so I hope you're super excited for it.

10:09.190 --> 10:10.240
I'll see you at the next lecture.

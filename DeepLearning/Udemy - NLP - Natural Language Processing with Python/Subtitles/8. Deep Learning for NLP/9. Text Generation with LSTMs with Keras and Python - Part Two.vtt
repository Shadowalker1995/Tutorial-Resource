WEBVTT

00:05.560 --> 00:10.210
Welcome everyone to part to touch generation with Python and Cara's.

00:10.340 --> 00:14.410
So for this lecture we're going to show you how to create the TAM based model.

00:14.540 --> 00:18.170
But before we do that we want to split the data into features and labels.

00:18.320 --> 00:22.550
So we're going to have our X features which is the first N-words of the sequence and then we'll have

00:22.550 --> 00:26.220
our white label which is the very next word after that entire sequence.

00:26.270 --> 00:30.710
Once you've done that we'll be able to fit the model on that feature to predict the next word in the

00:30.710 --> 00:31.560
sequence.

00:31.880 --> 00:33.850
OK let's jump to the notebook.

00:34.150 --> 00:38.910
OK so here we are where we left off last time with these actual sequences in a matrix format.

00:38.990 --> 00:45.440
What I want to do is perform a train test split and the train display is going to go ahead and separate

00:45.950 --> 00:48.780
these first columns as features.

00:48.890 --> 00:52.100
And then this last column as the target we want to predict.

00:52.100 --> 00:54.890
So we're going to do a couple of things here.

00:55.100 --> 01:02.800
We'll say from Kurus dot utils import to categorical.

01:03.380 --> 01:10.580
And again we're going to then take our sequences and grab everything but the last words.

01:10.780 --> 01:14.440
So we'll say for all rows starting from the very beginning.

01:14.490 --> 01:19.390
Grab everything but that very last column.

01:19.390 --> 01:21.150
So again if I take a look at what this means.

01:21.160 --> 01:25.980
Notice we have 14 24 9 6 5 here that's exactly we have here on the end.

01:26.080 --> 01:30.070
So essentially grabbing for every row all the columns except the very last column.

01:30.100 --> 01:33.010
That's what the slice notation is doing with no pie.

01:33.010 --> 01:36.610
So these are the first X-amount of words of your sequence length.

01:36.670 --> 01:39.040
In this case we happened to choose a sequence length 25.

01:39.040 --> 01:42.890
Again that's something you can edit as you go along and play this yourself.

01:42.940 --> 01:49.420
But here in this case we have the first twenty five words and the very last word of these sequences

01:50.490 --> 01:52.570
essentially is going to be that last column.

01:52.600 --> 01:56.960
So we're going to say OK grab all the rows and I'll just grab that last column.

01:57.250 --> 02:00.270
So that is 24 9 6 5 5 and so on.

02:00.310 --> 02:04.760
So notice what the values are here are the original sequences 24 9 6 5 and so on.

02:04.870 --> 02:13.510
And then 2 2 7 0 9 26 and here we have 2 2 7 9 26 so hopefully that's clear that now I can simply say

02:13.900 --> 02:17.700
X are my features those first X-amount of words.

02:17.830 --> 02:20.710
And then the label is what I'm looking for.

02:20.710 --> 02:29.920
So in this case that very last word and what I'm going to do is then change that y into a two categorical.

02:29.920 --> 02:35.900
So I'll say why is equal to two categorical Y.

02:35.960 --> 02:44.240
And then the number of classes is simply the vocabulary size we discussed earlier plus one because of

02:44.240 --> 02:45.680
the way Keres padding works.

02:45.680 --> 02:49.950
It needs an extra one to hold eight zero.

02:49.980 --> 02:58.300
OK so we passed that into two categorical and then to set our sequence length we simply asked well what

02:58.300 --> 03:01.450
was the shape of x.

03:01.640 --> 03:03.010
Index 1.

03:03.010 --> 03:08.830
So if I take a look at the shape of my features right now it's eleven thousand three hundred sixty eight.

03:08.920 --> 03:12.420
That means I have 11000 368 sequences.

03:12.460 --> 03:18.640
So those were essentially those shifted 25 words sentences and then how many words are in each sentence

03:18.640 --> 03:20.290
will I know it's 25.

03:20.290 --> 03:23.710
So that means my sequence length is this right here 25.

03:23.710 --> 03:28.270
So the reason we said it using X thought shape is that where you can later come back up here and then

03:28.270 --> 03:32.160
play around with the 2005 value if you come back up here.

03:32.170 --> 03:35.520
Remember we chose our training length to be 25 plus one.

03:35.530 --> 03:36.840
That one we're going to predict.

03:36.850 --> 03:41.800
So if you play around with that value that should be taken care of automatically as you come down here

03:41.950 --> 03:43.510
and then play around the shape.

03:43.570 --> 03:49.840
So now I have my sequence length expected is 25 predict the very next word after those 25 words.

03:50.050 --> 03:54.910
Now that we've organized our data and done a trained split on the data it's time to actually create

03:54.910 --> 03:55.850
the model.

03:56.250 --> 04:04.330
And for this we're simply going to say from cars that models import sequential and then from carious

04:04.540 --> 04:11.990
that lawyers import and we'll be importing a dense layer in Ellis T.M. layer to the all the sequences

04:12.260 --> 04:15.840
and in embedding layer to deal with the vocabulary.

04:15.860 --> 04:23.970
And we're going to functionals this so I'll say T.F. we'll call it create model and it takes in a vocabulary

04:23.970 --> 04:28.360
size as well as that sequence length that we just defined earlier.

04:30.170 --> 04:37.310
Well start off as we always do creating an instance of that sequential model and then we will add to

04:37.310 --> 04:38.110
this model.

04:38.180 --> 04:46.440
So we'll say embedding and we'll pasan a vocabulary size passen the sequence length.

04:46.440 --> 04:49.740
In fact we can just pasand what we passed and their sequence length.

04:49.860 --> 04:55.960
And then again we're going to define that the input length is equal to the sequence length.

04:56.010 --> 05:00.780
So it's actually going on here for this embedding is we're defining an input dimension for the embedding

05:00.840 --> 05:03.240
which is the entirety of the vocabulary.

05:03.240 --> 05:07.170
We're also defining the output dimension which is just the sequence length.

05:07.170 --> 05:11.750
And then we're defining the input length which is also equal to the sequence length.

05:12.040 --> 05:12.520
OK.

05:12.690 --> 05:16.870
So again that's what the embedding is doing and if front to figure out more about it it's really all

05:16.880 --> 05:25.010
it's doing here is it turns positive integers or indexes into a dense vectors of a fixed size.

05:25.050 --> 05:30.670
So essentially transforming something like this into this fixed size based vocabulary.

05:30.960 --> 05:34.340
And this has to be the first layer in the model OK.

05:34.400 --> 05:38.900
And you can see more examples of all this embedding essentially just allowing us to deal of that text

05:38.900 --> 05:39.610
data.

05:40.050 --> 05:47.030
OK so we have the imbedding and now we can add in some LACMA units to actually be trained on the sequence.

05:47.120 --> 05:53.500
So at Ellis T.M. and that LACMA you decide how many neurons are going to go in there or how many units.

05:53.550 --> 05:59.240
Again no right answer here but it's usually a good idea to make some sort of multiple off of your sequence

05:59.240 --> 06:00.130
length.

06:00.170 --> 06:06.030
So I'm going to go and go a little larger on this or just for the purposes of video.

06:06.050 --> 06:07.110
I'll go 150.

06:07.250 --> 06:10.180
But we provided some models are trained with more neurons.

06:10.190 --> 06:13.780
Again the more neurons we add here it's going to take slightly longer to train.

06:13.790 --> 06:17.000
So let's just go with two acts or sequence length so 15.

06:17.120 --> 06:21.650
And in fact if you really wanted to you could simply find this based off your sequence length something

06:21.650 --> 06:26.470
like sequence sequence length times 2 or something like that for the number of neurons.

06:26.780 --> 06:27.860
It's really up to you.

06:28.400 --> 06:30.240
We'll decide on 15 year olds.

06:30.290 --> 06:33.450
This will give us the best results but it should train a lot faster.

06:33.710 --> 06:39.940
And then we're going to ask for the return sequences to be true.

06:40.180 --> 06:46.690
And then we can add in another layer of elist T.M. again choosing some out of neurons for that one.

06:47.850 --> 06:50.820
And then at the end we're going to add in.

06:51.190 --> 06:58.450
But then Slayer this one you probably want to be a little larger but we'll go ahead and do 50 this network

06:58.450 --> 07:01.960
itself probably will give you such great results since it's a little smaller but they also want to take

07:01.960 --> 07:03.000
forever to train.

07:03.340 --> 07:05.820
In general it's going to take a really long time to train.

07:05.820 --> 07:09.000
I think the one we provide you took about three hours to train.

07:09.130 --> 07:13.880
So we have that's activation rectified linear unit at the very end.

07:13.960 --> 07:18.480
We want to translate this into a vocabulary word because it's just going to spit out a number.

07:18.640 --> 07:28.090
So we want a dense network of our vocabulary size where the activation is soft Max and the less we need

07:28.090 --> 07:32.900
to do here is simply say model and we're going to compile this.

07:33.010 --> 07:39.680
We're going to give a loss of categorical cross entropy categorical underscore cross entropy.

07:39.910 --> 07:42.750
Be careful not to make a typo here it's really easy to do so.

07:43.510 --> 07:47.700
And basically that means we're treating each vocabulary word as its own individual category.

07:49.200 --> 07:56.660
When you add the final optimizer leaves the atom optimizer and then for metrics will use accuracy.

07:58.110 --> 08:04.750
Then once that's done go ahead and give a summary of your model such a print out a little summary and

08:04.750 --> 08:06.370
then return your model.

08:06.370 --> 08:07.720
So we have a create model function.

08:07.720 --> 08:11.430
Let's go ahead to make sure we passed it incorrectly.

08:11.440 --> 08:18.040
So we'll say create model and we'll pass in our vocabulary size that we define earlier.

08:18.060 --> 08:23.970
Remember this vocabulary size we just define appear previously using the length of the word counts.

08:23.980 --> 08:26.250
So how many unique words are there.

08:26.300 --> 08:31.780
So 2 7 0 9 that particular case because of the way the embedding works with padding that we'll see later

08:31.780 --> 08:33.060
on for Pead sequences.

08:33.280 --> 08:36.320
We actually need to add one more space to essentially hold a zero.

08:36.340 --> 08:42.720
So we're going to see if we'll cover size plus one and then we're going to pasan our sequence length.

08:42.880 --> 08:44.890
And this is going to be our model.

08:45.130 --> 08:48.640
Go ahead and run this and make sure you get this summary output.

08:48.640 --> 08:52.310
If you got an error here it probably means you made a typo somewhere along here.

08:52.330 --> 08:55.690
In that case just go ahead and copy the function we created for you in the notebooks.

08:55.780 --> 08:58.480
But really easy to make a typo especially on this compile line.

08:58.900 --> 09:01.220
But if you saw the summary then you are good to go.

09:01.530 --> 09:01.990
OK.

09:02.290 --> 09:05.940
So coming up next we're going to generate new text.

09:06.010 --> 09:09.570
So the last thing to do here is actually train and fit our model.

09:09.850 --> 09:12.490
So what we do is we're going to save from pixel

09:15.590 --> 09:22.040
import dump Khama load that's going to allow us to actually save the file and then load it up later

09:22.040 --> 09:23.600
on.

09:23.600 --> 09:29.720
So we're going to say from pickle import dump load and then say model that fit are going to fit on our

09:29.750 --> 09:40.750
XT data in our y data here here we'll say batch size and then we'll say.

09:41.410 --> 09:43.220
The amount of epoxy we want to train for.

09:43.230 --> 09:47.910
I'll go ahead and just train for two epochs in this video and then say verbose is equal to one.

09:47.920 --> 09:49.510
So explain what's going on here.

09:50.540 --> 09:54.860
Now previously I was using the terms train test split it wasn't really a train split it was actually

09:54.860 --> 09:55.930
features labels split.

09:55.940 --> 10:00.900
So I apologize for using those words but essentially we have X and Y we don't truly have a train to

10:00.930 --> 10:01.850
split with testate.

10:01.880 --> 10:07.610
Because there's nothing to Testament's there's kind of no right answer as far as what text generated

10:07.610 --> 10:08.670
should look like.

10:08.810 --> 10:15.950
Instead we are really just texting or testing these features against the predicted label the batch size

10:16.040 --> 10:18.580
is how many sequences you want to passen at a time.

10:18.710 --> 10:20.600
You don't want to pasan everything at a time.

10:20.600 --> 10:23.200
Otherwise the neural network won't be able to handle that.

10:23.210 --> 10:25.400
So you only want to pass on a certain amount of sequences.

10:25.690 --> 10:29.810
128 was a value that I kind of just chose arbitrarily and worked well for me.

10:29.820 --> 10:34.640
E POCs to you POCs is not going to be nearly enough to generate any text that makes sense so you may

10:34.640 --> 10:37.870
just see a bunch of the most common words like the other repeated.

10:37.880 --> 10:42.820
But go ahead and treat it on a little bit of pocks just so you can tell that it worked or not and then

10:42.820 --> 10:45.260
verbose one is just going to be the output port.

10:45.260 --> 10:46.310
Let's go with this.

10:46.340 --> 10:48.120
Make sure it runs like this.

10:48.140 --> 10:49.750
Mostly just to check if it runs.

10:49.790 --> 10:55.600
You should probably be training for at least like 200 POCs to get something that's reasonable.

10:55.670 --> 10:58.610
So if you're able to run this and this should be good.

10:58.640 --> 11:01.190
Again our accuracy is absolutely horrible which makes sense.

11:01.190 --> 11:08.330
Right now it's probably just predicting the word to be the most common word but once done training especially

11:08.330 --> 11:15.050
if you trained for a really long time you're going to want to save this so you're going to want to take

11:15.050 --> 11:26.900
the model and save it and say something like my underscore Moby-Dick model thought age 5.

11:27.060 --> 11:32.390
So that saves the model but the other thing we're actually going to say is the tokenizer as well.

11:32.550 --> 11:36.910
And in order to do that we're going to use that pixel functionality.

11:36.920 --> 11:39.470
So we're going to say this is a pickle file.

11:39.470 --> 11:43.330
Now we're going to save in our tokenizer memory that tokenizer has information across the entire vocabulary

11:43.340 --> 11:44.200
like word counts.

11:44.210 --> 11:49.750
We'll need that later on though we don't get into tokenize text again so I'll say dump tokenizer.

11:49.940 --> 11:58.660
And this is where you would make up a name for it tokenizer like my simple tokenizer and then say WB

11:58.660 --> 11:59.060
here.

11:59.080 --> 12:04.240
So what this does is it will automatically create a file for you called my simple tokenizer and then

12:04.240 --> 12:09.340
write binary to it in order to dump this tokenizer object to that file.

12:09.340 --> 12:14.980
Notice there's no extension there saying go ahead and run that as well and save it and now you successfully

12:15.460 --> 12:22.790
train your model and then you save it as well as saved your tokenizer.

12:22.910 --> 12:25.350
Coming up next we're going to show you how to generate new text.

12:25.370 --> 12:25.890
We'll see it there.

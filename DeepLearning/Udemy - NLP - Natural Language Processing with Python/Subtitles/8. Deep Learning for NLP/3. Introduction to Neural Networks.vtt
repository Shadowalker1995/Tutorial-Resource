WEBVTT

00:05.720 --> 00:12.740
Welcome back everyone to an introduction to neural networks we've seen how a single perceptual behaves.

00:12.750 --> 00:16.650
Now let's expand on this concept to get the idea of a neural network.

00:16.650 --> 00:22.910
Let's see how to connect many perceptions together and then how to represent this mathematically.

00:22.930 --> 00:25.840
So what does a multiple perceptions network actually look like.

00:25.840 --> 00:27.540
Well essentially looks like this.

00:27.640 --> 00:32.890
Here we can see that we have various layers of single perceptions connected to each other through their

00:32.890 --> 00:38.810
inputs and outputs in this case we have an input layer all the way on the left which is purple.

00:38.810 --> 00:44.210
We have two hidden layers and hidden layers are the layers that are in-between the input layer and all

00:44.210 --> 00:49.370
the way on the right the output layer essentially hidden layers or layers that don't get to see the

00:49.370 --> 00:49.970
outside.

00:49.970 --> 00:56.060
That is all the way the inputs on the left or all the way the outputs on the right so input layers.

00:56.070 --> 00:57.660
Those are real values from the data.

00:57.670 --> 01:00.710
So they take in the actual data as their input.

01:00.790 --> 01:04.950
Then you have hidden layers and those are the layers between the input and output layers.

01:05.020 --> 01:10.990
And if you have three or more hidden layers that's considered a deep network and then an output layer

01:11.080 --> 01:17.820
where you have some sort of final estimate of whatever the output that you're trying to estimate is.

01:17.840 --> 01:22.520
So as you go forwards through more layers the level of abstraction increases.

01:22.790 --> 01:29.900
Let's not discuss the activation function and a little more detail previously our activation function

01:29.900 --> 01:33.580
was just a really simple function the output 0 or 1.

01:33.620 --> 01:38.600
Remember we were just taking the sum of the inputs in that simple perception model and then had an activation

01:38.600 --> 01:44.630
function that just had an output of either zero or one based off whether the number was positive or

01:44.630 --> 01:45.290
negative.

01:45.380 --> 01:47.120
And if we plot it out that looks like this.

01:47.120 --> 01:54.510
We can say the output is either 0 or 1 on the y axis and then the x axis we see are marking at zero.

01:54.620 --> 01:57.310
And if we have a negative value we output zero.

01:57.320 --> 02:04.730
And if we have a positive value we output one and we did this by having the x axis be w x plus B that

02:04.730 --> 02:07.970
is the weights times the inputs plus that bias.

02:07.970 --> 02:10.270
And it's really common to just set that equal to Z.

02:10.520 --> 02:15.150
So you can refer to Z instead of having to concentrate for it to w x plus B.

02:15.200 --> 02:22.390
So whenever I say C I'm essentially referring to those weights times those inputs plus that bias so

02:22.630 --> 02:27.190
unfortunately this is actually a pretty dramatic function since small changes aren't really reflected

02:27.550 --> 02:30.730
so you can have kind of really small changes in Z.

02:30.790 --> 02:36.420
For instance the change from zero point five and Z to zero point six doesn't really matter it's still

02:36.420 --> 02:40.960
going to output one as long as this positive or the change that are really dramatic from negative 1

02:40.960 --> 02:43.160
to negative 1000 doesn't really matter.

02:43.180 --> 02:48.340
It's gonna be zero so it'd be nice if we could have a more dynamic function.

02:48.350 --> 02:54.680
For example this red line here you can see here that our outputs now are no longer just 0 or 1.

02:54.680 --> 02:56.580
We have a little more detail to it.

02:56.600 --> 02:58.970
There's a little more nuance here.

02:58.970 --> 03:01.310
So lucky for us this is actually the sigmoid function.

03:01.310 --> 03:07.070
This red line function already exists and it's known as the sigmoid function and they said my function

03:07.100 --> 03:14.690
is just f of x is equal to 1 over 1 plus e to the negative x and we could replace X here with Z.

03:14.810 --> 03:17.440
So we have z is equal to w x plus B.

03:17.450 --> 03:23.210
So we just take the whole thing that Z and plug it into the sigmoid function so we'd have f of z is

03:23.210 --> 03:29.240
equal to one over one plus E to the negative z and that would be our sigmoid function for the activation

03:29.240 --> 03:35.310
function so changing the activation function used can actually be really beneficial depending on what

03:35.430 --> 03:40.530
the particular task we're trying to complete and we'll get into more detail on that later on when we

03:40.530 --> 03:42.850
actually implement our own neural networks.

03:44.080 --> 03:48.540
Let's discuss a few more activation functions that we're going to encounter throughout this course.

03:49.150 --> 03:54.370
So we'll really common function that's used as an activation function besides the sigmoid function is

03:54.370 --> 04:00.990
the hyperbolic tangent function or 10 H or Tench of Z where 0 members just w x plus B.

04:01.150 --> 04:06.910
And that looks really similar to the sigmoid function except note that the output can then be from negative

04:06.910 --> 04:10.360
1 to 1 unlike previously we had 0 to 1.

04:10.480 --> 04:16.030
So if we look at the sigmoid function again that went from 0 to 1 and that was equal to 1 over 1 plus

04:16.120 --> 04:22.900
eats a negative x or negative z whatever the input is and now for the Tench or hyperbolic tangent function

04:23.200 --> 04:26.800
it goes from negative 1 to 1 but still has its sort of same behavior shape.

04:26.800 --> 04:31.870
And if you're interested in how to express the hyperbolic tangent mathematically over here on the right

04:31.870 --> 04:34.110
hand side are the actual equations for it.

04:36.500 --> 04:43.610
Then there's also the rectified linear unit or R E L U and this is a really common activation function

04:43.910 --> 04:48.740
but it's actually relatively simple and the way it works is the following method.

04:48.740 --> 04:55.040
You basically passing your z value into this Max equation so you say Max of zero or Z.

04:55.490 --> 04:56.750
So what does that actually mean.

04:56.750 --> 05:01.450
Well the rectified linear unit that actual activation function works the following manner.

05:01.550 --> 05:06.830
It says Based off your z value and 0 just return the maximum value.

05:07.190 --> 05:13.430
And that actually means that if your value of Z is negative then the max between zero and any negative

05:13.430 --> 05:14.870
number is always going to be zero.

05:15.260 --> 05:17.800
So we can see here that is Z.

05:18.020 --> 05:24.530
If it's negative the function always returns zero and then between Max zero comma Z.

05:24.860 --> 05:28.670
If the Z is positive it's always going to return the value of Z.

05:28.700 --> 05:32.120
So essentially you of just that Y is equal to Z here.

05:32.120 --> 05:33.070
So on the right hand side.

05:33.080 --> 05:35.790
That's why you get that sort of straight linear line.

05:35.960 --> 05:38.020
So again relatively simple function.

05:38.030 --> 05:41.950
But this is kind of considered at certain points in time State of the art.

05:42.050 --> 05:46.760
So we're gonna definitely be seeing this activation function a lot throughout our lectures.

05:46.760 --> 05:52.640
R E L U or rectified linear unit actually tends to have the best performance in many situations.

05:52.640 --> 05:57.710
Now luckily for us the deep learning libraries were using specifically cares have all of these activation

05:57.710 --> 06:02.960
functions built in so it's gonna be really easy for us to swap them in and out or use certain activation

06:02.960 --> 06:04.770
functions in certain situations.

06:04.880 --> 06:09.560
Often we'll use something like a soft Max at the very end of a layer in order to actually get some sort

06:09.560 --> 06:13.200
of classification output either a zero or one.

06:13.340 --> 06:18.770
So we're going to do now is move on to understanding had actually implement and build our own neural

06:18.770 --> 06:23.070
network models with Caris now that we already understand the neural network theory.

06:23.240 --> 06:28.070
Once we understand neural network theory and Caris we'll move on to a more advanced type of neural network

06:28.430 --> 06:32.310
specifically designed for sequences called recurrent neural networks.

06:32.390 --> 06:34.270
The first let's move onto cares.

06:34.340 --> 06:35.320
I'll see you at the next lecture.

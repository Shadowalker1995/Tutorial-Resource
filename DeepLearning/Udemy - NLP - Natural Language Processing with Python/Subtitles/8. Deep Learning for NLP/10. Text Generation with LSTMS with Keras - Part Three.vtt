WEBVTT

00:05.470 --> 00:10.980
Welcome back everyone to three of texture narration of Python keris in part 3.

00:10.990 --> 00:14.860
We're going to learn how to generate new text based off of the seat input.

00:15.280 --> 00:16.300
Let's get started.

00:16.630 --> 00:21.260
All right so we have a Sade's trained model and we've also saved our tokenizer.

00:21.280 --> 00:23.860
What's next is to actually generate new text.

00:23.920 --> 00:28.890
So we're going to do is we're going to create a function that generates new text for US based off of

00:28.890 --> 00:34.480
a given model tokenizer sequence length a seed text and then the number words to be generated by the

00:34.480 --> 00:35.240
model.

00:35.710 --> 00:43.420
So I will say DLF generates underscore text and then it's going to take in a model.

00:43.820 --> 00:50.750
It's going to take in a tokenizer it's going to take in a sequence length some see text basically some

00:50.840 --> 00:56.180
text to start off on and then the number of words that we want to generate after this

01:00.330 --> 01:04.670
we're going to begin by preventing a placeholder for the final output text.

01:04.860 --> 01:07.290
And at the very end essentially up protect the list.

01:07.440 --> 01:12.750
So I'm going to at the very end join everything that list of space.

01:13.050 --> 01:15.000
So say output text.

01:15.240 --> 01:15.910
OK.

01:16.260 --> 01:20.290
Now comes the part of actually taking that model and generating new text from it.

01:21.250 --> 01:26.740
So the first we want to do is say input text and we're going to start off with some sweet text.

01:26.740 --> 01:31.300
So we actually need to feed it some sort of line of 25 tokens that we want to start off with and then

01:31.300 --> 01:35.920
it's going to generate one word after that then we're going to do is chop off the very first word of

01:35.920 --> 01:42.280
the seed taken our new word put it at the end and then we have our new sweet text or a new pretext for

01:42.280 --> 01:48.840
that and we're going to keep doing that however many times the user wants to generate words so I'll

01:48.920 --> 01:58.890
say for I in range number of generated words will take the input text string and encode it to a sequence.

01:59.090 --> 02:08.010
So I'll say in coded text is equal to the tokenizer and call text 2 sequences off of this tokenizer

02:08.010 --> 02:17.550
object so text the sequences and then we're going to pass in our input text and we're going to grab

02:17.550 --> 02:23.040
the first item here because it basically returns a tuple or a list so we're going to run the very first

02:23.040 --> 02:23.520
item.

02:23.660 --> 02:27.540
And want to encourage you to do is play around with print statements here so you can print out what

02:27.540 --> 02:28.690
impot text looks like.

02:28.800 --> 02:33.980
Print out what this tokenizer text sequences is etc. but we've seen this before previously.

02:34.010 --> 02:35.320
And one of the lecturers.

02:35.610 --> 02:42.300
And then we need to do is do some pad in coding so we're going to pad sequences to our trained very

02:44.200 --> 02:52.090
So say pad sequences pad sequences here and in order to do that we actually need to import it.

02:52.090 --> 02:57.060
So let's go ahead and do some imports.

02:57.070 --> 03:08.390
We'll say from Krista pre-processing from is that pre-processing we're going to import is sequencing

03:08.510 --> 03:13.790
and pet sequences so from care stop make sure he cares right.

03:13.860 --> 03:21.690
From Krista pre-processing that sequence import pad sequences.

03:21.960 --> 03:25.140
So we're going to now call Pead sequences here.

03:25.620 --> 03:31.060
We're going to pasand the encoded text as a list of just one.

03:31.500 --> 03:36.990
And then the max length here should be whatever the sequence length was that was passed then.

03:37.110 --> 03:43.030
This essentially makes mixer's that if you pass in a super long text it's it's really trained on 25

03:43.080 --> 03:43.770
tokens.

03:43.860 --> 03:47.430
We're going to pad it to make sure it's only 25 tokens.

03:47.460 --> 03:54.660
Or if you see text happens to be too short then we're going to pad it to fill up the 25 spaces again

03:54.870 --> 03:55.800
in order to get best results.

03:55.800 --> 04:00.170
I do recommend just passing a CTX that is actually the same expected length that your model has but

04:00.370 --> 04:06.640
I want to make this function a little more robust so give him pad them out and then for truncating we

04:06.650 --> 04:11.590
can either do a kind of pre or post beginning of string or after the string so will go ahead and do

04:11.590 --> 04:12.840
it at the beginning of the string.

04:12.920 --> 04:19.500
Since we're more dependent on the end of the string for a prediction purposes then we'll say predicted

04:19.650 --> 04:22.260
word index position.

04:22.320 --> 04:24.790
So is going to predict class probabilities for each word.

04:24.870 --> 04:33.170
So same model predicts classes of the padded encoded string.

04:33.410 --> 04:39.100
We'll see verbose is equal to zero so I don't see any output that will grab the very first index and

04:39.110 --> 04:45.360
return there which means the predicted word is just going to be off that tokenizer

04:48.600 --> 04:56.060
so tokenizer call index word and then grab that predicted word index and then we're going to update

04:56.600 --> 04:58.680
our input text.

04:58.890 --> 05:08.500
So say the input text is now going to have a blank space plus the predicted word and let's make sure

05:08.500 --> 05:10.900
we say a word here for a predicted word

05:13.780 --> 05:21.910
and then we'll take our output text and we'll append the predicate that word.

05:22.100 --> 05:22.720
And that's it.

05:22.970 --> 05:23.390
OK.

05:23.510 --> 05:28.700
So let's kind of go line by line here because there is a lot going on through a lot of function calls.

05:28.700 --> 05:30.630
So what does generate text function doing.

05:30.800 --> 05:35.640
It's taking in the model we just trained the tokenizer which has knowledge about the vocabulary in what

05:35.660 --> 05:38.760
ID number goes with what word the sequence link.

05:38.990 --> 05:41.100
Some see text you want to start off with.

05:41.300 --> 05:45.690
And this is robust enough to have shorter sweet text or longer see text and the sequence length.

05:45.740 --> 05:51.140
But for best purposes or best results you should try to make these essentially equal to each other you

05:51.140 --> 05:54.640
should make the text the same length as what was trained on.

05:54.680 --> 05:58.630
Otherwise you have to pad it and then the number of words we want to generate.

05:58.670 --> 06:01.580
So we have our output text which is the final output.

06:01.640 --> 06:07.010
We have our initial seed sequence that initial seeding text and then let's say I want to generate 10

06:07.010 --> 06:07.850
words.

06:07.850 --> 06:13.210
So for i in range number of words so I'm going to do this 10 times I want to generate 10 words.

06:13.580 --> 06:18.340
I'm going to first take the input text string and encode it to be a sequence.

06:18.440 --> 06:24.860
Essentially what we did earlier we transform those raw text data into sequences of numbers.

06:25.570 --> 06:31.420
Then if my CTX happens to be too short or too long I may need to pad it.

06:31.450 --> 06:34.600
I mean need to cut it off or I may need to add to it.

06:34.930 --> 06:41.530
So what I'm going to do is call pad and coding to make sure that it matches are trained rate for best

06:41.530 --> 06:42.030
results.

06:42.040 --> 06:44.830
Hopefully the CTX happens to be the same as a sequence like.

06:44.880 --> 06:46.880
But this makes it more robust.

06:46.990 --> 06:51.150
After that I'm going to predict the class probabilities for each word.

06:51.150 --> 06:55.270
So modeled classes is essentially going to throw out the entire vocabulary.

06:55.360 --> 06:58.210
Assign a probability to the most likely next word.

06:58.330 --> 07:04.270
So they'll maybe you'll say something like 6 percent probability that the next word is Ishmail 7 percent

07:04.270 --> 07:06.340
probability the next word is call et cetera.

07:06.340 --> 07:08.240
So we can do that across the entire vocabulary.

07:08.350 --> 07:12.000
So you can imagine most words are have a very small probability.

07:12.010 --> 07:14.450
Next we're going to have the actual predicted word.

07:14.470 --> 07:21.010
So the way that classes works when we index it with 0 it's going to return the index of that particular

07:21.010 --> 07:21.330
word.

07:21.350 --> 07:28.060
Essentially it's ID which if we can call tokenizer the index word from before we just pass in that index

07:28.090 --> 07:35.060
and it matches with the actual word then we're going to take in the input text and I'm going to add

07:35.060 --> 07:37.580
a space and then add on that perfect word.

07:37.880 --> 07:48.240
So if my input text in the very beginning was 25 words after running this here for the first loop were

07:48.290 --> 07:50.550
the first pass on this for a loop.

07:50.630 --> 07:55.670
It's now going to be 26 words which means I'm then going to pet it and that's why I'm going to truncate

07:55.670 --> 07:56.970
with PRI here.

07:57.020 --> 07:59.040
So it chops off the very first word.

07:59.090 --> 08:01.910
So essentially creating sequences as it goes along.

08:02.030 --> 08:05.210
But more and more of the sequence is going to be in my predicted words.

08:05.210 --> 08:10.700
And if you make numb generated words long enough eventually you'll just be predicting on your own predicted

08:10.700 --> 08:11.090
words.

08:11.090 --> 08:13.830
So true generation without even any seed.

08:14.060 --> 08:15.130
Well there is always a seed.

08:15.140 --> 08:20.540
But after you do this enough times if your number of generated words is longer than your CTX number

08:20.540 --> 08:24.190
of words then you'll be predicting off your predicted words.

08:24.200 --> 08:29.270
Now we still want actually a pen that predicted word so we'll say the up text and we'll append the predicted

08:29.270 --> 08:29.720
word.

08:29.960 --> 08:37.290
So this input text is for prediction purposes this output text is all I'm actually going to show some.

08:37.310 --> 08:39.620
Only going to show these predicted words of the up.

08:39.980 --> 08:40.430
OK.

08:40.700 --> 08:42.230
So we can go ahead and run that.

08:42.230 --> 08:44.170
So that function is ready to go for us now.

08:44.360 --> 08:46.640
And now it's time to generate a seed sequence.

08:46.730 --> 08:51.230
I'm first going to generate using the probably really poor network that we showed here and then are

08:51.230 --> 08:55.850
going to load in the much larger more robust network.

08:55.850 --> 09:00.150
So what I will do is just from my text sequences I only grab the very first one.

09:00.230 --> 09:02.310
Call me Ishmael some years ago et cetera.

09:02.660 --> 09:08.790
And then you can either grab the sequence of your choosing or you can grab a random sequence.

09:08.870 --> 09:17.540
So what I could also do is say import random random seed Possum's a number like 101 and then you could

09:17.540 --> 09:20.170
randomly pick out.

09:20.380 --> 09:31.210
So we can say random loops random dot Ranz integer from 0 to the length of text sequences and then just

09:31.210 --> 09:38.270
grab some random seed text by calling text sequences off of your random pick.

09:39.980 --> 09:45.430
So the random see Texan looks something like this is just a random sentence in our tech sequences.

09:45.440 --> 09:51.410
It's again up to you if you want to choose your own text sequence like the famous first lines or just

09:51.590 --> 09:53.630
randomly choose some see text.

09:53.660 --> 09:55.990
So once you've done that we'll go ahead and join this.

09:56.000 --> 10:07.290
So it's actually a sentence we'll say see text is equal to and we'll say join random CTX.

10:07.330 --> 10:10.100
So now our CTX just looks like a sentence like this.

10:10.120 --> 10:11.490
And throwing the clothes to one side.

10:11.500 --> 10:14.300
He really did this in not only a civil but it really kind and true.

10:14.320 --> 10:17.220
But the way I stood looking and so on.

10:17.710 --> 10:21.400
So I would recommend that because of the unique writing style of Moby Dick.

10:21.400 --> 10:27.970
It's an older book has a more unique writing style that you actually use the text from Moby Dick as

10:27.970 --> 10:28.920
your sweet text.

10:28.990 --> 10:32.830
But if you really wanted to you could technically just write your own sentence here.

10:32.830 --> 10:39.820
Like I went to the ship and then keep going until you hit 25 words.

10:39.890 --> 10:44.320
Again I would really recommend that because unless you're good at mimicking the style of Moby Dick writing

10:44.770 --> 10:46.200
in it it's up to you.

10:46.420 --> 10:50.900
But you see texts eventually at the end should be some string that looks like this.

10:50.950 --> 10:55.860
That is hopefully 25 tokens long or whatever your sequence length happens to be.

10:56.230 --> 11:03.130
Once you have that ready to go we're simply going to call generate text we Pastner model passenger tokenizer

11:04.180 --> 11:12.050
passenger sequence link all things that we defined earlier passen our text and it happens to be equal

11:12.050 --> 11:18.470
to see text just to make sure it happens to be the case soci testicle to see text and then the number

11:18.470 --> 11:23.360
of generated words will go ahead and generate 25 words off of this.

11:23.390 --> 11:27.650
Keep in mind this is a really poor model that we just train really train that on two epochs that horrible

11:27.650 --> 11:28.470
accuracy.

11:28.490 --> 11:30.870
So I would expect to get pretty bad results here.

11:30.950 --> 11:34.650
So let's go ahead and run this.

11:34.770 --> 11:36.150
It looks like I made a slight typo.

11:36.150 --> 11:42.120
I said text singular two sequences it should be text two sequences that will come back up here and fix

11:42.120 --> 11:44.760
that in our original file.

11:44.760 --> 11:47.310
So I kept mentioning kind of really easy to make a typo here.

11:47.580 --> 11:52.810
So it should be if we take a look at tab here it should be text with an s.

11:52.830 --> 11:56.490
So I'll say teks two sequences so run that again.

11:56.730 --> 11:58.080
Let's run these cells.

11:58.080 --> 12:02.090
So we have our CTX now generate Texican.

12:02.290 --> 12:06.060
It looks like there's one more typo that underscore actually shouldn't be there.

12:06.350 --> 12:09.440
So come back up here and fix that as well.

12:12.690 --> 12:14.580
Now you should be able to generate some text.

12:14.850 --> 12:15.960
And there we go.

12:15.960 --> 12:19.350
So if you ran this for two epochs you should see really similar results.

12:19.350 --> 12:24.400
It should is essentially just predict the most common word to be every single word.

12:24.480 --> 12:30.270
So since that performed poorly Let's now load in a model that I trained on the entirety of Moby Dick

12:30.270 --> 12:31.870
for about 300 epochs.

12:31.920 --> 12:36.510
It was still only getting around 60 percent accuracy after 300 epochs but the results are a lot better

12:36.510 --> 12:37.710
than what's shown here.

12:37.710 --> 12:38.610
Let's go ahead and load it in.

12:38.610 --> 12:48.480
We'll say from carious that models import load model and we'll say now are models equal to load model

12:48.810 --> 12:52.410
and the file is called IPAC and there's two of them here.

12:52.440 --> 12:55.070
The really large one is called IPAC big.

12:55.110 --> 13:01.140
So go ahead and import IPAC big five and then the next thing we need to do is load up the tokenizer

13:01.890 --> 13:09.350
that matches this larger file because this was tokenize thing the entirety of the Moby Dick text so

13:09.360 --> 13:15.840
say load and then I'm going to open and this one is the big with no.

13:15.910 --> 13:21.200
5 is the actual tokenizer and I'm going to read it in binary.

13:21.200 --> 13:25.190
So run that and then let's go ahead and copy and paste.

13:25.240 --> 13:26.100
Generate text.

13:26.180 --> 13:29.890
And when you run this you should get back much more reasonable results.

13:29.900 --> 13:35.150
So now that is generating text that says at that stub my frame Roman eyes of his own power for the whales

13:35.150 --> 13:37.100
green wrenched et.

13:37.160 --> 13:41.180
So whether or not this is a good prediction is kind of up to you.

13:41.440 --> 13:41.850
OK.

13:41.960 --> 13:47.510
I encourage you to play around with training your own models maybe on larger sequences or smaller sequences

13:47.780 --> 13:48.120
for more.

13:48.140 --> 13:52.880
E POCs as well as just playing around the different CTX maybe you write your own sweet text.

13:53.160 --> 13:54.270
Okay thanks.

13:54.290 --> 13:55.610
And we'll see you at the next lecture.

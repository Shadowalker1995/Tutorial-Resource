WEBVTT

00:05.390 --> 00:11.720
Welcome everyone to this stacked couter lecture Let's go ahead and take a look at a stack of couter

00:11.810 --> 00:17.990
operating on the digits dataset that will give us a really good opportunity to visualize how well the

00:17.990 --> 00:21.010
reconstructions are coming out at the output layer.

00:21.200 --> 00:23.600
Let's open a particular notebook and get started.

00:23.610 --> 00:27.380
Okay right off the bat as always we start off with a couple of imports here.

00:27.440 --> 00:31.000
We will be doing some plotting in order to visualize those numbers.

00:31.310 --> 00:33.190
So I need to import that plot lib in line.

00:33.230 --> 00:35.290
And of course we're gonna be using pi.

00:35.510 --> 00:43.110
And then finally let me import tensor flow as T.F. and let's grab our digits data set.

00:43.190 --> 00:47.890
So we'll say from tensor flow examples tutorials.

00:47.900 --> 00:55.010
Just import the input data function that's input data.

00:55.010 --> 01:01.420
So don't run that then the next thing I'm going to do is say this is equal to and we'll make sure this

01:02.650 --> 01:03.130
doesn't work.

01:03.130 --> 01:03.880
OK good.

01:03.890 --> 01:06.820
And this data is equal to input data.

01:06.820 --> 01:08.330
We're going to call read data sets.

01:08.500 --> 01:12.530
And right now I'm located underneath the auto encoders folder.

01:12.610 --> 01:17.670
So I need to grab the convolutional neural networks folder.

01:17.760 --> 01:23.200
So I just have autocomplete that and then I also want to grab the data and I want it to be one high

01:23.200 --> 01:24.100
end code.

01:24.570 --> 01:29.280
So we'll say one hot equals true OK.

01:29.340 --> 01:32.570
And you should only see extracting here if you did this properly.

01:32.640 --> 01:34.430
If you begin to see that it's downloading.

01:34.650 --> 01:36.780
That's totally OK if you want to wait for it to download.

01:36.780 --> 01:41.790
But remember you already have the data set ready for you underneath the convolutional neural networks.

01:41.790 --> 01:47.300
Up next we want to reset the default graph just in case to have some other notebooks running.

01:47.910 --> 01:49.190
And let's get started.

01:49.530 --> 01:53.320
OK so let's start creating the parameters for our stacked auto encoder.

01:53.490 --> 01:56.300
We'll go ahead and design a pretty simple stacked auto encoder.

01:56.370 --> 02:01.100
We're going to say we start off 784 inputs which is the result of 28 times 28.

02:01.140 --> 02:04.780
Remember we have 28 28 pixels in the digits dataset.

02:04.860 --> 02:07.880
Then for the next layer I'm going to go ahead and divide it by half.

02:07.920 --> 02:12.750
Is kind of an arbitrary choice for my stack stonecutter but I kind of want to make it clear that we're

02:12.750 --> 02:17.640
continually decreasing the number of neurons available and then for that sensor hidden layer I'll go

02:17.640 --> 02:20.760
ahead and divide that in half again we'll say 186.

02:20.760 --> 02:26.370
Now of course you could keep going to smaller and smaller encoding but instead we're going to begin

02:26.370 --> 02:27.360
to expand that back out.

02:27.360 --> 02:29.920
So we'll see 392 here again.

02:29.940 --> 02:35.530
And then finally 784 remember we need to have the output match the input.

02:35.580 --> 02:44.480
So let's set this up in terms of some variable names missing number of inputs is equal to 784 will say

02:44.660 --> 02:45.530
neurons.

02:45.530 --> 02:50.170
In fact lushest but the solid and useful that we don't get this kind of Misi commenting.

02:50.570 --> 02:53.570
OK we're going to have number of inputs at 784.

02:53.570 --> 02:56.280
I will say neurons in that first hidden layer.

02:56.350 --> 03:02.840
So your odds underscore here in one is equal to 392 going to see neurons.

03:03.070 --> 03:08.150
And the second hidden layer is equal to 186 than the number of neurons.

03:09.750 --> 03:11.060
In my third hit in Alair.

03:11.180 --> 03:14.800
Well that's going to be equal to the number of neurons in my first hit.

03:14.820 --> 03:17.530
So that's where we begin to expand backout 392.

03:17.730 --> 03:22.470
And then finally I will say the number of outputs is equal to the number of inputs.

03:22.470 --> 03:27.660
Now keep in mind as you begin to reduce the number of neurons available in the hidden layer later on

03:27.660 --> 03:32.250
when you visualize your reconstructions you may not get a clear reconstruction because you may have

03:32.250 --> 03:38.270
sacrificed too much information by decreasing number of neurons too quickly or by too much.

03:38.520 --> 03:42.930
And these are numbers you can definitely play around with and see how it affects your actual stack quarter

03:43.290 --> 03:45.360
because you're actually built to visualize this at the very end.

03:45.380 --> 03:47.160
We see the results.

03:47.160 --> 03:49.390
So kind of a fun project here.

03:49.550 --> 03:53.010
Finally a scribe are learning right because it's another constant we have to provide.

03:53.030 --> 04:02.860
So we'll see 0.01 and I'm going to say that my activation function is equal to T.F. But again rectified

04:02.860 --> 04:03.610
linear unit.

04:03.670 --> 04:08.250
And the reason I find the right now is a variable is that way I can just call this shorter variable

04:08.240 --> 04:11.960
name instead of having haven't call this all the time.

04:11.990 --> 04:12.390
All right.

04:12.410 --> 04:16.990
Now it's time to actually begin setting up everything we need for our session and our Grath.

04:17.000 --> 04:21.470
The first thing we need is a placeholder and we don't need a placeholder for our relabelled since this

04:21.460 --> 04:23.170
is unsupervised learning.

04:23.210 --> 04:29.610
So now that I'm just having a placeholder for X that's going to be to float 32 and the shape is going

04:29.610 --> 04:33.310
to be pretty simple just none which is going to be based off the bat size.

04:33.330 --> 04:40.050
And then the number of inputs which is the number of pixels OK so repassing and 784 pixels for each

04:40.080 --> 04:41.190
image in that batch.

04:41.280 --> 04:48.590
And then let's go ahead and set up our weights so I can say initialiser is equal to.

04:48.590 --> 04:54.880
And I can tell you something we haven't seen before which is T.F. variance scaling initialiser and I'm

04:54.870 --> 04:58.220
going to run that and then talk a little bit about it.

04:59.670 --> 05:05.310
If we scroll back up here we realize that we're going to have to build the weights for our inputs are

05:05.310 --> 05:08.680
hidden layer 1 in our hidden layer 2 and then back out again.

05:08.820 --> 05:13.410
And when you're constructing and initializing these weights typically will be done in the past just

05:13.740 --> 05:19.530
grab from some sort of Renn distribution or maybe even set them all to start off it's ones or zeros.

05:19.560 --> 05:26.310
But there's going to be a problem that occurs in the fact that the scale of these weight tensors is

05:26.310 --> 05:29.890
quite different because we're dividing it in half each time.

05:30.150 --> 05:36.600
And what T.F. variance scaling initialiser allows us to do is basically as a name says it's an initialiser

05:36.600 --> 05:41.640
that's capable of adapting its scale to the shape of the weight sensors and there are parameters you

05:41.640 --> 05:42.200
can do here.

05:42.210 --> 05:47.400
But basically it's going to if you do shift help here have some basic scaling.

05:47.400 --> 05:50.920
So at one point zero and it's going to scale it's the size of the tensors.

05:50.940 --> 05:55.990
So because here it's going to adapt its initialization based on the distribution that it chooses.

05:56.100 --> 06:01.290
Distribution normal to the shape of the weight tensors and that's going to allow us to have much better

06:01.290 --> 06:07.320
training because we're going to initialize it based off a distribution that's based off the size of

06:07.320 --> 06:09.340
these hidden layers.

06:09.360 --> 06:13.830
So that's going to create a much better training than if you just initialized it with random weights

06:13.830 --> 06:17.990
across the board because of the size difference in these weight sensors.

06:18.410 --> 06:23.370
So that's usually a necessity when you're dealing with auto encoders especially if you start to do a

06:23.370 --> 06:26.460
really quick shift in the size of the hidden layers.

06:26.460 --> 06:34.190
So let's show you how we can use this guy to actually quite easy and then create some weights variables.

06:34.260 --> 06:38.820
So just as we then in the past we're going to say T.F. variable and then we're going to initialize it.

06:38.830 --> 06:45.420
Previously we did things like T.F. zeros or T.F. ones or T.F. you know truncated normal those kind of

06:45.420 --> 06:46.230
things.

06:46.230 --> 06:52.230
But what I'm going to do now is just call the initialiser object they just created and then passed in

06:52.240 --> 06:54.310
the shape so it's expecting Here's the shape.

06:54.610 --> 06:56.710
So it's going to be not quite simple.

06:56.710 --> 07:03.520
The first weight needs to be the number of inputs and then the output is going to be neurons hit one

07:04.150 --> 07:08.710
and then when you can also tell it what data type is going to be for the variable and the data type.

07:08.880 --> 07:12.740
So the type is equal to T.F. flute 32.

07:13.200 --> 07:18.550
OK so let's copy and paste this because we need a couple of more sets of weights and then let's change

07:18.550 --> 07:19.500
and edit what we need to.

07:19.570 --> 07:22.660
So we have the first set of weights for the input layer.

07:22.660 --> 07:24.990
Second set of weights for that first layer.

07:25.470 --> 07:29.900
Third set of weights for that next hidden layer and force that awaits there.

07:29.920 --> 07:31.280
So let's see what happens here.

07:31.290 --> 07:35.740
We have a number of inputs two neurons hit and one that means neurons hit and one is going to be the

07:35.740 --> 07:43.040
next set and the weights are which means you're get into coming up next they're And then that means

07:43.100 --> 07:47.050
you're on hit into is coming over here.

07:48.730 --> 07:54.650
And then you're on student 3 coming over there which means you're on student 3 going to come here.

07:54.730 --> 07:56.340
And then finally we're going to have our outputs.

07:56.420 --> 07:59.220
So outputs.

07:59.490 --> 07:59.890
All right.

08:00.770 --> 08:03.470
Let's make sure I didn't actually have a typo.

08:03.470 --> 08:06.920
So neurons hit three not define.

08:06.980 --> 08:09.950
Make sure I find that appear neurons high theory.

08:09.950 --> 08:11.610
OK.

08:12.140 --> 08:13.420
Let's run that again.

08:13.960 --> 08:15.410
Okay perfect.

08:15.410 --> 08:20.920
So save that and now we're going to do the same thing but offer our biased terms now are biases.

08:21.020 --> 08:25.200
Those still have to be as strict as the weights we can just start those off as ones or zeroes.

08:25.280 --> 08:26.180
Basically up to you.

08:26.300 --> 08:27.260
Well go ahead.

08:27.290 --> 08:31.280
Because when I was running earlier that tended to give a little bit of some cleaner results.

08:31.400 --> 08:34.790
So I'll say T.F. zeros and that's going to be defined here by the output.

08:34.790 --> 08:42.440
So neurons hit one going to copy this and then paste it four more times for each of those weights.

08:42.470 --> 08:51.300
So we have to be three before zeros menow now need to say it and Lerer to hit only three in the last

08:51.300 --> 08:55.490
decade is going to be the output tools and numb outputs.

08:55.730 --> 09:02.970
OK so we have our weights our biases and we have our activation function defined appear as a CTF.

09:03.000 --> 09:09.990
So we'll keep that and in the notes actually redefined as LCT underscore phunk just to make it a little

09:09.990 --> 09:10.910
more readable.

09:10.920 --> 09:12.160
So I'll go ahead to do the same thing here.

09:12.180 --> 09:15.990
We match exactly the notes but again serving the same purposes like that up there.

09:16.110 --> 09:20.040
Except now I think it's a little more readable than actif.

09:20.050 --> 09:23.990
OK so our activation phunk Let's go ahead and create our hidden layers.

09:24.340 --> 09:30.610
So our hidden layer one is going to be equal to the activation function we chose and then we're going

09:30.610 --> 09:37.360
to say just T.F. matrix multiplication of our placeholder by those weights and then we add in our bias

09:37.360 --> 09:43.640
term Sofala is feels really familiar from the very first lectures covering tensor flow basically doing

09:43.730 --> 09:49.790
the classic neural network design of multiplying your input your weights adding some biased term and

09:49.790 --> 09:51.810
then passing it through an activation function.

09:52.070 --> 09:55.990
We're going to need to do this a couple of more times though let's copy paste here.

09:56.970 --> 10:00.290
So after a hit or one as you may have guessed is hidden there too.

10:00.320 --> 10:05.240
Same deal we're going to say T.F. Mitchell to cation except instead of passing x now we're passing and

10:05.240 --> 10:15.020
the result of Hitler 1 times waits to plus Bice's do and similar thing for that next it.

10:16.050 --> 10:17.150
Doodler 3.

10:17.480 --> 10:20.700
Instead of saying Hit it layer 1 we're going to see him later to

10:24.570 --> 10:28.910
and then this is going to be the third way Tobias's.

10:29.160 --> 10:37.040
And this is actually the upper layers let's call it up layer and this is going to be hidden layer 3

10:38.670 --> 10:47.250
Hitler 3 hopefully and how it's PayPal's there and then for Byass for OK.

10:47.650 --> 10:49.980
And now it's time to find the last function.

10:50.170 --> 11:00.860
We'll just go ahead and say T.F. reduce mean T.F. square of the output layer basically the predictions

11:00.890 --> 11:02.250
minus the results.

11:03.510 --> 11:04.030
OK.

11:04.300 --> 11:08.830
And this is not really in essence a prediction it's more like how good of a representation are we able

11:08.830 --> 11:11.020
to create based off the original data.

11:11.050 --> 11:12.730
So just the mean square error here.

11:13.620 --> 11:16.090
And then we're going to pick an optimizer.

11:16.230 --> 11:18.340
We're going to use the atom optimizer for this.

11:18.360 --> 11:26.230
Say T.F. train atom optimizer and then going to pass in the learning rate that we decided earlier on

11:26.230 --> 11:34.320
which was I believe 0.01 find my training operation as just the optimizer attempting to minimize our

11:34.320 --> 11:36.160
loss function.

11:36.170 --> 11:38.270
Let's go ahead and initialize the variables.

11:38.360 --> 11:41.200
Hopefully this all feels pretty familiar right now.

11:41.340 --> 11:46.250
There's a T.F. global variables initialiser and run that.

11:46.580 --> 11:47.010
OK.

11:47.120 --> 11:49.130
So we basically have everything ready to go.

11:49.220 --> 11:54.680
All we have next is to actually run our session and then test out the output.

11:54.680 --> 11:56.650
So let's go ahead and check that out.

11:56.660 --> 11:58.320
We're almost done here actually.

11:58.570 --> 12:04.820
I'm going to be saving this in order to get on the train data or testator's Well we'll say T.F. train

12:05.430 --> 12:07.170
Savir.

12:07.190 --> 12:09.960
OK so let's actually start constructing our session.

12:10.250 --> 12:17.560
We'll go ahead and run it through five e POCs and we'll fit in in batch sizes of 150 where each IPAC

12:17.570 --> 12:23.450
is going to be an entire run through all your training data and the batch sizes just how large of batches

12:23.450 --> 12:24.290
you're feeding in.

12:24.440 --> 12:27.780
So 15 in and 150 images at a time.

12:28.160 --> 12:33.670
And I say with T.F. session as SPSS we're going to say session ront in

12:36.570 --> 12:37.770
and then we'll do the following.

12:37.770 --> 12:46.920
So I'm going to say for every epoch in the range of the number of the POCs chosen we'll do the following.

12:46.920 --> 12:50.380
The first thing to decide well how many batches is that actually going to be.

12:50.580 --> 12:55.010
Well luckily for us that's quite simple the way this dataset works for us.

12:55.050 --> 13:02.470
We know the number of batches is just going to be this train number of examples divided by the batch

13:02.470 --> 13:04.400
size.

13:04.510 --> 13:09.810
And the reason I'm doing this two forward slashes is in order to not get a small point now.

13:09.820 --> 13:14.450
This basically does a kind of classic division sort of truth division here.

13:14.590 --> 13:18.170
So we know the number of batches we need to go through now just the number of examples of about the

13:18.340 --> 13:20.740
size that equals the number of batches to go through.

13:20.950 --> 13:28.750
So say for every iteration in range of the number of batches we need to go through that are going to

13:28.760 --> 13:35.540
say X batch y batch is equal to this train.

13:35.720 --> 13:43.690
The next batch pests and the batch size again kind of uses convenience methods from him.

13:44.070 --> 13:50.180
They were frightened to say session run that training Optum or training operation we define earlier

13:50.500 --> 13:56.910
and that our dictionary just expects one placeholder value that's going to be X as the next batch.

13:59.700 --> 14:06.420
So once we train that entire number of batches we'll go head and print out our training loss which is

14:06.420 --> 14:08.800
just going to be evaluating our loss.

14:09.030 --> 14:13.870
And again we'll pass it in the dictionary next ex-pats.

14:13.870 --> 14:18.550
Whoops make sure it'll have strings there right.

14:20.690 --> 14:26.120
And we'll go ahead and print out something like Poch

14:29.280 --> 14:38.940
the Los Angeles pitance of formatting there will pass in the pocket number and then that training loss.

14:39.000 --> 14:45.390
Finally once we're done with all the pox I'm going to save this so we'll pasand and save session and

14:45.390 --> 15:01.770
then we'll say example Stex auto encoder checkpoint auto Khutor checkpoint S.K. OK run that and fast

15:01.770 --> 15:03.450
forward in time for this to finish running.

15:03.540 --> 15:07.170
Then we're going to test our auto encoder output on the test data and then we're going to visualize

15:07.170 --> 15:09.330
it.

15:09.510 --> 15:12.480
OK so I just finished running a fiver POCs.

15:12.570 --> 15:16.130
This may take a little bit of time depending on how good your machine is.

15:16.140 --> 15:18.330
Let's go ahead and test that 10 images.

15:18.330 --> 15:24.360
So we're going to say run now 10 test images or images that the auto hasn't seen before.

15:24.450 --> 15:26.580
And let's see how they come out on the other end.

15:26.610 --> 15:29.580
Hopefully they still represent numbers more or less.

15:29.580 --> 15:34.260
They should look like the numbers the past then they may be a little blurry or a little skewed a little

15:34.260 --> 15:39.880
noisy but we should be able to tell that they are fairly similar.

15:39.950 --> 15:45.490
So I'm going to restore that model I just credence let me copy and paste this here.

15:45.560 --> 15:47.680
Examples that out and couter.

15:47.930 --> 15:51.950
And again if you don't actually want to run this there is an example of checkpoint file free to load

15:51.950 --> 15:55.350
up that we don't wait for all this training OK.

15:55.350 --> 16:01.710
So those are examples that Uttam couter I'm going to say that the results are equal to the output layer.

16:01.860 --> 16:07.270
And then I will evaluate it same as we did last time based off the feed dictionary of X..

16:07.450 --> 16:09.630
But in this case of pasand test images.

16:09.670 --> 16:14.890
So the in my test images and then we're just going to passen the number of test images we actually want

16:14.890 --> 16:17.810
to test which in this case are tentes images.

16:17.820 --> 16:19.740
All right so right now I have my results.

16:19.770 --> 16:25.650
So if I take a look at my results they're just basically 10 arrays and I'm going to copy and paste a

16:25.650 --> 16:28.050
plotting function that's in your notebook.

16:28.050 --> 16:30.880
I'll briefly walk through it to tell you what it's kind of actually doing here.

16:31.910 --> 16:35.930
But this is from the notebook that goes along this lecture.

16:35.930 --> 16:40.070
Basically what this does is it creates that polyps subplots.

16:40.460 --> 16:46.340
So it's going to create two rows by 10 collops and the figure size is something you can adjust.

16:46.490 --> 16:52.340
And what it's going to do is for every thing for every test image basically it's going to show the original

16:52.340 --> 16:56.840
test image shown just reshaping it to a 28 by 20 then passing it to show.

16:57.080 --> 17:00.120
And what this represents is the axes of the subplot.

17:00.320 --> 17:03.830
So it's going to plot it at the very top that first rose zero.

17:03.980 --> 17:06.830
So at the very first row we'll see all the real test images.

17:06.830 --> 17:08.370
So those should look really clean.

17:08.630 --> 17:13.640
And then on that second row we're going to see what happens when we pass those test images through the

17:13.640 --> 17:15.840
actual stacked auto encoder.

17:15.860 --> 17:17.510
So let's run this.

17:17.750 --> 17:18.830
And here we go.

17:18.990 --> 17:24.640
OK so this is actually basically we should expect depending on how you actually ran this.

17:24.680 --> 17:26.250
It may be a little noisier.

17:26.300 --> 17:31.250
That is to say you may see a little more coloring around these numbers but you should be able to basically

17:31.250 --> 17:36.060
tell especially for really obvious ones like the ones that it did OK.

17:36.150 --> 17:38.960
Now you'll notice there is definitely some noise here.

17:39.020 --> 17:42.560
For example if you look at the two it's kind of drawing as if it was a 3.

17:42.560 --> 17:44.300
So that's not really good.

17:44.420 --> 17:48.500
And you can see for the four it's kind of getting a piece of a 7 and this one that's kind of like a

17:48.500 --> 17:49.800
really bad five.

17:49.820 --> 17:51.740
It's starting to really get some blurry here.

17:51.770 --> 17:57.280
But for really obvious ones like zeros and ones and even the input 7 it performed pretty well.

17:57.530 --> 18:01.850
So keep that in mind you can definitely kind of begin to tell what features that's learning here.

18:02.060 --> 18:05.340
As a quick note something that's really common for people to do that.

18:05.390 --> 18:10.700
Otto coder's is that instead of the valuating at the output layer with the end up doing is to see what

18:10.700 --> 18:14.390
features the stack auto Khutor is kind of learning.

18:14.480 --> 18:19.570
They end up coming up here and grabbing one of the hidden layers and evaluating at that.

18:19.760 --> 18:23.390
So this actually isn't very useful for the way we designed our stack out in coater.

18:23.440 --> 18:25.310
Basically just nothing or noise.

18:25.310 --> 18:29.690
But if you're interested to see kind of what neurons are highlighting at certain inputs what you could

18:29.690 --> 18:35.510
do theoretically is grab this second hidden layer rummer or second hand and layers one of the smaller

18:35.510 --> 18:35.870
ones.

18:35.900 --> 18:41.680
The one that D6 come all the way down and evaluate at the second hidden layer is sort of the outer layer.

18:41.900 --> 18:47.450
So we're going to pass in these images and then here you're going to see kind of what pixels light up

18:47.450 --> 18:50.630
essentially in this model second hidden layer.

18:50.810 --> 18:54.400
As I mentioned in our case it's going to be almost nothing or kind of noisy.

18:54.560 --> 18:58.490
And if you actually want to run this so now it's evaluating at the hidden layer that's two that's actually

18:58.490 --> 19:06.750
just 196 pixels which means the second one is to be 14 by 14 because 14:10 14 is 186.

19:06.760 --> 19:11.260
So if you're on that you should be able to just kind of see some noise here so you can kind of see maybe

19:11.260 --> 19:11.860
some trends.

19:11.890 --> 19:14.020
In this case it's not super clear.

19:14.410 --> 19:17.120
Maybe you can added some more neurons that kind of play around with that.

19:17.140 --> 19:22.240
But the more important part is actually evaluating at the output layer where you do get to see basically

19:22.240 --> 19:27.430
that the stacked couter is trying its best to represent the numbers at the output.

19:27.430 --> 19:29.670
All right I hope you found this lecture interesting.

19:29.680 --> 19:31.480
I will see you at the next section of course.

19:31.480 --> 19:32.250
Thanks everyone.

WEBVTT

00:05.270 --> 00:10.270
Welcome everyone to this lecture we're going to go over the exercise we're going to perform them and

00:10.300 --> 00:12.910
anality reduction with a linear audio encoder.

00:12.980 --> 00:15.400
Let's walk through Jupiter notebook and see if we have to do.

00:15.410 --> 00:20.510
All right so here we are at the linear auto encoder for PC exercise and for this what you're going to

00:20.510 --> 00:25.370
end up doing is basically very similar to what we just did in the lecture where you going to do just

00:25.370 --> 00:29.430
follow along here get the data and the data is this anonymize data.

00:29.440 --> 00:33.400
C s v file and it's included underneath the auto encoder folder.

00:33.530 --> 00:38.660
And then notice that it's basically just a bunch of columns that have anonymised column names so just

00:38.660 --> 00:42.650
for letters and then there are some values and each of the columns and if you go all the way to the

00:42.650 --> 00:44.990
right you also have a label column.

00:44.990 --> 00:49.310
Now remember we're not doing any sort of classification task we're just using that label to see if we

00:49.310 --> 00:54.630
can maintain that class separation even if we reduced down to just two mentions.

00:54.650 --> 00:58.670
Believe it or not you're going to take this 30 of them internal data set and you're going to reduce

00:58.670 --> 01:03.530
down to two that mentions and you'll be able to clearly see that the cluster are still separated.

01:04.280 --> 01:09.070
So you'll scroll down here you'll skil the data and then build that the linear auto encoder and a lot

01:09.070 --> 01:11.870
of these steps match very closely to what we did in the lectures.

01:11.890 --> 01:16.690
You can always check the lecture for more details of the placeholder layers loss function optimizer

01:16.960 --> 01:21.280
then you'll go ahead and run your session and hopefully at the end you still see that even with just

01:21.280 --> 01:26.110
to them mentions this theory then mentionable original data set will reduce down to two mentions with

01:26.110 --> 01:27.410
the linear auto encoder.

01:27.520 --> 01:33.370
The auto encoders hidden layer has learned enough of the real combination of features to create two

01:33.370 --> 01:38.560
new feature planes that allow it to still maintain class severability with just two mentions worth of

01:38.560 --> 01:39.410
data.

01:39.430 --> 01:39.940
OK.

01:40.120 --> 01:43.510
Best of luck on this and I'll see you at the next lecture where we go through the solutions.

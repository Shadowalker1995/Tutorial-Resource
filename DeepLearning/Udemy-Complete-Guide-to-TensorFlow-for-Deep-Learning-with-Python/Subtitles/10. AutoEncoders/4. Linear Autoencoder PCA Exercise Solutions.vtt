WEBVTT

00:05.350 --> 00:06.380
Welcome back everyone.

00:06.400 --> 00:11.500
The solutions lecture for the dimensionality reduction exercise for linear audio and encoders.

00:11.530 --> 00:14.360
Let's go to the Jupiter notebook and walk through the solutions.

00:14.680 --> 00:16.270
OK so here we are at the notebook.

00:16.390 --> 00:21.400
We'll start off with our imports will the numb pie and then we're also going to use pandas to read and

00:21.400 --> 00:22.840
to see as we file.

00:22.840 --> 00:28.570
And then finally we're going to import map plot lived that pipe plot as PLDT and then we'll say map

00:28.570 --> 00:32.010
plot live in line OK.

00:32.030 --> 00:36.710
First things first we need to actually get our data will say DMF or whatever you want to call your data

00:36.710 --> 00:39.220
frame is equal to read CXXVIII.

00:39.380 --> 00:43.730
And since I'm already located in the auto encoder folder there's going to tab autocomplete for that

00:43.730 --> 00:45.160
CSP dataset.

00:45.260 --> 00:48.920
And if you take a look at the head of it it's going to look something like this.

00:48.950 --> 00:53.900
Basically you have these 30 columns of data and if you go all the way to the right you have that label

00:53.900 --> 00:57.460
column and the hope here that's just the result of calling info.

00:57.470 --> 01:01.970
It's another way of seeing general information but your data frame specifically about each of the columns

01:01.970 --> 01:06.920
so we can see here they're all basically the same thing just five hundred entries of numbers and all

01:06.920 --> 01:09.310
the way down and we have that label column.

01:09.320 --> 01:10.810
Now we want to scale the data.

01:11.080 --> 01:14.930
An important thing to note here when we're scaling data is we're not doing any sort of train test but

01:15.320 --> 01:16.930
because this is unsupervised learning.

01:16.940 --> 01:24.170
We just really care about the features we'll say from a scalar and pre-processing import and will improve

01:24.220 --> 01:31.640
the minimax scaler will create an instance of a min max scaler and then we'll finally say our scale

01:31.640 --> 01:34.310
data is equal to scaler.

01:34.370 --> 01:38.780
And again we'll just call it transform on our entire features dataset.

01:38.890 --> 01:47.350
So we'll say the drop the label and say x is equal to 1 OK.

01:47.380 --> 01:48.750
Now we have our scale data.

01:48.760 --> 01:50.410
So those are just the features.

01:50.410 --> 01:55.090
Up next we want to actually begin to build out a real linear auto encoder which means we're going to

01:55.090 --> 02:03.520
import sensor flow as T.F. then the next thing we're going to import is that fully connected layer will

02:03.520 --> 02:15.040
say from tensor flow contrib layers and poor fully connected OK then we're going to fill out the number

02:15.040 --> 02:17.610
of inputs to fit them mentions the data set.

02:17.650 --> 02:22.330
So we have 30 features so that means we're going to have 30 inputs the number of hidden units in this

02:22.330 --> 02:25.660
case we're going to reduce it down to two then mentions So we'll have that out there.

02:25.660 --> 02:30.130
And then it is already set for us the learning rate you can kind of play around of this but we shouldn't

02:30.130 --> 02:32.140
need a very high learning rate.

02:32.260 --> 02:35.820
The fact of the matter is the state is highly separable even in 30 dimensions.

02:35.950 --> 02:38.190
So we should be able to learn on it pretty fast.

02:39.550 --> 02:41.080
Next we have the placeholder.

02:41.200 --> 02:49.780
So we're just passing in our data will say x is equal to T.F. placeholder and then we'll say flow 32

02:50.450 --> 02:53.730
and shape it's going to basically be defined by that batch size.

02:53.980 --> 03:01.000
And then the number of inputs which is 30 features But next we're going to create some layers hidden

03:01.000 --> 03:05.120
layer is equal to a fully connected layer.

03:05.320 --> 03:11.620
So pass an X than the number of hidden units and then for this case we'll say activation function is

03:11.620 --> 03:12.110
done.

03:13.420 --> 03:17.810
And then the outputs are just going to be a fully connected layer.

03:19.030 --> 03:28.210
And also a hidden number of outputs and then finally no activation function Let's go ahead and create

03:28.240 --> 03:36.940
our last function that's going to be T.F. reduce mean use mean.

03:37.470 --> 03:38.760
And then we're going to take the square.

03:38.770 --> 03:40.300
So basically I mean square there

03:43.760 --> 03:49.110
of the outputs minus X OK we ever mean squared error.

03:49.150 --> 03:59.010
Let's create our optimizer will use an Adam Adam optimizer T.F. train call atom optimized for this person

03:59.010 --> 04:05.310
the learning rate that we defined earlier and then say or trainer operation is just the optimizer trying

04:05.310 --> 04:09.710
to minimize the loss.

04:09.710 --> 04:12.830
Next we're going to create an instance of that global variable initialiser.

04:12.890 --> 04:18.040
So we'll say it is equal to T.F. global variables initializer.

04:18.230 --> 04:21.980
And then we actually create instance of it so we'll make sure we run that and then we actually want

04:21.980 --> 04:23.140
to run the session.

04:23.150 --> 04:27.480
So we're going to create a tensor for session that runs the optimizer for at least a thousand steps.

04:27.680 --> 04:33.690
So you can use ebox if you prefer one ypocras just defined by one single run through the entire dataset.

04:34.860 --> 04:40.000
Well St number of steps is equal to 1000.

04:40.020 --> 04:42.950
Again you could have said something like just run it for two epochs.

04:43.130 --> 04:44.860
Let's make sure that's not connected.

04:44.940 --> 04:56.020
So we'll say it with a session as SPSS session run and it's and then it is safe for every iteration

04:56.090 --> 04:56.850
in the range.

04:56.850 --> 05:04.410
The number of steps we defined earlier thousand steps we're going to say session run train and then

05:04.410 --> 05:10.590
our feed dictionary here is just going to be our scale data.

05:10.780 --> 05:13.010
So we'll go ahead and run that should be pretty fast.

05:13.030 --> 05:18.660
And now we want to create a session that calculates the scaled data going through that hidden layer.

05:18.670 --> 05:27.480
So with T.F. session as SPSS and this is one of those instances where you might want to run a interactive

05:27.480 --> 05:35.600
session with initialiser variables again and then say that the two the output is equal to that hidden

05:35.600 --> 05:45.730
layer and then it evaluates given a dictionary X and then the scale data OK.

05:45.740 --> 05:49.890
So we're going to confirm now that our output is actually the output we expected some to them actual

05:49.910 --> 05:50.590
output.

05:50.990 --> 05:52.650
So we'll check out the shape.

05:52.730 --> 05:58.230
So now it has 500 entries but there's only two features so let's plot it out and see how well we did.

05:58.230 --> 06:04.530
So we'll say Pulte scatter and we want to get output to the I will take.

06:04.800 --> 06:06.890
Well let's just show you output to the it looks like.

06:06.930 --> 06:13.630
So you can kind of get an idea I now have just two columns worth of data from the 30 columns.

06:13.630 --> 06:21.320
So then in going I say grab all the rows in the first column and plot them against all the rows in the

06:21.320 --> 06:23.000
second column.

06:23.000 --> 06:27.200
Now if you just run that and you get something that looks like this where there's not very clear separation

06:27.620 --> 06:33.230
and you can determine that by asking for the color will say color is just defined by that previous label

06:33.230 --> 06:35.120
column.

06:35.140 --> 06:38.130
So we see here they're not really well separated still.

06:38.220 --> 06:41.290
What we can do is we'll come back up here and train it for a couple of more steps.

06:41.290 --> 06:43.180
Let's go ahead and just say 2000 steps.

06:43.480 --> 06:48.400
So we'll run these cells again evaluate again if that new model run the output and then let's run this

06:48.400 --> 06:48.820
again.

06:48.820 --> 06:51.230
So you can see we're getting better or better separation.

06:51.250 --> 06:56.900
So let's run that for just let's say two overkilled or 5000 steps.

06:56.980 --> 07:00.130
Hopefully it doesn't run forever.

07:00.140 --> 07:01.940
Ok it looks like it ran pretty good.

07:01.940 --> 07:02.950
And here we can see.

07:02.990 --> 07:04.860
So now we going getting clear separation.

07:04.880 --> 07:09.050
So if you don't get clear separation at the very beginning you may just have to run it for some more

07:09.050 --> 07:10.530
steps.

07:10.530 --> 07:15.270
Now if we scroll back up here at the instructions that call for a thousand steps that may again not

07:15.270 --> 07:15.870
be enough for you.

07:15.870 --> 07:20.840
So go ahead and just you know change it to some higher number Fermanagh and again at 1000 this time

07:20.850 --> 07:24.320
you can see I kind of get lucky here and I do get that full separation.

07:24.330 --> 07:28.210
So again the more steps you do the more likely you get a higher separation there.

07:28.440 --> 07:32.410
If after many many thousands of steps you're still not getting separation.

07:32.490 --> 07:37.200
Go ahead and run the solution is no book and double check that your all your codes actually matching

07:37.200 --> 07:38.740
the solutions and the solutions.

07:38.740 --> 07:41.550
No but those have a thousand steps as a number of steps there.

07:41.580 --> 07:43.980
Go ahead and amp that up and then check again.

07:44.070 --> 07:44.410
OK.

07:44.430 --> 07:47.070
If you have any questions on this feel free to post that on the forums.

07:47.070 --> 07:48.930
But hopefully it was pretty straightforward.

07:48.930 --> 07:50.690
Thanks everyone and I will see at the next lecture.

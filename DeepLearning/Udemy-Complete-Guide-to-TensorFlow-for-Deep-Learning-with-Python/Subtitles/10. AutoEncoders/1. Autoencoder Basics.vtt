WEBVTT

00:05.520 --> 00:11.230
Welcome everyone to this lecture on the basics of an auto encoder the auto encoder is actually a very

00:11.230 --> 00:15.610
simple neural network and it's going to feel pretty similar to the multi-layer preceptress models we

00:15.610 --> 00:21.380
saw in the very beginning of the course and it's designed to reproduce this input at the output layer.

00:22.440 --> 00:28.680
So that idea of reproducing your input at the output layer causes a key difference between an auto encoder

00:28.770 --> 00:34.260
and a typical multilayer perception model network and that's because the number of input neurons is

00:34.320 --> 00:40.050
equal to the number of output neurons versus previously we were seeing models that had maybe a lot of

00:40.050 --> 00:46.350
input neurons maybe one for every pixel in a picture and then the actual output neurons were just 10

00:46.410 --> 00:49.440
such as a one output neuron for each class.

00:49.500 --> 00:53.400
But in this case for the auto encoder we're actually going to have the number of input neurons equal

00:53.400 --> 00:54.570
to the number of output neurons.

00:54.570 --> 00:59.050
So let's explore what this looks like in a diagram and why we would actually use it.

00:59.700 --> 01:04.470
So this is a diagram of a simple auto encoder and you can see here it's actually stacked so there's

01:04.800 --> 01:06.410
more than one hidden layer.

01:06.420 --> 01:08.000
There's three hidden layers here.

01:08.100 --> 01:12.600
And the important thing to note here is what's happening over on the left hand side with these inputs.

01:12.600 --> 01:17.940
We have five input neurons and then that gets reduced to three neurons in the next hidden layer the

01:17.940 --> 01:20.250
two neurons in that center hit the layer.

01:20.250 --> 01:26.250
Then it gets gradually increased until the output layer again has the same number of neurons as the

01:26.250 --> 01:27.150
input layer.

01:27.890 --> 01:31.940
So we're going to do is we're going to walk through the layers and explain the basic idea of an auto

01:31.940 --> 01:37.240
encoder and to simplify this even further we're just going to have one single hidden layer.

01:37.490 --> 01:40.130
And this is a very simple basic auto encoder.

01:40.130 --> 01:44.040
The other one we just saw with more hidden layers is known as a stacked auto encoder.

01:44.150 --> 01:46.950
And we'll explore that later on in a future lecture.

01:46.970 --> 01:52.110
But now let's just go ahead and try to get the most simple auto encoder possible.

01:52.160 --> 01:56.120
So in order simplify this further we're just going to have this diagram here where we have inputs that

01:56.120 --> 01:57.480
hidden layer and outputs.

01:57.500 --> 02:01.790
And keep in mind when we talk about these layers we're just talking about these actual neurons here.

02:01.820 --> 02:06.740
So we have an input layer of five neurons that goes to a hidden layer which is smaller than the input

02:06.740 --> 02:12.080
layer and then it gets expanded back out to our player which has the exact same number of neurons as

02:12.080 --> 02:13.540
that input layer.

02:13.970 --> 02:15.720
So that's what we're modeling here.

02:16.550 --> 02:21.380
And keep in mind it's just a feedforward network that's trained to reproduce it's input at the output

02:21.380 --> 02:22.010
layer.

02:22.040 --> 02:27.230
So all these auto encoders trying to do is directly mimic its input at the output layer.

02:28.190 --> 02:31.180
So that means the output size is the same as the input layer.

02:32.210 --> 02:39.050
So what does it look like mathematically Well let's say we have inputs X and then that gets fed into

02:39.080 --> 02:40.210
some hidden layer.

02:40.400 --> 02:46.880
So we have that result B H of x so hidden takes an X and then we have a biased term in the hidden layer

02:46.940 --> 02:50.830
along some weights attached to the inputs going into the hidden layer.

02:51.020 --> 02:55.490
And then from that hidden layer we have another set of weights going out and then another set of biased

02:55.490 --> 02:57.060
terms which we can call see.

02:57.110 --> 02:59.770
So the space is just moving along the alphabet.

02:59.770 --> 03:00.340
All right.

03:00.620 --> 03:05.840
So there's two main parts to an auto encoder and that is the first part the encoder and the second part

03:05.840 --> 03:06.980
the decoder.

03:06.980 --> 03:10.850
So let's start from the bottom and work our way to the top.

03:10.850 --> 03:13.240
So at the bottom here we have the inputs.

03:13.310 --> 03:18.680
So we have some input and we'll call that input X and then we're going to do the exact same process

03:18.680 --> 03:22.430
we did back when we're dealing with the very basic neural network models.

03:22.430 --> 03:27.440
We just take her input X we're gonna multiply it by some weights and at a biased term we'll just call

03:27.440 --> 03:32.390
that a linear transformation and then we can pass that through some sort of activation function such

03:32.390 --> 03:36.740
as a sigmoid activation function or many other activation functions that we talked about.

03:36.760 --> 03:39.260
But keep things simple We'll just keep the sigmoid.

03:39.260 --> 03:42.050
And then that results in that hit layer.

03:42.050 --> 03:47.840
So again the hidden layer is just taking in the inputs and applying that linear transformation and then

03:47.840 --> 03:49.680
passing it through an activation function.

03:49.910 --> 03:54.950
And that's the first half of going from the input to that sensor hidden layer that's called the encoder

03:55.490 --> 03:58.210
and then that second half is the decoder.

03:58.250 --> 04:02.450
So what it does is it takes the result of that hidden layer we'll call it a bunch of Acts.

04:02.660 --> 04:08.300
Multiply it by another set of weights w out and then again applies a linear transformation with its

04:08.300 --> 04:09.950
own biased terms that see.

04:10.100 --> 04:14.630
And then we can pass it through an activation function and then we get back what we call X hat or X

04:14.630 --> 04:21.890
bar and that X bar is basically this auto encoders reproduction of the inputs.

04:21.920 --> 04:27.350
Now a really common practice when dealing with auto encoders since you're trying to basically reproduce

04:27.350 --> 04:28.960
your input at the output.

04:29.120 --> 04:31.290
Often you will tire weights.

04:31.370 --> 04:37.910
So all that means is that your weights from the input to the hidden layer are going to be the same as

04:37.910 --> 04:41.340
your weight's coming from out of the Hilarie to your outputs.

04:41.360 --> 04:46.940
So all you end up doing you see your weights coming out is equal to the transpose weights coming in

04:47.360 --> 04:50.180
and we'll show you how to do that later on with their actual code.

04:50.180 --> 04:54.680
It's actually quite easy of tensor flow but keep in mind if you ever reading a paper about encoders

04:54.680 --> 04:58.940
and they say they're using tight weights that's all that means that we're actually not figuring out

04:59.150 --> 05:00.830
another set of weights at the output.

05:00.840 --> 05:04.940
Instead we're just tying that to the set of weights that's being figured out from the input to the hidden

05:04.940 --> 05:05.600
layer.

05:05.600 --> 05:08.420
Now keep in mind you're not going to tie the biased terms together.

05:08.420 --> 05:09.540
Those are going to be separate.

05:09.560 --> 05:12.970
All you're tying are the weights.

05:12.980 --> 05:18.650
So the key idea here is really occurring at that hidden layer so that hidden layer is typically going

05:18.650 --> 05:23.360
to be smaller than the inputs and that's called an under complete auto encoder.

05:23.360 --> 05:28.550
Technically you can also have that hidden layer be larger the inputs but that's called an over complete

05:28.780 --> 05:29.470
autem coder.

05:29.600 --> 05:31.160
And we're not going to focus on that right now.

05:31.160 --> 05:36.560
Instead we'll focus on the typical auto encoder structure which has a smaller number of neurons in that

05:36.560 --> 05:37.730
hidden layer.

05:37.730 --> 05:42.790
So again that's called an under complete auto encoder because we have less neurons in that hidden layer.

05:43.010 --> 05:48.620
And the idea is if you're taking a bunch of inputs and then having to reduce them to that smaller hidden

05:48.620 --> 05:54.320
layer that hidden layer is going to have some sort of internal representation that tries its best to

05:54.320 --> 05:56.760
maintain all the information of the input.

05:56.790 --> 05:59.760
However it has less neurons to do it with.

05:59.780 --> 06:05.630
Now the really interesting aspect of this hidden layer and a big part of using auto encoders is that

06:05.630 --> 06:08.970
we can use this hidden layer to extract meaningful features.

06:09.050 --> 06:13.850
And we'll also explore later on in the next lecture how we can use principal component analysis with

06:13.850 --> 06:14.720
auto encoders.

06:14.960 --> 06:20.870
But the main idea here being that if we have a large number of input neurons and then we essentially

06:20.870 --> 06:27.410
force them to go down into a smaller number of hidden neurons and then ask for the representation back

06:27.410 --> 06:28.720
at the outputs.

06:28.790 --> 06:34.670
Then what we're doing is we're training this hidden layer to try to get some sort of compressed representation

06:34.790 --> 06:40.280
of our input data and you can easily imagine that if you think back to the original diagram you're taking

06:40.280 --> 06:46.430
something like 10 inputs and then feeding it into a hidden layer of only two neurons than those two

06:46.430 --> 06:51.620
neurons are going to have to try their best to figure out how they can essentially compress that information

06:51.950 --> 06:57.050
of ten features into just two using some sort of combination of all those features multiplied by those

06:57.050 --> 06:57.640
weights.

06:57.770 --> 06:59.360
And then with those biased terms.

06:59.600 --> 07:04.070
So again you can kind of think about this almost as a compression capability.

07:04.740 --> 07:08.850
So later on we're going explore stacked auto scooters with more hidden layers so it's not just a quick

07:08.880 --> 07:13.850
jump from the inputs all the way to a small hidden layer so that's known as a stack of encoder and we'll

07:13.860 --> 07:19.080
explore that in a future lecture for now you should understand a very basic auto encoder.

07:19.100 --> 07:24.110
Again the principle is really simple you just have the encoder going from that number of input neurons

07:24.110 --> 07:28.760
to the hidden layer and then you have the decoder go in from the hidden layer that the output where

07:28.760 --> 07:33.530
the input neurons equals the same amount as the output neurons and all the auto code is trying to do

07:34.010 --> 07:37.420
is get some sort of reproduction of its input at its output.

07:37.550 --> 07:43.220
And through that process you end up training that hidden layer to try to compress the information available

07:43.520 --> 07:44.900
from the input.

07:44.900 --> 07:49.460
So up next we're going to discuss a little more about this compression and dimensionality reduction

07:49.790 --> 07:54.850
by exploring how we can use auto encoders to basically mimic principal component analysis.

07:54.890 --> 07:56.690
Thanks everyone and I'll see you at the next lecture.

WEBVTT

00:05.870 --> 00:12.470
Hello everyone this lecture is going to serve as a quick note on the word to VAK lectures.

00:12.500 --> 00:16.440
So there's going to be an optional series of lectures describing how you could implement the words of

00:16.450 --> 00:22.550
ECK model which is a model that those embeddings in a vector space for individual words with tensor

00:22.550 --> 00:23.330
flow.

00:23.330 --> 00:28.220
Now if you actually want to practically implement word to VEC I would personally not recommend you use

00:28.220 --> 00:28.840
tensor flow.

00:28.850 --> 00:34.580
In fact lots of people have already done a lot of work to create nice API libraries that quickly and

00:34.580 --> 00:40.310
easily create word to Veck models doing what we do in the series of lectures that you're about to see.

00:40.310 --> 00:44.510
Basically it does all the hard work over again and you're gonna end up reinventing the wheel.

00:44.570 --> 00:49.880
So if you actually want to do a practical implementation of sorts of EQ on your own datasets I highly

00:49.880 --> 00:52.500
recommend you check out the gensym library instead.

00:52.610 --> 00:56.870
If you're further interested in words HVAC and I'll mention it again later on during the lectures.

00:56.870 --> 01:02.300
So keep in mind that this series of lectures is basically going to be a full manual implementation of

01:02.360 --> 01:04.680
using tenderfoots create the words of model.

01:04.820 --> 01:09.380
But if you actually want to do it in real life I would recommend that you avoid tensor flow and use

01:09.380 --> 01:10.700
gensym instead.

01:10.700 --> 01:14.650
In fact let's hop over to the gensym Web site so I can give you just a quick tour of it.

01:14.660 --> 01:18.710
We don't cover this course because it's technically not related to sensor flow but I do want you to

01:18.710 --> 01:23.700
be aware of it as a practical solution for applying or its effect to your own problems.

01:24.170 --> 01:26.600
OK so here I am at the gensym Web site.

01:26.590 --> 01:30.510
If you just google search Python plus gensym you'll end up finding it.

01:30.710 --> 01:36.080
And basically this is topic modeling and natural language processing but it uses a really nice simple

01:36.080 --> 01:41.810
to use API so you can click here on tutorials and it basically has full tutorials and everything you

01:41.810 --> 01:42.320
need to do.

01:42.380 --> 01:46.670
If you want to go from strings to vectors you can click on this here and it basically shows you how

01:46.670 --> 01:49.150
you can end up representing strings as vectors.

01:49.190 --> 01:54.470
Otherwise known as word to VEC so shows you how to do it and you'll notice here that the code is really

01:54.470 --> 01:55.260
quite short.

01:55.460 --> 02:00.890
So it's a lot simpler to use than the full implementation will see with tensor flow later on in this

02:00.890 --> 02:01.360
course.

02:01.430 --> 02:03.510
So I recommend if you want to use words of Eq.

02:03.530 --> 02:04.740
Check out gensym.

02:04.760 --> 02:08.690
It also has a lot more additional functionality and methods you can call once you've already trained

02:08.690 --> 02:09.480
your model.

02:09.560 --> 02:13.190
If you need instructions on how to install it you click here on install and it basically tells you you

02:13.190 --> 02:15.510
can just use Pipp install or an easy install.

02:15.650 --> 02:17.170
Or you can just download it directly.

02:17.450 --> 02:18.840
And then there's also support.

02:18.920 --> 02:21.380
There's API so you have full references here.

02:21.560 --> 02:25.230
So again I really recommend if you're going to dive deep into Word.

02:25.400 --> 02:27.590
Or even just natural language processing in general.

02:27.590 --> 02:30.170
You definitely give gensym a try.

02:30.200 --> 02:32.480
OK so that's my little plug here.

02:32.540 --> 02:37.200
Again for practical implementation of word to use this library don't use tensor flow.

02:37.490 --> 02:43.040
But since this is a curse on tensor flow and words of x is a pretty famous example of using deep learning

02:43.160 --> 02:46.670
will go ahead and show you how it's done in the next series of lectures.

02:46.860 --> 02:49.250
OK thanks everyone and I'll see you at the next lecture.

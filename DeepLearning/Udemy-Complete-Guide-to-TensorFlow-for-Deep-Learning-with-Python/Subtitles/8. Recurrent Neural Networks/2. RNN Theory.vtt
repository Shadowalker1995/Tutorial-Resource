WEBVTT

00:05.350 --> 00:10.000
Hello everyone and welcome to the recurrent neural networks theory lecture here in this lecture which

00:10.000 --> 00:15.490
is going to give you a brief overview of the theory behind a recurrent neural net.

00:15.490 --> 00:20.590
So first off we're going to mainly be working with recurrent neural networks to solve problems that

00:20.590 --> 00:24.570
deal with sequence information in a couple of examples of sequence information.

00:24.580 --> 00:27.370
Are things like generally time series data.

00:27.370 --> 00:32.560
So that could be something like a stock price changing throughout time or even things like the sales

00:32.560 --> 00:35.050
of a store changing throughout time.

00:35.050 --> 00:36.480
Then there's also sentences.

00:36.490 --> 00:39.890
So even just natural language has a sequence to it.

00:40.030 --> 00:43.130
So there's going to be words in a sentence in a particular order.

00:43.150 --> 00:45.390
So as you move along the words should be in a particular order.

00:45.400 --> 00:47.930
So that can also be thought of as a sequence.

00:47.980 --> 00:50.280
Then there's music and audio information.

00:50.410 --> 00:52.800
So that's sound and in particular at time ordering.

00:52.810 --> 00:58.150
So that can also be thought of a sequence and even things like contradictories so where a car is going

00:58.150 --> 01:03.850
to go or things like if you throw a ball up in the air or the trajectory of its path throughout the

01:03.850 --> 01:07.460
air it can also be thought of as a sequence.

01:07.620 --> 01:13.770
So we can imagine a sequence as just a vector of information for the index location basically points

01:13.770 --> 01:15.520
out its points in time.

01:15.690 --> 01:18.990
And then the actual values are the training values.

01:18.990 --> 01:24.960
So if we imagine a sequence that's 1 2 3 4 5 6 the question that you could ask anybody is are you able

01:24.960 --> 01:28.920
to predict a similar sequence shifted one tiny step into the future.

01:29.010 --> 01:30.750
And for most people it's actually pretty easy.

01:30.750 --> 01:35.700
You feel that there's a pattern here of increasing by 1 as you move along one time step.

01:35.700 --> 01:40.430
So if I asked you hey what's the sequence that's similar to this except shifted over one time that you

01:40.550 --> 01:41.670
would hopefully be able to figure out.

01:41.670 --> 01:44.840
Well I'll just say two three four five six seven.

01:44.940 --> 01:51.060
So we're going to try to later on training network to predict a time series shifted over one time step

01:51.120 --> 01:55.530
into the future and you can imagine that will be really beneficial if we're trying to predict sales

01:55.530 --> 01:56.440
for the next day.

01:59.300 --> 02:04.130
So the way we can do this is by using a recurrent neural network so let's review how the normal neuron

02:04.130 --> 02:06.290
works in a feed forward network.

02:06.350 --> 02:11.840
Remember that a normal neuron just takes in some input and it can be multiple inputs so it can aggregate

02:11.840 --> 02:17.150
them and then once it aggregates those inputs it passes it through some sort of activation function.

02:17.150 --> 02:21.950
Here we're just using the symbol for a rectified linear unit that it can be things like a sigmoid function

02:22.250 --> 02:24.110
or a 10:08 function et cetera.

02:24.290 --> 02:26.210
And then from that we have an output.

02:26.210 --> 02:30.810
So that's one normal neuron works a recurrent neuron is a little different.

02:30.860 --> 02:34.640
What it's going to do is send the output back to itself.

02:34.880 --> 02:39.590
So the output goes back into input of the same neuron.

02:39.950 --> 02:43.170
So we can actually unroll this throughout time.

02:43.460 --> 02:47.400
So if we roll this throughout time it ends up looking something like this.

02:47.510 --> 02:52.670
So we can see time here kind of represent it on the x axis and we have this particular recurrent neuron

02:53.090 --> 02:59.720
where it's input at T minus one gives an output at T-minus 1 and then that gets passed into the neuron

03:00.050 --> 03:04.520
in its state input at time t and then that has an output at time t.

03:04.520 --> 03:11.240
And then we can take that output and pass an input for the same neuron at time T plus 1 and then so

03:11.240 --> 03:12.140
on and so on.

03:12.140 --> 03:15.260
So this is called unrolling that recurrent neuron.

03:15.410 --> 03:20.180
Something that's important to note here is that the neuron is actually receiving both inputs from a

03:20.210 --> 03:24.390
previous timestep as well as inputs from the current time step.

03:24.400 --> 03:27.560
So you can see each of these neurons has two sets of inputs there.

03:27.710 --> 03:33.650
These cells that are a function of input from previous time steps are also known as memory cells in

03:33.650 --> 03:38.900
recurrent neural networks are also flexible in their inputs and outputs for both sequences and single

03:38.900 --> 03:42.390
vector values so show a couple of examples that in just a little bit.

03:42.560 --> 03:46.940
But I want to know it's also very easy to create a layer of recurrent neurons.

03:46.940 --> 03:51.980
Previously we just thought one recurrent neuron then we unrolled it through time but we could do the

03:51.980 --> 03:54.380
same thing for an entire layer.

03:54.470 --> 03:59.750
So if we want an entire layer of recurrent your aunts then it would look something like this where we

03:59.750 --> 04:03.890
have input X and then it goes through those recurrent neurons.

04:03.890 --> 04:05.180
Here we have three of them.

04:05.180 --> 04:10.940
Then we see the output y and then we take the output and then just pass it back in to all the neurons

04:10.970 --> 04:11.950
in that layer.

04:12.110 --> 04:17.960
And we could do the same idea and unroll this entire layer throughout time so that we get an input to

04:17.960 --> 04:26.180
equal zero and output it to equal zero and then pass that along with the output and input at time plus

04:26.180 --> 04:26.620
1.

04:26.730 --> 04:30.100
And same idea except now we're doing it with an entire layer.

04:30.140 --> 04:36.230
Now since the output of these recurrent neurons at a time that T is technically a function of all the

04:36.230 --> 04:42.540
inputs from previous times that you could then begin to think that has some form of memory because we're

04:42.560 --> 04:48.410
technically passing in historical information into that recurrent neuron or that layer of recurrent

04:48.410 --> 04:49.370
neurons.

04:49.370 --> 04:55.880
So part of this neural network that preserves some sort of stay across these types steps is called a

04:55.940 --> 05:01.120
memory cell and later on we're going to see much more complex examples of memory cells.

05:01.190 --> 05:07.250
But for now we have this basic recurrent neural near on that it just sends its output back into its

05:07.250 --> 05:07.930
input.

05:09.480 --> 05:13.420
Recurring year old networks are also very flexible in their inputs and outputs.

05:13.420 --> 05:18.640
So let's walk through a few examples of different combinations of inputs and outputs we can use for

05:18.640 --> 05:26.200
recurrent neural networks so we can actually perform a sequence input to a sequence output and an example

05:26.200 --> 05:32.780
of that would be passing in a set of Time series information such as a year's worth of daily sales data

05:33.180 --> 05:39.060
and then wanting back a sequence of that same sales data shifted over a certain time period into the

05:39.060 --> 05:39.600
future.

05:39.600 --> 05:44.010
And that's basically the initial example we first thought of that 1 2 3 4 5.

05:44.160 --> 05:51.420
And then you asked back for 2 3 4 5 6 TSOs a sequence however shifted over one step into the future

05:52.520 --> 05:53.730
another set of examples.

05:53.750 --> 05:59.710
Inputs and outputs would be to pass a sequence and put the request back a vector output.

05:59.780 --> 06:06.350
So a common example of using recurrent your own networks for this sort of input output is sentiment

06:06.350 --> 06:06.980
scores.

06:06.980 --> 06:13.550
So you can feed in a sequence of words maybe a paragraph of a movie review and then request back a vector

06:13.610 --> 06:17.490
indicating whether it was a positive sentiment such as they really liked the movie.

06:17.500 --> 06:22.460
So that's usually indicated by one versus they really dislike the movie which is indicated by usually

06:22.460 --> 06:24.020
negative 1 or 0.

06:24.020 --> 06:28.760
So that would be an example of a sequence input so that paragraph of words to just a single vector value

06:29.210 --> 06:33.770
and you essentially have a bunch of training data where you have a bunch of paragraphs and then some

06:33.770 --> 06:39.200
sort of sentiment score attached them and you train your recurrent neural network on that data to have

06:39.200 --> 06:41.690
a sequence input to a vector output.

06:41.750 --> 06:48.070
So on the opposite side you could also feed in a vector input so a single input at just a first time

06:48.140 --> 06:54.540
that and basically passen zeros for the rest of your time steps and then let the output be a sequence.

06:54.560 --> 06:58.180
So that's a vector to sequence network.

06:58.190 --> 07:05.150
So what you could do here is maybe pasan a single image and then request back a sequence of words describing

07:05.150 --> 07:06.740
the image that would be a caption.

07:06.920 --> 07:12.940
And if you've ever seen any research on line of images and auto generating captions you've seen examples

07:12.950 --> 07:18.080
of that where they show a picture of maybe a person on the beach and the neural network actually returns

07:18.080 --> 07:19.160
back a sentence.

07:19.160 --> 07:24.740
Man Standing on a beach or something of that nature obviously is a really complex networks take a lot

07:24.740 --> 07:29.150
of training time and a lot of data but it's definitely possible.

07:29.160 --> 07:33.530
So what we're going to do now is explore how we can build a simple recurrent in your own that work model

07:33.870 --> 07:39.420
with tensor flow manually then we're going to see how we can use tensor Flo's built in recurrent your

07:39.420 --> 07:41.170
own network API classes.

07:41.400 --> 07:46.110
So we're going to build a network manually still using tensor flow but a little more manually than usual.

07:46.110 --> 07:51.510
And then later on we'll see that Tanstaafl actually has a really nice API for recurrent neural networks

07:51.630 --> 07:54.540
that gives a nice level of abstraction to all of this.

07:54.540 --> 07:57.000
OK thanks everyone and I'll see you at the next lecture.

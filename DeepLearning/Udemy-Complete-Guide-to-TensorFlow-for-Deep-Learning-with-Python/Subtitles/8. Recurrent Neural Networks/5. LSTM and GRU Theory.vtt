WEBVTT

00:05.400 --> 00:11.220
Welcome everyone to this lecture we're going to be discussing Ellis T.M. and G.R. units or long short

00:11.220 --> 00:14.380
term memory neurons and gated recurrent units.

00:15.540 --> 00:20.370
Many of the solutions we just previously presented for vanishing gradients can also apply to recurrent

00:20.370 --> 00:25.830
neural networks that is using different activation functions applying Bachan formalizations etc..

00:26.100 --> 00:31.980
However because of the length of sequence input such as a really long time series input these could

00:31.980 --> 00:38.940
slow down training a possible solution would just be to shorten the actual sequence you use for prediction.

00:39.090 --> 00:42.310
But then that makes the model worse at predicting longer trends.

00:42.480 --> 00:49.120
So we don't really want to have to sacrifice shortening our inputs in other issue recurrent neural networks

00:49.120 --> 00:54.370
face is that after a while the networks are going to begin to forget those first inputs so we basically

00:54.370 --> 00:58.600
have some sort of memory loss as information is lost at each step going through the recurrent neural

00:58.600 --> 00:59.340
network.

00:59.380 --> 01:03.460
So we really need some sort of long term memory solution for our networks.

01:04.510 --> 01:09.730
This is where the long short term memory cell was created to help address some of these recurrent neural

01:09.730 --> 01:10.780
network issues.

01:10.900 --> 01:12.900
And it was introduced in 1997.

01:12.910 --> 01:19.480
So we're going to go through how an LACMA cell works but quickly recall that a typical recurrent neuron

01:19.510 --> 01:24.850
basically has this sort of structure where we have some input at T-minus one that we have the output

01:24.940 --> 01:31.040
at T minus one and that output is also fed back in a long input at t.

01:31.070 --> 01:34.770
So as a quick note these outputs are often called hidden.

01:34.790 --> 01:39.710
So we can have instead of 2st output at T minus one we can say H T minus one.

01:39.920 --> 01:44.420
And then that also gets fit in the input t that gets passed through some sort of activation function

01:44.630 --> 01:48.640
like the hyperbolic tangent then that gives the output h of t etc..

01:48.830 --> 01:54.300
So when you see HFT just kind of think of that as a typical output of a recurrent neuron cell.

01:54.440 --> 01:59.090
All right so here we can see the entire model of a long short term memory cell.

01:59.300 --> 02:03.800
And I know it can look really complicated if you see it in this format but it's actually not so bad.

02:03.830 --> 02:06.020
When you break it down in parts.

02:06.260 --> 02:11.320
So let's go ahead and first take a look at what's being inputted and what's being outputted here.

02:11.360 --> 02:14.770
We still have those original inputs from a normal recurrent neuron.

02:14.780 --> 02:17.610
Those h of T minus 1 and X of T.

02:17.780 --> 02:21.980
But note here we have this third input and we'll call that the cell state.

02:21.980 --> 02:28.410
So right now we're receiving the cell state at T-minus 1 and then the outputs we again output H A T

02:28.850 --> 02:30.700
and then we also have the cell state.

02:30.710 --> 02:33.140
So that's the current cell state c of T.

02:33.140 --> 02:38.480
So what we're going to end up doing is we're going to take an H of T minus 1 and X of T as well as the

02:38.510 --> 02:39.950
previous cell state.

02:39.950 --> 02:44.960
C of T minus 1 and will output h of T as well as the current cell state.

02:44.990 --> 02:45.950
C of T.

02:45.980 --> 02:52.990
So we're going to do this step by step so the very first step is called the forgets Gates layer.

02:53.180 --> 02:58.640
And this is going to be the first step or we decide what information are we going to forgets or throw

02:58.640 --> 03:00.990
away from the cell state.

03:01.130 --> 03:08.690
So what we end up doing here is the passen h of T minus 1 an X of t and we pass that in after performing

03:08.690 --> 03:13.640
a linear transformation with some weights and biased terms into a sigmoid function.

03:13.660 --> 03:16.640
I remember because this is essentially a sigmoid layer.

03:16.640 --> 03:23.590
It's always going to output a number between 0 and 1 and a 1 is going to represent to keep it and a

03:23.600 --> 03:27.050
zero is going to represent to forget about it or get rid of it.

03:27.320 --> 03:33.080
So if you think back to maybe a language model where we're trying to predict the very next word based

03:33.080 --> 03:39.200
on previous ones a cell state might include the gender of the present subject so that you end up picking

03:39.200 --> 03:40.510
the correct pronoun.

03:40.580 --> 03:42.320
So we end up seeing a new subject.

03:42.350 --> 03:45.540
You want to forget about the gender of the old subject.

03:45.590 --> 03:53.440
So that may be used use for a forget gait later when working with natural language as a sequence.

03:53.450 --> 03:59.410
Now the very next step is to decide what new information are you going to store in the cell state.

03:59.420 --> 04:01.270
So we already decided what we're going to forget.

04:01.280 --> 04:04.960
Now we need to decide what are we actually going to store in the cell state.

04:04.970 --> 04:07.030
Remember that c of T.

04:07.100 --> 04:09.600
So the first part is a sigmoid layer.

04:09.650 --> 04:12.320
And the second part is a hyperbolic tangent layer.

04:12.320 --> 04:17.750
So let's go ahead and tackle that first part that sigmoid layer so that sigmoid layer is called the

04:17.840 --> 04:19.080
input gate layer.

04:19.100 --> 04:22.690
So we'll say that's equal to I have to look for input gate layer.

04:22.880 --> 04:29.870
And again we take an H of T minus 1 and X of t do a linear transformation on it with W of II plus B

04:29.890 --> 04:36.790
of-I we pass that into a sigmoid function and again now we have a bunch of values between 0 and once.

04:36.970 --> 04:40.180
Then the second part of this is the hyperbolic tangent layer.

04:40.400 --> 04:45.880
And that's again going take h of T minus 1 and ex-city do that linear transformation and then pass it

04:45.920 --> 04:47.410
through a hyperbolic tangent.

04:47.660 --> 04:51.980
So that ends up creating a vector of what we call new candidate values.

04:51.980 --> 04:55.250
So that's that c of T with a little tilde above it.

04:55.250 --> 05:00.170
So these are candidate values that could be added to the state and then the next step we're going to

05:00.170 --> 05:04.540
combine these two to create an update to the cell state.

05:04.580 --> 05:09.560
So if we think back to an example of a language model we essentially want to add the gender of the new

05:09.560 --> 05:16.090
subject to the cell state and replace the old one that we've already decided we're forgetting.

05:16.120 --> 05:22.270
So now it's time to update the old cell state to remember the old cell state is c of T minus one and

05:22.270 --> 05:29.470
we eventually want to update that to the new cell say c of T so we can pass that on to the T plus one

05:30.220 --> 05:32.130
state of the cell.

05:32.140 --> 05:35.260
So in the previous steps we already decided what we're going to forget.

05:35.320 --> 05:37.950
And we also already decided what we're going to store.

05:37.960 --> 05:42.370
So now we need to do is just perform or execute these actions.

05:42.370 --> 05:48.780
So what we end up doing is we multiply the old state c of T minus one by f of T.

05:48.790 --> 05:53.200
So we end up forgetting the things that we essentially decided we're going to forget based off that

05:53.200 --> 06:00.400
first sigmoid layer then we're going to add I have t that input gate layer times those candidate values

06:00.400 --> 06:02.590
c of T with tilde on top.

06:02.620 --> 06:08.170
So these are the new Kennedy values and now they're being scaled by how much we decided to update each

06:08.260 --> 06:09.370
state value.

06:09.700 --> 06:14.440
So if we think back to a case of a language model this is where we would actually drop the information

06:14.740 --> 06:20.980
about that old subjects gender and add in new information based off what we decided in the previous

06:20.980 --> 06:22.540
steps.

06:22.540 --> 06:27.460
Now our final decision is going to be what do we output for H of T.

06:27.520 --> 06:30.130
So this output is going to be based off your cells.

06:30.190 --> 06:31.930
That is just a filter diversion.

06:32.200 --> 06:36.790
So it's actually pretty straightforward now we just using H A T minus 1 an X of T.

06:36.880 --> 06:43.030
We passed that after a linear transformation into the sigmoid layer and that's going to decide what

06:43.030 --> 06:46.310
parts of the cell state were going to be outputting.

06:46.380 --> 06:50.190
Then we put the cells stay through the hyperbolic tangent.

06:50.260 --> 06:54.020
So that's going to push the values to be between negative 1 and 1.

06:54.160 --> 06:59.730
And we're going to then multiply it by the output of that sigmoid again so that we only output the parts

06:59.740 --> 07:01.180
we decided to.

07:01.180 --> 07:03.790
So that's what's occurring here in this red line.

07:03.790 --> 07:09.130
Again we're taking H A T minus 1 an X A T doing a linear transformation on it passing it through the

07:09.130 --> 07:12.100
sigmoid And then once we have that output.

07:12.190 --> 07:14.600
So we're going to say that's o of T output of T.

07:14.620 --> 07:21.000
We're then going to multiply by the hyperbolic tangent of the sea of t or that current cell state.

07:21.100 --> 07:23.220
And that gives us h of T.

07:23.320 --> 07:25.740
So that's how an elist cell works.

07:27.400 --> 07:30.010
Now there's different variants on an elist himself.

07:30.040 --> 07:33.670
So there's a variance called the peep hole variance.

07:33.700 --> 07:36.160
So this is actually quite popular.

07:36.160 --> 07:40.660
And the reason for that is because it adds peepholes to all the gates.

07:40.660 --> 07:50.110
So basically it allows these f of t of t and O of t to be able to see the previous cell state.

07:50.110 --> 07:51.640
Or C of T minus 1.

07:51.670 --> 07:56.700
So you can see here now instead of being a function of just h of T minus 1 and X of T.

07:56.770 --> 07:59.060
We're also passing in a C of T minus one.

07:59.200 --> 08:03.310
And then for the output we're also passing in a C T directly.

08:03.310 --> 08:06.170
So that's called LS T.M. with peepholes.

08:06.200 --> 08:12.670
Another variation of the LS tiam cell is called the gated recurrent unit or G or you and this was actually

08:12.670 --> 08:18.670
introduced quite recently around 2014 and what it ends up doing is it actually simplifies things a bit

08:18.940 --> 08:24.470
by combining the forget and input gates into a single what they call update Gates.

08:24.610 --> 08:27.100
And it also merges the cells stay and hidden state.

08:27.340 --> 08:33.420
And it makes a few other changes so this resulting model is actually simpler than standard elist him

08:33.430 --> 08:33.940
models.

08:33.970 --> 08:39.300
And because of that it's been growing increasingly popular over the last few years that it's been around.

08:39.310 --> 08:44.260
So there's actually lots of other slight variations off of this and they're being introduced all the

08:44.260 --> 08:44.650
time.

08:44.650 --> 08:50.260
There was another one called depth gated recurrent neural networks and that was introduced in 2015.

08:50.440 --> 08:54.940
And I mean who knows maybe in a few years we'll see another variation that further improves on this

08:54.940 --> 08:55.720
model.

08:55.720 --> 09:01.960
But the main idea is to understand how elist teams work and that allows you to quickly learn how these

09:01.960 --> 09:03.500
variations work off of it.

09:05.160 --> 09:10.540
Now fortunately we won't actually have to implement all of that logic ourselves tensor flow comes a

09:10.620 --> 09:13.640
really nice year on models built in a nice API.

09:13.740 --> 09:19.810
So it makes it really easy to swap in say in LSD hemself for a GI or use cell.

09:19.980 --> 09:24.290
Essentially it's just a call to tensor flow which is why people use tensor flow in the first place.

09:24.290 --> 09:29.730
So because of this we're going to do now is explore using this tensor flow recurrent neural network

09:29.730 --> 09:34.350
API for time series prediction and generating new time series data.

09:34.350 --> 09:39.780
So we've gone through kind of the more difficult parts of understanding vanishing gradients LACMA cells

09:39.810 --> 09:41.270
gaitered recurrent units.

09:41.280 --> 09:46.440
Now it's time to actually code this out and take it full advantage of tensor Flo's API.

09:46.440 --> 09:48.920
All right thanks everyone and I'll see you at the next lecture.

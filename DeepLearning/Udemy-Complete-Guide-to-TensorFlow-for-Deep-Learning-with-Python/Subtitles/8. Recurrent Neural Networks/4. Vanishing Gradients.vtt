WEBVTT

00:05.400 --> 00:06.960
Welcome back everyone.

00:06.990 --> 00:13.500
Before we actually dive into using tensor Flo's nice API functionality to build recurrent your own that

00:13.500 --> 00:15.270
works for time series analysis.

00:15.300 --> 00:21.180
I want to discuss a few key challenges for analyzing larger time series data sets with recurrent neural

00:21.180 --> 00:25.150
networks and one of those key problems is the vanishing gradients issue.

00:26.860 --> 00:33.760
Back propagation recall goes backwards from the output layer to the input layer and it propagates back

00:33.790 --> 00:36.790
that error gradient for deeper networks.

00:36.790 --> 00:43.480
Issues can arise from back propagation sometimes called vanishing or exploding gradients.

00:43.510 --> 00:49.270
So as you go back to these lower level layers or these front layers closer to the input gradients often

00:49.270 --> 00:54.250
get smaller and eventually causing whites to never change at these lower levels.

00:54.250 --> 01:00.260
The opposite problem can also occur for activation functions who used to rivet that take on larger values.

01:00.310 --> 01:02.990
So that's an exploding gradient problem.

01:03.010 --> 01:04.540
So that can also cause issues.

01:04.570 --> 01:11.420
However it's not as common as the Vanish Ingredion problem so discuss why this problem occurs especially

01:11.420 --> 01:14.740
how it relates to activation functions and how we can fix it.

01:14.870 --> 01:19.670
Then the next lecture will discuss these issues specifically to recurrent neural networks and how we

01:19.670 --> 01:24.600
can use specialized neuron units such as Ellis T.M. and gear you to fix these problems.

01:24.600 --> 01:30.140
But for now let's look at activation functions as the source of this problem and see different activation

01:30.140 --> 01:31.620
functions that can help solve it.

01:32.550 --> 01:38.490
So why do these vanish ingredients actually happen in relation to the activation function choice.

01:38.490 --> 01:42.630
Well if we take a look at some of the typical activation functions the traditional ones that we've been

01:42.630 --> 01:48.310
looking at such as sigmoid activation we have this sort of curvature that goes from 0 to 1.

01:48.360 --> 01:53.900
Remember that the Cygwin activation is just 1 over 1 plus E to the negative x or negative input.

01:54.150 --> 01:58.470
And then there's also a traditional activation function such as the hyperbolic tangent that we've seen

01:58.470 --> 02:03.240
before which essentially has the same shape except it goes from negative one to one and sort of from

02:03.240 --> 02:04.410
zero to 1.

02:04.410 --> 02:09.480
So we have these traditional activation functions and recall that back propagation computes the grievance

02:09.480 --> 02:11.520
by the chain rule.

02:11.540 --> 02:17.940
Now if you have an input going into the sigmoid function or this hyperbolic tangent function and it's

02:17.940 --> 02:23.310
quite a large input then your gradient or your slope coming out of this activation function ends up

02:23.310 --> 02:24.870
being very close to zero.

02:24.930 --> 02:28.850
And it's the same deal if you have really large negative numbers.

02:29.250 --> 02:36.720
So this chain rule of back propagation computed the gradients has the effect of multiplying n of these

02:36.720 --> 02:42.290
small numbers to compute gradients of these lower or front layers in an analyser network.

02:42.300 --> 02:49.230
And basically what that means is the gradients is going to decrease exponentially with N as you get

02:49.230 --> 02:54.540
more and more layers while the front layers train very slowly because you're multiplying these small

02:54.540 --> 02:59.200
numbers over and over again which is just going to lead to smaller smaller numbers.

03:00.540 --> 03:04.830
So one way to solve this is attempting to use different activation functions.

03:04.830 --> 03:09.720
So we've already heard of that activation function that says the rectified linear unit and that's what

03:09.720 --> 03:10.720
it looks like.

03:10.780 --> 03:15.870
Every call here that the rectified linear unit is then not going to saturate positive values.

03:15.870 --> 03:20.880
So unlike the sigmoid function you can see here that even if we have large positive values we don't

03:20.880 --> 03:25.030
end up saturating them with any gradient that's coming close to zero.

03:25.050 --> 03:30.090
Now an issue with this rectified linear unit is that for negative numbers it's always going to output

03:30.180 --> 03:32.070
zero or they're.

03:32.090 --> 03:37.430
So there's another activation function called a leaky rectified linear unit which then has sort of a

03:37.430 --> 03:42.890
negative slope for negative numbers in the input.

03:42.900 --> 03:48.300
There's also other activation functions such as an exponential linear unit which also attempts to solve

03:48.300 --> 03:51.070
some of the problems that lead to vanishing gradients.

03:51.180 --> 03:55.500
So hopefully you get this idea that we can use different activation functions to try to solve this problem

03:56.970 --> 04:03.240
and other solution is to perform batch normalization where your model normalize each batch using the

04:03.240 --> 04:09.080
batched mean and the standard deviation so apart from rational realization Researchers have also used

04:09.260 --> 04:14.660
what they call a gradient clipping where gradients are cut off before reaching a predetermined limit.

04:14.660 --> 04:21.090
So for example you would cut off gradients to always be between negative 1 and 1 so recurrent neural

04:21.090 --> 04:24.610
networks for time series analysis present their own gradient challenges.

04:24.630 --> 04:29.670
So we're going to explore special neuron units specially designed for recurrent neural that works especially

04:29.670 --> 04:32.880
for time series analysis with them to help fix these issues.

04:32.880 --> 04:36.600
So you'll see that the next lecture where we discuss these specialized neuron units.

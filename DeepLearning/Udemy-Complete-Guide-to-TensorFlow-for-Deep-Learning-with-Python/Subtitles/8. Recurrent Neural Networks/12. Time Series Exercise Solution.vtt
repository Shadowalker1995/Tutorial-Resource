WEBVTT

00:05.270 --> 00:08.980
Welcome everyone to the solutions lecture for the time series exercise.

00:08.990 --> 00:12.700
Let's hop over to the exercise notebook and start coding along the solutions.

00:12.950 --> 00:14.570
OK here I am at the notebook.

00:14.600 --> 00:21.560
First thing to do is import some pie and then we're also going to need to import Pandurs to work with

00:21.560 --> 00:25.490
that CXXVI file and then we'll do some visualization later on.

00:25.520 --> 00:33.140
Let's say I live as tea and then map plot lib in line OK.

00:33.150 --> 00:35.700
So we're going to use pandas to read and to see as we file.

00:35.700 --> 00:39.740
So I going to say milk is equal to p the read C S V.

00:39.870 --> 00:42.480
And then we'll say monthly milk production.

00:42.510 --> 00:47.460
So just as an untapped autocomplete there and then I'm also going to say that the index column is equal

00:47.460 --> 00:48.940
to month.

00:49.500 --> 00:53.740
So check out the head of the data frame it should look something like that.

00:54.150 --> 00:56.880
And then we're going to do next is set the index.

00:56.880 --> 01:00.460
So we'll do the following basically just copy and paste that line.

01:00.480 --> 01:02.190
So milk index.

01:02.230 --> 01:07.480
Copy that run that and then I'm also going to plot out the time series data.

01:07.480 --> 01:13.510
So that's actually really easy only if you say milk dot plot and it should create this milk production

01:13.510 --> 01:14.930
plot automatically for you.

01:15.130 --> 01:19.100
And this happens automatically because of the index being a date time index.

01:19.170 --> 01:22.360
So Pan those again really convenient to use there.

01:22.420 --> 01:24.310
Now it's time for the train to split.

01:24.700 --> 01:31.780
So remember we want to actually have the last 12 months be the tail end of our test set.

01:32.110 --> 01:39.620
So we're going to do is if we say milk that info you'll notice I have a hundred and sixty eight rows.

01:39.820 --> 01:48.770
So what I'm going to do is say my training set is equal to milk dot head.

01:49.050 --> 01:54.730
And then I'm going to say essentially 168 minus 12 Well that's equal to 156.

01:54.750 --> 02:03.900
So I'll put in 156 here and then I want the test set to be the talents all say milk tail last 12 months

02:04.430 --> 02:05.690
and that's a really easy way.

02:05.730 --> 02:13.740
You could also use I Alosi or I look with some slicing to do the same tasks but head and tail just makes

02:13.740 --> 02:14.870
it really easy.

02:14.880 --> 02:20.790
So in other ever trainset those first months or first years and the tail is just the last year we want

02:20.790 --> 02:22.080
to scale the data.

02:22.080 --> 02:32.290
So what we're going to do here now say from as Kaylor pre-processing import min max scaler are going

02:32.290 --> 02:41.280
to create an instance of the scaler will say minimax scaler and then I will say train scaled and then

02:41.280 --> 02:48.120
I'm going to do perform a transform only all the training set and then I'm going to say test scaled

02:49.920 --> 02:54.460
and I'll say scaler transform and this is all the test set.

02:54.650 --> 02:57.210
So now we've been able to successfully scale our data.

02:57.230 --> 03:02.210
Remember we only want to fit it to the training set if we fit it to the entire set then we're assuming

03:02.210 --> 03:04.520
we'll know about future behavior when scaling.

03:04.520 --> 03:09.730
So we only want to do the fit transform on the training and then just transform on the test.

03:09.740 --> 03:11.660
Up next is this batch function.

03:11.660 --> 03:13.550
So let's go ahead and create it.

03:13.550 --> 03:14.540
It's actually not so bad.

03:14.540 --> 03:16.860
It's really similar to what we did during the lectures.

03:16.970 --> 03:18.660
We create some random starting point.

03:18.690 --> 03:25.730
So I'm going to say N.P. ran them and I'm going to choose some random integer between 0 and the length

03:25.850 --> 03:28.530
of the training data that's being passed then.

03:28.580 --> 03:31.150
So that's this training data appear.

03:31.430 --> 03:37.930
So I will say training data and then I'm also going to subtract the number of steps.

03:37.930 --> 03:40.090
So that's essentially what these instructions are trying to tell you.

03:40.090 --> 03:41.520
So use this.

03:41.560 --> 03:45.640
We ran them that ran that to set a random starting point index for the batch.

03:45.680 --> 03:48.900
Remember that each batch needs to have the same number of steps in it.

03:49.120 --> 03:52.940
So that means you should limit the starting point to zero.

03:53.060 --> 03:58.480
Or excuse me the starting point to length of data minus steps which is going to be length for training

03:58.480 --> 03:59.850
data mining steps.

04:00.010 --> 04:00.880
Okay.

04:01.480 --> 04:03.160
So there is our starting point.

04:03.160 --> 04:13.680
Step two is to create a y data for this all say why batch is equal to and pray you trick taking that

04:13.680 --> 04:25.850
training data and then say go from Branch start all the way to Rand start plus the number of steps plus

04:25.850 --> 04:34.740
1 and then we just need to reshape this to be shapes of one wups steps plus one.

04:34.740 --> 04:38.140
So again basically what the comment that steps we're telling you to do.

04:38.130 --> 04:44.680
Then finally we just need to return the batches so I get return y batch.

04:45.140 --> 04:56.010
All those rows and then will say one step backwards there reshape negative one steps one and then will

04:56.010 --> 04:56.690
also return.

04:56.700 --> 05:03.480
Why batch and essentially all the rows except one of say one to the verion here and then we'll reshape

05:03.480 --> 05:08.690
that to be negative 1 steps 1 OK.

05:08.720 --> 05:11.630
So now we have our next batch function.

05:11.630 --> 05:14.110
It's time to actually set up the recurrent neural network model.

05:14.150 --> 05:21.970
So the first thing you need to do is say import sensor flow as T.F. and then up next we have the constants.

05:22.340 --> 05:25.320
So let's go ahead and fill these all out.

05:25.330 --> 05:29.960
So essentially just use the ones that provided or play around with them totally OK as well.

05:30.100 --> 05:31.070
The number of inputs.

05:31.090 --> 05:35.970
Well that's just going to be one feature which essentially is the time series so that stays is one than

05:35.980 --> 05:37.050
the number of time steps.

05:37.060 --> 05:40.240
So the number of steps in each batch will go in limit that to just 12.

05:40.240 --> 05:46.530
Since we're dealing with monthly data so a number of time steps it's going to be 12 and then the number

05:46.530 --> 05:50.280
of neurons will go ahead and just say 100 just like we did last time during the lectures.

05:50.280 --> 05:54.350
Again a value can easily play around with a number of outputs.

05:54.350 --> 05:58.040
Are you going to create one sequence all number of up it's going to be one.

05:58.290 --> 06:00.480
Then we have a learning rates.

06:00.480 --> 06:02.690
Then we go ahead and choose a bit of a faster learning rate.

06:02.680 --> 06:09.600
So like 0.03 And then number of training iterations we'll say let me scroll down here just a little

06:09.600 --> 06:15.280
bit number of training iterations going to set that equal to 4000.

06:15.310 --> 06:20.580
Again another valley you can play with and then the batch size will go ahead and just feed in one at

06:20.580 --> 06:21.100
a time.

06:22.590 --> 06:25.610
So now we can create placeholders for x and y.

06:26.400 --> 06:36.040
So I'll say X is T.F. placeholder and it's going to be float 32 and then the shape will say none because

06:36.040 --> 06:37.750
that's defined by the bat size.

06:38.020 --> 06:41.170
And then we have the number of time steps so 12.

06:41.560 --> 06:48.430
And then by the number of inputs and that's going to be almost the same thing for why except instead

06:48.430 --> 06:50.900
of number of inputs that's going to be number of outputs

06:53.630 --> 06:54.440
There we go.

06:54.740 --> 06:57.230
So now we have X and Y are placeholders there.

06:57.260 --> 07:02.130
It's time to create the recurrent neural network where you have a lot of stuff you can play around with

07:02.150 --> 07:02.890
here.

07:03.320 --> 07:11.800
So I'll just do the basic one will say T.F. contrib Arnon n ill say output projection mapper because

07:11.890 --> 07:18.910
that's going to allow us to make up their go up prediction mapper or output projection rapper excuse

07:18.910 --> 07:24.400
me because that's going to allow us to basically just get one output back and then I'm going to wrap

07:24.580 --> 07:30.510
the T.F. contrib Arnon and we'll use a basic LACMA cell

07:33.510 --> 07:37.250
where the number of units is going to be equal to the number of neurons.

07:37.260 --> 07:44.890
That's just the 100 the activation that's going to be equal to T.F. and rectified linier act rectified

07:44.920 --> 07:52.120
linear unit and then we have the output size that's going to be set up that size is equal to the number

07:52.120 --> 07:52.770
of outputs.

07:52.810 --> 07:55.160
In this case just one.

07:55.210 --> 07:55.930
So let's run that.

07:55.930 --> 07:59.180
Looks like I got my column mixed up.

07:59.290 --> 08:02.680
So output size that goes into the upper projection wrapper.

08:02.680 --> 08:03.270
There we go.

08:05.220 --> 08:05.690
OK.

08:05.990 --> 08:11.180
So we have our output projection wrapper that's outputting this just to be 1 output and that's wrapping

08:11.180 --> 08:12.170
this basic elist.

08:12.190 --> 08:16.520
So with a 100 year odds so that we don't get 100 outputs Instead we just have the output protection

08:16.520 --> 08:18.880
wrapper sending just one output.

08:18.920 --> 08:23.800
Next we want to pass in the cells variable into T.F. and in dynamic Arnon.

08:23.810 --> 08:24.960
So let's go ahead and do that.

08:26.700 --> 08:33.870
So we get back outputs and states just like we did in lecture with CTF and then we have a dynamic Arnon

08:34.410 --> 08:37.200
we have a cell X that placeholder.

08:37.230 --> 08:45.580
And we also specify the data type so data type is going to be as always Tier 3 to next we have the last

08:45.580 --> 08:47.170
function and the optimizer.

08:47.170 --> 08:59.120
So this is essentially same as the lectures will say to reduce mean staff reduce mean and then TFA squares

08:59.120 --> 09:03.310
is the means quarter the product that outputs minus Y.

09:03.330 --> 09:04.920
That true placeholder.

09:05.140 --> 09:14.670
And then let's go ahead and create our optimizer will use the atom optimizer and we'll set its learning

09:14.670 --> 09:17.460
right to be the learning rate with the above.

09:17.850 --> 09:24.930
And then we want to set train equal to optimizer and we want the optimizer to end up minimizing or say

09:24.990 --> 09:33.830
minimize loss that we need to initialize the global variables so we'll see it is equal to T.F. global

09:33.830 --> 09:35.430
variables initialiser.

09:36.050 --> 09:41.000
And then finally I want to create an instance of T.F. transceiver so I can see if my model will say

09:41.000 --> 09:46.790
saver is equal to T.F. train sabor.

09:47.510 --> 09:51.590
OK now it's time to actually run the session so we're going to run a session that trains in the batches

09:51.590 --> 09:53.320
created by our next batch function.

09:53.600 --> 09:58.450
And it's also going to add in that last evaluation just like we did during the lecture.

09:58.520 --> 10:03.420
Going to add in a quick line here for my GPS options just in case.

10:03.600 --> 10:11.990
So say T.F. GPU options and you don't need to do this if you're not using the actual GPU version of

10:11.990 --> 10:12.820
Tancer flow.

10:14.060 --> 10:17.290
So I need to save per process.

10:17.460 --> 10:25.120
GP You memory underscore fraction and will use 90 percent.

10:25.120 --> 10:28.350
My GP is memory.

10:28.510 --> 10:31.260
OK so that I'm going to passen that config.

10:31.440 --> 10:33.630
So let me just go ahead and copy and paste it.

10:33.630 --> 10:37.840
Then again you don't need to do this.

10:37.940 --> 10:40.490
So there we go GP units ready to go.

10:40.490 --> 10:43.030
And now for the code.

10:43.030 --> 10:48.770
So I'm going to say a session run and it's and then I'll do the following.

10:48.770 --> 10:55.400
I'll say for iteration in a range and then I'm going to say some train iterations

10:59.040 --> 11:07.560
XT batch y batch and then I'm going to call my next batch function on my scale data.

11:07.560 --> 11:13.710
So remember I have my trained skilled data and then my batch size that I defined earlier it's going

11:13.710 --> 11:17.360
to pass that in as well as the number of times steps I passed in earlier.

11:17.520 --> 11:20.650
So those are the concepts I find all the way up here.

11:21.760 --> 11:29.260
So scrolling back down now how are batches going to run our train variable.

11:29.320 --> 11:31.360
And we need our feed dictionary.

11:31.360 --> 11:46.880
So that's going to be x x batch y y batch that I'll say if my iteration Model 100 is equal to zero then

11:46.910 --> 11:55.320
I mean squared air is equal to last evil and we'll have a feed dictionary here where X isn't going to

11:55.340 --> 12:04.840
equal to or passen X batch and Y person white batch The other thing we could do isn't passing just on

12:04.840 --> 12:05.880
our test data.

12:07.010 --> 12:13.640
But right now just keep it the same way we had in the lecture that are going to say iteration is all

12:13.640 --> 12:15.350
the same as we did in the lecture.

12:15.510 --> 12:18.690
I mean square error comma.

12:18.750 --> 12:25.400
MSE OK then that's going to save this and pass the Senate's code along just so I don't overwrite anything.

12:25.400 --> 12:27.200
I would recommend you do the same.

12:27.200 --> 12:30.230
So we'll run that and hopefully be in the training.

12:30.530 --> 12:31.110
OK.

12:31.310 --> 12:33.950
So this is going to then train for 4000 times.

12:33.950 --> 12:37.160
That's something to hop for on time until this is done training.

12:37.160 --> 12:41.070
All right so my model has now finished running and it's gone to a pretty low MSCE.

12:41.150 --> 12:47.540
And as a quick note I previously made a little typo here on Savir so I had a rerun that to rerun the

12:47.540 --> 12:49.260
entire session during the training.

12:49.460 --> 12:51.320
But now there's trying for 4000 steps.

12:51.320 --> 12:53.300
It's time to predict the future.

12:53.360 --> 12:54.740
So we're going to show the test set.

12:54.770 --> 12:56.270
So let's go ahead and do that.

12:56.270 --> 12:58.930
Remember a test set just looks like this.

12:58.940 --> 13:04.010
So a test set is the month that time stamp column and then the milk production.

13:04.010 --> 13:09.050
And we want to actually add in our generated data.

13:09.050 --> 13:10.470
So we're going to scroll down here.

13:10.640 --> 13:13.260
So we want to attempt to predict those 12 months of data.

13:13.310 --> 13:15.730
So we're going to create generative session.

13:15.770 --> 13:20.960
So let's go ahead and do that we'll see with T.F. session as s.c.s I'm going to restore the model I

13:20.960 --> 13:21.850
just created.

13:22.070 --> 13:24.100
So let's copy and paste that string.

13:24.110 --> 13:27.710
So it's stored as time series model code along.

13:27.990 --> 13:35.530
So don't come back down here and place that in so we're going to restore that model and then I'm going

13:35.530 --> 13:42.910
to set a training seemed to be equal to list and it's going to be my scaled training data.

13:42.910 --> 13:46.540
Except it's not going to include the last 12 months.

13:46.690 --> 13:49.580
So I'm going to see it with the entire training set.

13:50.680 --> 13:55.530
So remember in the lectures we seeded it first of zeros and then a little bit of a sinusoidal wave.

13:55.540 --> 14:00.400
Now I'm going to see it an entire training set and see what it says for the test set that last 12 months

14:00.400 --> 14:01.360
of data.

14:01.510 --> 14:13.150
So I'll say for iteration in range 12 because I want 12 months I'll say my next batch is equal to an

14:13.150 --> 14:18.990
umpire rating of my train speed.

14:19.430 --> 14:25.350
And we're going to go from number of time steps remember this just 12.

14:25.370 --> 14:29.220
So from 12 from the very end all the way to the end.

14:29.720 --> 14:37.040
And then I need to reshape this to be one by number of times that's by.

14:38.210 --> 14:44.500
And then we're going to have our white widespread and that's going to be session run then we're going

14:44.500 --> 14:53.320
to go outputs and we'll create a feed dictionary where we have X and then that X batch and then finally

14:53.320 --> 15:01.110
we're going to append to our training seed y predicted values.

15:01.180 --> 15:05.370
So we need to index actually grab them up.

15:05.470 --> 15:07.710
So let's go in and run this.

15:08.170 --> 15:10.170
And now we should have a train seat.

15:10.390 --> 15:12.040
So we're going to show a result of the prediction.

15:12.040 --> 15:19.330
So if I train speed you should get some predictions that look more or less like this.

15:19.350 --> 15:20.730
So let's see how he did.

15:20.820 --> 15:21.140
All right.

15:21.140 --> 15:25.710
The next step is to grab the portion of the results that are actually that generated values.

15:25.710 --> 15:30.350
So if we take a look at train speed that's going to be everything past 12.

15:30.360 --> 15:31.900
So here are my first 12.

15:32.050 --> 15:37.440
I want to grab these results right here and then apply inverse transform because remember right now

15:37.500 --> 15:40.710
these are not the correct units they are still scaled to units.

15:40.710 --> 15:42.340
So let's go ahead and do that.

15:44.290 --> 15:51.470
We'll see the result is equal to scaler and then I'm going to call inverse transform we'll say NPR-A

15:52.970 --> 16:02.300
train speed from 12 all the way onwards and then I need to reshape this to be 12 1 and then I want to

16:02.300 --> 16:04.630
create a new column on the test called generated.

16:04.670 --> 16:07.310
So let's go ahead and make that happen also.

16:07.340 --> 16:07.930
Test set

16:10.580 --> 16:14.480
generated it's going to be able to results.

16:14.480 --> 16:17.030
So going to run that you make it a warning here.

16:17.060 --> 16:18.000
Feel free to ignore it.

16:18.010 --> 16:23.620
It's just basically telling you that you may be setting a copy then we're going to view the test set

16:23.710 --> 16:24.150
data.

16:24.150 --> 16:26.040
So let's go in and see that test set.

16:26.330 --> 16:28.910
And now it should look something like this.

16:29.000 --> 16:31.280
So you can see here we are definitely overestimating.

16:31.280 --> 16:34.130
So we'll have to explore that when we actually plot this out.

16:34.130 --> 16:35.940
So let's go ahead and do that.

16:36.050 --> 16:41.350
We'll say test set the plots and let's see how far off we're.

16:41.390 --> 16:41.740
All right.

16:41.750 --> 16:49.460
So the good news is we're definitely approaching that behavior but unfortunately we're overestimating

16:49.520 --> 16:50.530
the milk production.

16:50.550 --> 16:52.340
To what actually happened.

16:52.400 --> 16:58.490
So what we can do is try to maybe use a different neuron type and see if we can have more information

16:58.490 --> 16:58.800
there.

16:58.850 --> 17:01.770
So let's go in and go back and try to fix that.

17:02.090 --> 17:07.520
So I'm going to scroll back up here to where we actually chose the recurrent neural network type.

17:07.520 --> 17:14.300
All right so what I'm going to try doing is changing the cell type to a gated recurrent unit and then

17:14.330 --> 17:19.550
I'm going to expand the number of iterations but then because of that I'll give it a slower learning

17:19.550 --> 17:20.230
rate.

17:20.360 --> 17:22.330
So let's go ahead and try that out.

17:22.430 --> 17:25.950
And I'm going to hop forward in time and see what the results are.

17:25.970 --> 17:26.350
All right.

17:26.360 --> 17:31.250
So looks like that resulted in a much closer fitting to the actual test data.

17:31.280 --> 17:37.010
A few things to keep in mind is that our original model is still only being trained to predict the time

17:37.010 --> 17:38.990
series one step ahead.

17:39.050 --> 17:44.930
And it's kind of a lot to ask it to generate 12 full steps ahead which is why you'll never end up getting

17:44.930 --> 17:45.820
perfect results.

17:45.860 --> 17:47.470
Given the way we're trained the model.

17:47.720 --> 17:54.620
So somebody could end up doing in the future is trying to if the original model train it with an error

17:54.770 --> 17:58.690
of not just one step ahead but a full 12 steps ahead.

17:58.820 --> 18:02.780
Now in issue with this particular training set is that it's a little too small and it's going to let

18:02.780 --> 18:07.850
me your options as far as the amount of data you have if you're going to end up testing and generating

18:08.120 --> 18:09.570
a full 12 steps ahead.

18:09.650 --> 18:13.430
But if you do take that approach you should see even better results.

18:13.430 --> 18:16.040
All right thanks everyone and I'll see you at the next lecture.

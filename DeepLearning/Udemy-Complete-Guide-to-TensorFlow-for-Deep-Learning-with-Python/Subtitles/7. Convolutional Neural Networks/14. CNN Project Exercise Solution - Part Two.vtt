WEBVTT

00:05.250 --> 00:09.160
Welcome everyone to part two of the solutions for the Safar it's an exercise.

00:09.360 --> 00:12.660
Let's go right to where we left off last time and finish this up.

00:12.660 --> 00:14.870
All right so mainly the solutions.

00:14.940 --> 00:19.980
One was just going through the code that was already provided for you in understanding how it actually

00:19.980 --> 00:23.010
works and how it actually produces images.

00:23.010 --> 00:24.780
Now it's time to actually create the model.

00:24.930 --> 00:33.380
So we're going to do the following I'll say imports tensor flow as T.F. and then we're going to create

00:33.380 --> 00:38.010
two placeholders one for data X and then y true.

00:38.060 --> 00:39.770
So their shapes should make sense to you.

00:39.770 --> 00:49.620
Now we have T-Rexes a placeholder and it's going to be T.F. flow thirty two or three Remember the need

00:49.620 --> 00:56.010
to provide a data type and the shape is just going to be none because that's dictated by the batch size.

00:56.150 --> 01:06.360
And then we're doing 3 2 3 2 3 3 2 3 2 pixels three color channels then we're going to say why true

01:07.140 --> 01:13.380
is equal to a placeholder as well to 32 as well.

01:13.380 --> 01:17.390
And then this shape is just going to be none because of the batch size.

01:17.400 --> 01:20.130
And then 10 because it's one encoded.

01:20.210 --> 01:26.690
So again your typical shapes is going to be images height width and then channels just like we did for

01:26.690 --> 01:29.320
the digits data set.

01:29.350 --> 01:34.900
So I had these two placeholders for my data and my labels and I'm going to create one more cold hold

01:34.900 --> 01:42.370
probability and no need for shape here because it's essentially going to be a single number of what

01:42.370 --> 01:50.990
percentage you want for the probability that neurons are being hold when we actually perform dropout.

01:50.990 --> 01:55.110
All right so now we have a whole probability then we have the helper functions.

01:55.130 --> 01:59.600
So you have the option of grabbing the helper functions or seeing if you can recreate them yourself

01:59.600 --> 02:00.780
for hard challenge.

02:00.950 --> 02:07.850
Well just go ahead since you've already gone through these in great detail during the mist with CNN

02:08.810 --> 02:10.000
we'll just copy and paste them here.

02:10.010 --> 02:12.760
So remember we initialize weights initialized bias.

02:12.860 --> 02:17.850
We have these helper functions for a to the convolution Max pool two by two.

02:18.050 --> 02:19.100
And then convolutional.

02:19.130 --> 02:20.660
And normal full layer.

02:20.660 --> 02:25.940
And if you want you can kind of play around with these sizes maybe you want to see three by three pulling

02:25.940 --> 02:29.510
or maybe you want to do different strides for convolutional to the.

02:29.540 --> 02:30.650
Again totally up to you.

02:30.860 --> 02:33.980
But we'll go ahead just use the same values as last time.

02:33.980 --> 02:35.640
Should be fine for our purposes.

02:35.810 --> 02:38.570
And now it's time to create the letters.

02:38.690 --> 02:43.000
So we're essentially going to create the same type of layers as we did last time.

02:43.040 --> 02:48.500
Again a lot of these sizes are up to you just some things are going to be aware of the fact that three

02:48.500 --> 02:52.430
color channels and that the shapes are 32 by 32.

02:52.490 --> 02:54.870
So we'll start off creating convo 1.

02:54.930 --> 03:02.550
So our first convolution layer is convo underskirt one that's going to be a convolutional Lehre takes

03:02.560 --> 03:09.310
center data X and then our shape will go ahead and have it be four by four.

03:10.510 --> 03:12.730
And then we'll say three and three to

03:15.650 --> 03:21.880
and then we're going to do a pooling layer after this all say convoke one pooling.

03:22.600 --> 03:25.780
And that's just going to be equal to that Max pool function.

03:25.840 --> 03:34.150
And then we passen Canova 1 now it's up to you what size you want these actual convolutional filters

03:34.150 --> 03:39.560
to be here we're just using four by four but you can always play around with those values.

03:39.610 --> 03:43.280
So next we want to create in other convolutional there.

03:43.300 --> 03:50.710
So it's going to do that we'll say Canova 2 is equal to a convolutional layer and then we're going to

03:50.720 --> 04:01.590
input the result of the first pooling will say shape is equal to 4 4 and then all going to say 32 64

04:01.590 --> 04:13.300
here and then we're going past that into pulling so convoke to pooling is equal to max pool two by two

04:13.360 --> 04:17.950
and then pass and the result of convo to OK.

04:17.980 --> 04:21.400
So we have to convolutional errors leading into two pulling layers.

04:21.440 --> 04:26.780
So we have kind of one leading and one pulling that leads into the second convolutional there and then

04:26.780 --> 04:28.130
we have pulling after that.

04:28.460 --> 04:34.670
So next we're going to create a flattened layer by reshaping that pulling layer into minus 1 8 by 8

04:34.670 --> 04:41.000
time 64 or essentially negative 1 four thousand ninety six and we did a really similar step with the

04:41.820 --> 04:43.370
digits dataset.

04:43.430 --> 04:53.100
So we're going to see convo to flat and always say tenderfoot reshape going to reshape that last puling

04:53.490 --> 05:04.220
layer and we're going to reshape it to negative one by eight times eight times 64 so that you create

05:04.220 --> 05:12.460
a new fooler using the normal full air function using this fine layer all these 1024 neurons will say

05:12.470 --> 05:16.450
full layer 1 is equal to T.F..

05:16.460 --> 05:21.800
And again this is going to be a normal error that uses the rectified linear unit.

05:22.920 --> 05:31.900
As the activation functions will say normal full convo to flat 1024.

05:31.910 --> 05:33.770
Next we're going to create the dropout layer.

05:33.800 --> 05:37.060
So that's also pretty straightforward we'll say.

05:38.190 --> 05:43.560
Fool ONE drop out is equal to tensor flow.

05:43.560 --> 05:51.980
Neural Network dropout full air 1 and we'll say the probability for being held is equal to that placeholder

05:51.980 --> 05:56.730
recreated hold prob and hopefully this feels really familiar.

05:57.020 --> 06:03.030
This needs to be keep prob not just prob keep prob there we go.

06:03.230 --> 06:08.240
And like I was saying hopefully it feels really familiar because this is almost in line directly from

06:08.240 --> 06:10.540
the examples that we walk through.

06:10.970 --> 06:17.810
So finally we need to set the output so we'll say that our predicted y value is equal to a normal flare

06:18.410 --> 06:25.750
that has the full one dropout layer and then 10 year on is here because there's 10 possible errors.

06:25.760 --> 06:28.970
Now we're going to use a loss functionally as a cross entropy loss function.

06:30.200 --> 06:36.800
Well say cross entropy is equal to T.F. reduce mean.

06:37.000 --> 06:44.140
And then we'll say T.F. and thought and then we call soft Max cross entropy and will provide the labels

06:44.140 --> 06:46.950
to be quite true.

06:48.650 --> 06:51.490
And the logic is to be why spreads are predicted values.

06:51.490 --> 06:57.560
They're essentially comparing our predictions to the true values that we want an optimizer.

06:57.860 --> 06:59.710
So we'll say optimizer.

07:00.020 --> 07:06.250
And this is basically the same things we did last time is T.F. train we're going to use an atom optimizer

07:08.680 --> 07:14.590
and we'll set the learning rate to be zero point zero zero or 1 something you can kind of play around

07:14.590 --> 07:16.000
with there as well.

07:16.340 --> 07:20.100
And then we want to use that optimizer to minimize our loss.

07:20.260 --> 07:25.960
So say optimizer minimize cross entropy.

07:26.950 --> 07:32.500
Next create that in a variable that we always do to initialize everything so global variables initialiser

07:33.780 --> 07:34.190
OK.

07:34.230 --> 07:35.660
Now it's time for the graph session.

07:35.680 --> 07:38.940
So you can see here we already have some output so I'm going to clear that by running in that cell that's

07:39.000 --> 07:39.570
empty.

07:39.930 --> 07:47.120
And let's build that will say with T.F. session as SPSS.

07:48.010 --> 07:50.910
I'm going to say a session run will say T.F..

07:51.030 --> 07:52.590
Actually you just need to say in it's here

07:55.670 --> 07:58.430
and then for I in range.

07:58.790 --> 08:01.620
And this is up to you how many train steps you want to do.

08:01.950 --> 08:02.910
I did five thousand.

08:02.910 --> 08:04.360
So let's try that out.

08:05.590 --> 08:12.370
Our batch is going to be equal to C.H. the next batch.

08:12.400 --> 08:16.690
So we want to make sure that we provide a batch size in there.

08:16.690 --> 08:23.460
So if we scroll back up remember there's instructions here and how to actually use the above code to

08:23.500 --> 08:25.100
make sure that we actually ran it.

08:25.120 --> 08:29.120
So we're going to create an instance of that Safar helper and then set up the images.

08:29.470 --> 08:35.230
So if you run this lips Safar help result define wups for author on these cells.

08:35.230 --> 08:40.950
There we go and let's make sure 100 codes not define that or come up here and run this one too.

08:40.960 --> 08:43.890
So now that I ran these two there you go.

08:44.110 --> 08:46.900
So it set up the training images labels set up test images labels.

08:46.900 --> 08:47.840
It's ready to go.

08:47.950 --> 08:52.970
So should be ready to feed batches using this next batch command will come back down here.

08:54.330 --> 08:57.790
It will say we'll grab a at a time.

08:58.000 --> 09:02.150
And as I mentioned earlier you can change about if you want.

09:02.170 --> 09:10.420
There was a session run train and we need to provide a feed dictionary will say x is equal to match

09:10.600 --> 09:12.900
zero.

09:13.260 --> 09:24.240
Why true is then going to be that one and we'll provide a hold probability of 0.5 just like we did last

09:24.240 --> 09:24.660
time.

09:24.660 --> 09:29.850
So a 50 percent chance of being held next we're going to run some tests here.

09:29.870 --> 09:38.090
So I will say if I mod 100 as equal to zero meaning or on a hunter's step are basically a step that's

09:38.090 --> 09:39.620
the visible by a hundred.

09:39.770 --> 09:45.370
Say Prince the step and we'll just say that format.

09:45.380 --> 09:48.410
I say things with it and this dataset.

09:48.450 --> 09:50.640
Let's zoom in here to see it clearly.

09:52.050 --> 10:01.100
Then we're going to do the exact same thing as the last time we'll say matches T.F. equal T.F. RMX of

10:01.170 --> 10:10.980
widespread one and they'll say T.F. RMX of y true come a 1.

10:11.010 --> 10:14.390
So that returns that Boulia a list of where they matching.

10:14.430 --> 10:21.460
Set up an accuracy measurement which is going to be reduced mean and I want to cast that list of booleans

10:22.880 --> 10:33.390
So when the cast matches to be to float 32 and then I'm going to print out session run run that accuracy

10:34.180 --> 10:37.430
and then we want to provide a feed dictionary now of the test data sets.

10:39.320 --> 10:46.850
So we do that is we basically just call it off that Helper Object that we created so we can say S.H.

10:47.670 --> 10:53.270
grabbed the test images.

10:53.460 --> 10:58.230
Why true it's going to be S.H. groud the test labels.

10:58.230 --> 11:07.140
So these are attributes from the subject and they will say hold probability is just 1.0.

11:07.300 --> 11:11.900
So we want everything to be held while we're testing and old print new line

11:14.860 --> 11:15.290
OK.

11:15.350 --> 11:18.590
Let's go in and run this check for any typos I'm sure I made one or two.

11:19.040 --> 11:20.200
So run that.

11:20.720 --> 11:20.960
OK.

11:20.960 --> 11:22.840
Looks like we actually are doing pretty good.

11:23.090 --> 11:30.020
So immediately right off the bat we're jumping straight to 50s and I'm going to fast forward in time

11:30.110 --> 11:32.630
until this is done training for 5000 steps.

11:32.720 --> 11:36.740
But you can already see we're kind of approaching 70 percent accuracy which is kind of what we're shooting

11:36.740 --> 11:37.490
for.

11:37.490 --> 11:41.660
So let me jump forward in time until this is done training for 5000 steps.

11:44.180 --> 11:44.540
OK.

11:44.560 --> 11:49.780
So after around 5000 steps looks like we just barely hit that 70 percent accuracy mark that we were

11:49.780 --> 11:51.930
looking for.

11:52.150 --> 11:56.080
But mind this may take a long time to run on your computer the pending on how fast the computer you

11:56.080 --> 11:58.310
have especially if you're just using CPQ.

11:58.570 --> 12:04.040
Right now I'm running this with GPS so it only took me around one or two minutes to run this whole thing.

12:04.060 --> 12:09.280
It could take definitely much much longer depending on how powerful your computer is and if your computer

12:09.670 --> 12:12.100
is really low power you may even run out of memory.

12:12.100 --> 12:13.680
So kind of keep that in mind.

12:13.720 --> 12:19.240
You can always use other services like AWOS or paper space to try to run some of this online.

12:19.240 --> 12:19.950
OK.

12:20.020 --> 12:22.300
If you have any questions feel free to post the Q&amp;A forums.

12:22.300 --> 12:24.420
I hope you found this exercise enjoyable.

12:24.420 --> 12:27.150
And now we definitely encourage you to kind of play around of some of these values.

12:27.160 --> 12:32.140
Now you play around the shapes of the filters and just really make sure you understand the basic flow

12:32.140 --> 12:32.720
here.

12:32.740 --> 12:37.240
It should feel really analogous to what we did together with the data.

12:37.300 --> 12:41.860
Again really hard part for when you're dealing with your own data is basically creating this class that

12:42.100 --> 12:44.290
kind of handles the images and reshapes them.

12:44.290 --> 12:48.820
I would say is probably by far the hardest part because as far as the convolutions are concerned a lot

12:48.820 --> 12:54.910
of that there are just too lenience calls and just taking advantage of tensor Fullers built in functionality.

12:54.910 --> 12:57.970
All right thanks everyone and I'll see you at the next section of the course.

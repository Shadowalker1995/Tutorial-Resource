WEBVTT

00:05.230 --> 00:09.180
Hello everyone and welcome to the lecture on convolutional neural networks.

00:10.120 --> 00:16.000
We just solved the feminist Digic classification task with three very simple linear approach using soft

00:16.060 --> 00:20.890
Max regression and we got pretty good results around 90 percent accuracy.

00:20.890 --> 00:26.230
But to get closer to that state of the art results closer to almost 100 percent accuracy we need to

00:26.230 --> 00:30.820
explore a much better approach using convolutional neural networks.

00:30.840 --> 00:35.670
Now it's really cool but convolutional neural networks is just like the simple perception model.

00:35.760 --> 00:39.420
They have their origins directly linked in biological research.

00:39.420 --> 00:45.930
Recall that when we were designing our perception model we directly converted a biological neuron into

00:45.930 --> 00:48.060
our own artificial representation.

00:48.390 --> 00:52.950
And we're going to do a similar idea with convolutional neural networks and the biological research.

00:52.950 --> 00:58.740
We base this off of is from a pair of scientists David Heibel and Torsen Weisel who were studying the

00:58.740 --> 01:06.000
structure of the visual cortex in mammals and their research is actually so fundamental to the field

01:06.000 --> 01:12.620
of biological vision that actually won them a Nobel Prize in 1981.

01:12.820 --> 01:17.590
One of the key components that their research revealed was that neurons in the visual cortex had what

01:17.590 --> 01:21.440
they call a small local receptive field that is the same.

01:21.490 --> 01:29.170
These neurons are actually only looking at a local a smaller subsection of the entire image that the

01:29.170 --> 01:36.820
person is viewing and then these local subsections can then later on overlap and create a larger image

01:36.880 --> 01:38.740
and visual field.

01:38.740 --> 01:44.410
What's also important to note is that these certain neurons in the visual cortex are only activated

01:44.410 --> 01:46.240
when they detect certain things.

01:46.240 --> 01:51.340
They realize that neurons may be only active when they detect the horizontal line or a black circle

01:51.340 --> 01:52.170
etc..

01:52.420 --> 01:58.670
So we can see here we already have this idea of local portions of the entire image.

01:58.810 --> 02:04.780
This idea of these local subsets directly inspired the artificial neural network architecture that would

02:04.780 --> 02:11.770
become the convolutional neural network and it was famously implemented in the 1998 paper by John McCain

02:11.830 --> 02:17.590
who is actually now the director of AI research at Facebook and they created what they call the Linette

02:17.590 --> 02:23.890
5 architecture that was first used to classify that dataset.

02:23.890 --> 02:28.240
So when you're learning about convolutional networks maybe you're Googling around you often see a diagram

02:28.270 --> 02:29.820
that looks something like this.

02:29.920 --> 02:34.180
And when you're reading papers on different convolutional neural network architectures you see a lot

02:34.180 --> 02:35.640
of images that look like this.

02:35.650 --> 02:40.720
And I remember when I was first learning about convolutional neural networks this basically looks like

02:40.720 --> 02:42.040
a foreign language to me.

02:42.040 --> 02:45.010
It was pretty hard for me to understand what was actually going on.

02:45.010 --> 02:50.500
Each of these steps so worked to break down the various aspects of the convolutional neural network

02:50.500 --> 02:55.950
seen here and the two main aspects we haven't really seen before are these ideas of these convolutions

02:56.040 --> 02:58.040
and subsampling or pooling.

02:58.070 --> 03:02.760
So that entire feature extraction portion is we're really going to focus our attention to and learn

03:02.760 --> 03:05.530
about in this come illusional neural network lecture.

03:05.640 --> 03:10.080
That second half of classification is actually stuff we've already covered before basically fully connected

03:10.080 --> 03:13.570
networks and soft Max regression for multiple outputs.

03:13.620 --> 03:18.960
Well we're going to focus on is this idea of convolutions and subsampling or pooling.

03:18.990 --> 03:22.560
So in order to learn about that we need to break down into various topics.

03:22.560 --> 03:27.510
So first I have to review what are its sensors then we'll compare a densely connect a neural network

03:27.540 --> 03:33.000
to convolutional neural network then we'll dive deeper into convolutions and filters we'll learn about

03:33.000 --> 03:39.700
padding as well as pooling layers or subsampling and then we'll also review dropout.

03:39.750 --> 03:42.390
So let's get started by talking about tensors.

03:42.390 --> 03:49.450
Recall that tensors are just and dimensional arrays that we build up to as we increase levels of dimension.

03:49.500 --> 03:51.270
So we start off with just scalar numbers.

03:51.270 --> 03:55.010
Those are just individual digits or floating point numbers.

03:55.050 --> 04:00.570
Then we can have a vector of numbers essentially just a one dimensional array of various scalar numbers

04:00.880 --> 04:05.220
that we can actually have a matrix when they mentioned higher up for that which is then basically a

04:05.220 --> 04:06.530
list of vectors.

04:06.630 --> 04:10.950
And beyond that we have tensors Where are just higher than missional arrays.

04:12.270 --> 04:17.970
So tensors make it really convenient to feed in sets of images into our model which is why it's really

04:17.970 --> 04:22.740
vital that we understand tensors and their shapes when we end up building these models.

04:22.740 --> 04:30.490
Intenser flow so we can imagine that if we have a group of training images such as those 55000 training

04:30.490 --> 04:36.610
images and the data set that we can have for them missional tensor where one dimension are the images

04:36.610 --> 04:42.580
themselves along another dimension we have the height of the image in pixels so that pixel representation

04:42.880 --> 04:48.010
in the height along another dimension we can have the width of the images and then if we want we can

04:48.010 --> 04:52.900
have the four of them mentioned for the color channels where right now for em the digits were only dealing

04:52.900 --> 04:53.720
of grayscale.

04:53.800 --> 04:59.630
But later on for color images will have three channels red green and blue.

04:59.640 --> 05:05.280
So let's explore now the difference between a densely connected neural network those CNN's any convolutional

05:05.280 --> 05:06.060
neural network.

05:06.150 --> 05:11.020
And recall that we actually already been able to create densely connected neural networks easily with

05:11.040 --> 05:14.350
the T.F. estimator API.

05:14.570 --> 05:19.940
So remember densely connected layer basically just looks like this where every neuron in one layer is

05:19.940 --> 05:22.960
directly connected to every other neuron in the next layer.

05:24.070 --> 05:27.970
Then free convolutional air we have a different approach.

05:28.150 --> 05:35.170
Each unit is connected to a smaller number of nearby units in the next layer basically directly inspired

05:35.170 --> 05:43.280
by that idea of the biological visual cortex where you're only looking at local receptive fields.

05:43.310 --> 05:47.870
So now the question arises why bother if it convolutional neural network instead of a densely connected

05:47.870 --> 05:48.790
neural network.

05:50.240 --> 05:54.110
Well the this dataset was 28 by 28 pixels.

05:54.140 --> 05:56.490
So that's 784 total pixels.

05:56.570 --> 06:03.620
When you feed it in but most images are going to be much larger at least 256 by 256 pixels and that

06:03.620 --> 06:10.640
would be 56000 total pixels and that's just for a relatively small image of 256 by 256.

06:10.640 --> 06:15.980
So as your images get larger and larger if you were to have a densely connected neural network that

06:15.980 --> 06:21.410
would lead to way too many parameters and it would just be unscalable to new image images because you'd

06:21.410 --> 06:25.700
have so many parameters that you already fit.

06:25.720 --> 06:33.070
So instead of convolutions and convolutions also have a major advantage for image processing that are

06:33.070 --> 06:36.010
directly inspired by that biological research.

06:36.010 --> 06:41.800
We can know intuitively that if we're looking at an image pixels that are nearby each other they're

06:41.800 --> 06:47.080
much more correlated to each other for image detection which is directly related to this idea of those

06:47.080 --> 06:51.280
convolutional layers only being connected to those local neurons.

06:53.300 --> 06:59.300
So each convolutional neural network layer looks at an increasingly larger part of the image and having

06:59.300 --> 07:03.710
units only connected to nearby units also aids in invariance.

07:03.770 --> 07:08.360
So convolutional neural networks help with regularisation and they also limit the search of weights

07:08.600 --> 07:12.120
to the size of the convolution.

07:12.130 --> 07:16.840
Let's go ahead and explore just as a very high level overview what the convolutional neural network

07:16.840 --> 07:19.800
would look like as it relates to image recognition.

07:19.870 --> 07:25.580
You would basically just start with some input layer which is the image itself then come layers are

07:25.610 --> 07:28.450
only connected to pixels in their respective fields.

07:28.460 --> 07:33.710
So if we kind of think about this on a to the plane we can see that become illusional errors only have

07:33.710 --> 07:39.980
those local connections and then another higher level of convolution is again only connected to the

07:39.980 --> 07:42.260
local connections of that first convolutional layer.

07:43.820 --> 07:49.790
Now there is an issue here as you get towards the edge of the image there may not be any input neurons

07:50.120 --> 07:57.050
from the actual input data so we can fix this by adding a padding of zeros around the image.

07:57.060 --> 08:01.530
Then this is just generally known as padding OK.

08:01.580 --> 08:06.170
Let's now walk through a one dimensional convolution in more detail and then we're going to expand this

08:06.170 --> 08:08.570
to the idea of a two dimensional convolution.

08:08.810 --> 08:14.030
And this is going to begin to look like some of those images we first saw describing the architecture

08:14.150 --> 08:16.470
the convolutional neural network.

08:16.490 --> 08:21.380
So again let's take that back revisit then really connect in your own networks and convert that to a

08:21.380 --> 08:24.860
one dimensional convolution.

08:24.870 --> 08:27.930
So here we have an example of how densely connected neural network.

08:27.930 --> 08:29.060
Again same idea.

08:29.220 --> 08:35.010
We have one dimension of neurons densely connected to every neuron in the next layer that's the blue

08:35.010 --> 08:35.540
layer.

08:35.640 --> 08:38.630
And again densely connected to everything in the output layer.

08:39.590 --> 08:45.850
However let's cover that first input layer to a one deconvolution.

08:45.900 --> 08:52.140
So what we see here is just local connections to that next layer of neurons and what's really interesting

08:52.320 --> 08:59.410
is we can then treat these weights of these local edges as what we call a filter.

08:59.430 --> 09:07.890
So let's take an example of one filter we can see here on that bottom we have y is equal to w one or

09:07.890 --> 09:16.320
wait 1 times the input X plus W2 or wait two times the input x 2 and that will be the output Y which

09:16.320 --> 09:18.520
is then fed into that next neuron.

09:19.640 --> 09:25.660
So if w 1 and 2 are equal to 1 a negative 1 respectively then we have the equation.

09:25.700 --> 09:29.300
Why is equal to x 1 minus x 2.

09:29.300 --> 09:31.420
So I kind of chose these arbitrary weights here.

09:31.760 --> 09:34.310
And given that they've chosen one a negative one.

09:34.310 --> 09:40.970
We ask ourselves Well one is why at a maximum so one is that maximum activation there and that occurs

09:41.050 --> 09:44.850
at x 1 equal to 1 and x 2 equal to zero.

09:44.990 --> 09:51.440
And that is basically an example of a filter that is specifically designed for edge detection because

09:51.440 --> 09:56.930
remember edges when we represent them as pixels are essentially just a large difference in the actual

09:56.930 --> 10:00.840
darkness darkness of two pixels that are right next to each other.

10:00.860 --> 10:04.580
So if we have one pixel one right next to it we have a pixel zero.

10:04.670 --> 10:09.410
Then we're essentially describing an edge of really dark pixel versus the really light pixel and we

10:09.410 --> 10:15.340
can see here if we describe these weights as one common negative one then when why is that a maximum

10:15.350 --> 10:19.720
we have an edge so we have a filter here that's essentially now an edge detector.

10:20.370 --> 10:23.880
So then we can apply that to the rest of these edges.

10:25.170 --> 10:30.000
So we have a set of weights that can act as a filter for detection and we can then expand this idea

10:30.000 --> 10:32.680
to multiple filters.

10:32.710 --> 10:35.310
So here's an example of having just one filter.

10:35.320 --> 10:40.290
So that's the same way across where the filter size is too because I have two weights there.

10:40.480 --> 10:42.140
And the stride is two.

10:42.160 --> 10:44.150
So let's talk about this stride term.

10:44.380 --> 10:49.360
You can see here in this first layer of pink neurons moving up.

10:49.360 --> 10:51.940
These weights two neurons at a time.

10:51.940 --> 10:58.060
So if we start from the bottom I have w one and w 2 I repeat them every two neurons along the input

10:58.480 --> 11:02.120
and that's known as a stride of this filter.

11:02.260 --> 11:07.290
And if this visualization doesn't make a whole lot of sense to you don't worry we'll later on show you.

11:07.310 --> 11:13.340
In other visualization representations of filters that might make more sense.

11:13.390 --> 11:15.670
So really lock in on this idea of stride.

11:15.700 --> 11:18.650
Let's fix this so now we have a stride of one.

11:18.820 --> 11:22.470
So if we're going up one unit at a time then we have something that looks like this.

11:22.600 --> 11:24.200
We can see they'll be gone and they'll be too.

11:24.220 --> 11:26.520
But now we're only moving up one year on at a time.

11:26.680 --> 11:29.130
So we have blue than orange than red.

11:29.380 --> 11:33.750
So you can see here this is a stripe of two moving up to neurons at a time.

11:33.790 --> 11:38.790
This is a streite of one moving up one neuron out of time.

11:38.790 --> 11:42.170
Now remember we can see here that we have some neurons that aren't connected.

11:42.180 --> 11:47.640
That is where we could have added in some zero padding to include more edge pixels so I could add in

11:47.640 --> 11:51.730
a top neuron above my top peak neuron and just have that be zero.

11:51.780 --> 11:57.020
So I can kind of finish off that stride count.

11:57.050 --> 12:02.810
So that was just showing one filter we can actually then expand to multiple filters or multiple sets

12:02.810 --> 12:03.790
of weights.

12:03.800 --> 12:08.950
So here I can see this is a representation of two filters where my stride is still one.

12:08.960 --> 12:14.360
But now my filter size is two seconds here and added another set of neurons ready to accept another

12:14.360 --> 12:16.140
set of weights for another filter.

12:16.790 --> 12:20.210
And we can expand this idea to any number of filters we want.

12:20.210 --> 12:21.840
So here I have now four filters.

12:22.010 --> 12:29.690
So now four different sets of weights filter sizes now two and streite is still one so each filter is

12:29.710 --> 12:36.680
affecting a different feature and realistically the representation would kind of get a little messy

12:36.680 --> 12:43.060
as we begin to draw arrows to every feature as we are every filter as we come in through the input.

12:43.070 --> 12:48.140
So instead of trying to represent it like this you'll often see it represented as sets of blocks.

12:48.140 --> 12:52.760
So for simplicity simplicity we're going to begin to describe this and visualize all this as a set of

12:52.760 --> 12:55.530
blocks instead.

12:55.570 --> 13:01.390
So we're going to convert our input to just be one by L. this one dimensional convolution we just have

13:01.480 --> 13:09.310
one set by a length L of pixels and then we describe that set of filters as just this tensor which is

13:09.580 --> 13:13.750
the number of filters by the number of units in that actual layer.

13:15.290 --> 13:21.110
So we now have these concepts of these neurons converted to these block images for to the convolution

13:21.200 --> 13:23.080
we're mainly going to be dealing with images.

13:23.910 --> 13:25.560
So it's a really similar structure.

13:25.560 --> 13:31.500
We have our input image but now instead of being one D it's to D where we have a height and width to

13:31.500 --> 13:32.610
the image itself.

13:32.880 --> 13:39.000
And then for that next layer we have this tensor of the number of filters and the number of units for

13:39.000 --> 13:41.370
the width by the number of units for the height.

13:41.370 --> 13:48.590
So now we have this theory that missional tensor object and we can see that subsections are going to

13:48.590 --> 13:51.540
directly be related to this tensor.

13:51.590 --> 13:57.680
So if we're looking at a local section of the input image of a height and width then it actually directly

13:57.680 --> 14:01.640
belongs to a local section of that next tensor.

14:01.640 --> 14:09.320
So again height by with my number filters if we're working color images it's basically a really similar

14:09.320 --> 14:13.360
concept except we add in one dimension for the number of color channels.

14:13.370 --> 14:18.530
So for a grayscale images which have one color channel but for color images we typically have three

14:18.860 --> 14:20.030
red green and blue

14:22.890 --> 14:27.660
and it's the same idea here you have height width and then color and actually then corresponds to number

14:27.660 --> 14:29.150
filter's number of units.

14:29.150 --> 14:36.310
The number of units for height now convolution filters are commonly visualized with a grid system.

14:36.310 --> 14:40.620
So in case the last explanation of convolution filters wasn't clear enough.

14:40.660 --> 14:46.520
Let's go ahead and walk through how they work with grids on a to the input here on the left hand side.

14:46.540 --> 14:49.870
I have an input that's already been padded with zeros.

14:49.870 --> 14:56.140
So my original input was just that inner four by four grid where ones represent the darkest pixels and

14:56.140 --> 14:58.390
negative ones represent the lightest pixels.

14:58.390 --> 15:01.890
So if we pretend this is a handwritten digit it's basically a zero.

15:02.020 --> 15:08.060
And then I've padded it with an edge of zeroes as far as pixel values around that I take a three by

15:08.060 --> 15:09.150
three filter.

15:09.230 --> 15:14.660
So I have weights inside this three by three filter and I'm going to apply it to the image and then

15:14.660 --> 15:21.740
one I'm going to do is multiply the pixels in the image by the basically weight values in the filter.

15:21.770 --> 15:26.420
Then once I multiply by those filter weights I'll get some result and then I'll take the some of that

15:26.720 --> 15:30.280
and I'll end up with whatever value in this case it was 0.

15:30.290 --> 15:35.960
So this is a way people commonly visualize filter's with a grid system where you have the filter of

15:35.990 --> 15:37.250
weights on a grid.

15:37.320 --> 15:43.280
Then you multiply the input by the filter weights and then you take that result take the sum and then

15:43.280 --> 15:48.530
that's going to be your output value for your convoluted image.

15:49.820 --> 15:56.300
Now stride distance also takes into account how fast you're going to move across image with your filters.

15:56.300 --> 15:59.270
Here we see a stride distance of one as an example.

15:59.420 --> 16:04.380
So I'm basically taking the filter moving over by one pixel and then applying all those changes.

16:04.430 --> 16:06.150
Or I could do it right.

16:06.170 --> 16:10.250
So here I'm moving over by two pixels and then applying those changes.

16:10.640 --> 16:16.700
Let's go ahead and jump to a really cool visualization Web site that again shows this process in even

16:16.700 --> 16:17.530
more detail.

16:17.560 --> 16:18.720
I'm going to jump to it now.

16:18.950 --> 16:23.930
So here I am at Sentosa the I O which is a really awesome visualization Web site and they have this

16:23.930 --> 16:28.430
really interesting visualization for convolution kernels which is basically what we've been explaining

16:28.430 --> 16:33.650
here and on the left hand side you can see I have this little red cursor that's highlighting a pixel

16:33.650 --> 16:38.910
value and then it highlights the corresponding pixel in the actual representation of the image.

16:38.910 --> 16:43.350
He can kind of move around and play with this and then all we're gonna do is walk through applying A-340

16:43.390 --> 16:47.600
three sharpened kernel to the image of the face from above.

16:47.600 --> 16:50.750
So here we have our filter or a kernel with those weights.

16:50.750 --> 16:56.140
Here we have zero negative 1 5 etc. and then what we end up doing is the following.

16:56.150 --> 16:57.380
So here's our input image.

16:57.380 --> 17:00.770
You can see my little convolution filter moving across.

17:00.770 --> 17:08.480
So I have those pixel values I multiply them by the filter or kernel waits and then that ends up representing

17:08.660 --> 17:14.470
once I sum them the output of that actual location in the output image.

17:14.510 --> 17:16.980
So you can kind of move along and see how this is working.

17:17.000 --> 17:18.800
So we kind of hover above this.

17:18.810 --> 17:26.090
I then I have nine values that I'm going to multiply by my filter weights and then take that sum and

17:26.090 --> 17:29.570
then that's going to be in the output image at that location.

17:29.570 --> 17:31.550
So this is a sharpened filter.

17:31.550 --> 17:35.430
We can also see other convolution kernels such as a blur.

17:35.570 --> 17:38.840
So this is the result of this blur filter.

17:38.840 --> 17:40.270
You can see the weights there.

17:40.460 --> 17:44.570
And I encourage you to check out the resources and the only play around of this but hopefully this gives

17:44.570 --> 17:46.910
you a good idea of the actual process.

17:46.910 --> 17:51.110
So again we're just passing along this filter and then getting out the output so you can see here as

17:51.110 --> 17:56.690
I go along the edge as the stride distance of one there is my actual value of the output as I multiply

17:56.690 --> 17:59.560
by those filter weights and then take the sum.

17:59.570 --> 18:01.410
OK let's go back to the presentation.

18:01.490 --> 18:06.740
So we just covered those convolution filters and if you have multiple filters you could visually represent

18:06.740 --> 18:07.500
them like this.

18:07.520 --> 18:12.980
Here we can see we have multiple by convolution filters and this is the sort of representation that

18:12.980 --> 18:17.030
is actually being shown in that original convolutional neural network diagram.

18:17.030 --> 18:19.100
So here's that five by five convolution.

18:19.100 --> 18:23.160
So earlier we were using three by three but here they're using five by five.

18:23.630 --> 18:28.980
So that's exactly what we saw here and now we have an understanding of what convolution is and how a

18:28.980 --> 18:30.680
convolution filters work.

18:31.010 --> 18:34.380
But we still have to discuss subsampling or pooling.

18:34.400 --> 18:39.050
So in the very next lecture we're going to continue right where we left off here and talk about pooling

18:39.050 --> 18:41.210
and subsampling I'll see you at the next lecture.

WEBVTT

00:06.870 --> 00:08.050
Welcome back everyone.

00:08.070 --> 00:12.840
Here we are right where we left off last time we've defined all our layers and all that's left to do

00:12.930 --> 00:17.910
is create a lost function our optimizer initialize the variables and then run a session.

00:17.910 --> 00:25.830
So let's start for a last function we'll use cross entropy to be our last function and we essentially

00:25.830 --> 00:32.770
just use the built in functions that are already intenser for us so we say reduce mean to take an average

00:33.520 --> 00:41.180
and we'll say soft Max cross entropy and we're going to say that the labels is equal to y true which

00:41.190 --> 00:50.770
will be provided by our dictionary later on and then we'll say logic is to why spreads why.

00:50.810 --> 00:59.290
There we go OK it's all run that and then what we're going to do is set up our optimizer So our optimizer

00:59.530 --> 01:05.440
we're going to go ahead and use an atom optimizer say Adam optimizer and then we'll say the learning

01:05.440 --> 01:14.290
rates is zero point zero 0 1 and then our trainer is just going to beat for this optimizer to minimize

01:14.290 --> 01:20.410
our loss which in our case is the cross entropy loss.

01:20.590 --> 01:27.990
Then we're going to initialize our variables so say an equal to T.F. global variables initializer OK

01:28.000 --> 01:30.330
now it's time to define our session.

01:30.610 --> 01:33.910
So I'll say we'll run this for 5000 steps.

01:33.910 --> 01:39.190
Keep in mind I'm running on a really fast computer it's also a GPU accelerated on a very good high end

01:39.190 --> 01:42.270
graphics card so you may have to wait a while.

01:42.280 --> 01:45.090
Much longer than I am to get results.

01:45.100 --> 01:47.480
This could take up to several hours so keep that in mind.

01:47.590 --> 01:49.480
Depending on the speed of your computer.

01:49.970 --> 01:59.890
So say session run in it and then we'll say for I in range steps and again reduce the number of steps.

01:59.970 --> 02:05.460
If you have a slower computer you won't end up getting as accurate results but they'll still run and

02:05.460 --> 02:09.690
will say batch X that y.

02:09.750 --> 02:16.140
Same deal as last time we're basically just grabbing from the next batch 50 so we'll do 50 images at

02:16.140 --> 02:23.970
a time and this batch and then we're going to say session run run the training the optimizer and then

02:23.970 --> 02:25.640
we need feed dictionary.

02:25.970 --> 02:28.410
So remember that we actually have three placeholders.

02:28.620 --> 02:31.360
We have the two classic ones which is just x and y.

02:31.400 --> 02:37.230
So we'll say X is batch x and y true is that y.

02:37.530 --> 02:43.100
And then we have the hold probability because remember we are designing that dropout layer appear.

02:43.260 --> 02:48.630
We have this whole probability which is a placeholder and that basically is the probability that a neuron

02:48.630 --> 02:50.370
is held during dropout.

02:50.610 --> 02:53.870
So during training we'll go ahead and say 50 percent.

02:53.940 --> 02:59.980
So each near on has a 50 percent chance of being held essentially randomly dropping out half our neurons

03:01.330 --> 03:06.600
they'll say if I bought one hundred equals equals zero.

03:06.970 --> 03:13.630
Meaning if you're on step divisible by 100 every 100 steps we're going to do something every 100 steps

03:13.720 --> 03:22.560
we're going to report back our accuracy on the Tesa will say print will say on Stapp and then I can

03:22.570 --> 03:24.910
use that format here.

03:24.910 --> 03:31.700
So in a safe format I that I'm going to print accuracy.

03:34.830 --> 03:38.160
And let's go ahead and calculate our accuracy.

03:38.160 --> 03:45.870
We'll do the calculation essentially just like with last time will say T.F. equal T.F. RMX y pred come

03:45.870 --> 03:46.820
a 1.

03:46.860 --> 03:56.420
So this is the exact same calculation we did previously and will say T.F. RMX y true comma 1.

03:56.800 --> 04:02.220
OK so we have our matches remember that's a list of booleans So we want to cast it into a list of floats

04:02.280 --> 04:04.020
and then take the average.

04:04.080 --> 04:13.770
So accuracy isn't equal to T.F. reduce mean and I'm going to cast that list of booleans that is the

04:13.820 --> 04:28.990
matches into 32 perfect then it we'll say print the result of session run and I'm going to run that

04:29.020 --> 04:34.590
accuracy measurement in my feed dictionary is then going to be the test results or the test set.

04:34.590 --> 04:38.850
I should say so this test images.

04:39.090 --> 04:41.310
So those are actual images that I'm feeding in.

04:41.380 --> 04:44.710
And then why true is going to be honest.

04:45.040 --> 04:46.190
Test labels.

04:46.330 --> 04:51.340
Let me zoom out one level here so we can see the whole thing labels and then remember I still have one

04:51.340 --> 04:54.340
placeholder for hold probability and there is the testing.

04:54.340 --> 05:00.370
I don't want to actually drop neurons so I will say hold probability is 1.0.

05:00.470 --> 05:04.730
Basically no neuron is going to be dropped because they'll have a hundred percent chance of being held

05:06.130 --> 05:11.260
and there we're going to print a new line OK.

05:11.300 --> 05:12.710
Let's make sure we don't have any typos.

05:12.710 --> 05:14.410
Let's try to run this.

05:14.630 --> 05:15.270
And there we go.

05:15.290 --> 05:19.780
So you can see it's already started as you go and read out of the gate even just that 100 steps or you

05:19.790 --> 05:22.760
have 94 percent accuracy which is really good.

05:22.760 --> 05:26.600
And you can see here it's continuing and a 6 percent 97 is 7.

05:26.660 --> 05:30.270
So this isn't exactly state of the art but it's still pretty darned good.

05:30.770 --> 05:36.500
You can see here already approaching 98 after almost just two thousand steps.

05:36.500 --> 05:40.460
So hopefully if we keep running this long enough and we can play around with the size of the convolutions

05:40.850 --> 05:42.750
we'll get closer to 99 percent.

05:42.850 --> 05:44.590
So 98 98 etc..

05:44.600 --> 05:50.480
So you can see here a huge improvement by using convolutional Lares and pooling layers.

05:50.480 --> 05:51.050
All right.

05:51.110 --> 05:53.300
So I hope you enjoy that exercise eventually at the end.

05:53.300 --> 05:56.570
We got pretty close to 99 percent accuracy.

05:56.660 --> 05:59.240
If you have any questions feel free to post the Q Any forums.

05:59.300 --> 06:03.810
Coming up next is going to be your exercise for convolutional neural networks.

06:03.830 --> 06:04.730
I'll see you at the next lecture.

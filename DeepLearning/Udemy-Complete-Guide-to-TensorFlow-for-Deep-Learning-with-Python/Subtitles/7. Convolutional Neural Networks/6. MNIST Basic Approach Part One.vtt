WEBVTT

00:05.320 --> 00:11.710
Welcome everyone to this lecture where we take a basic or normal approach to the dataset for classifying

00:11.740 --> 00:17.290
digits and this sort of approach is a lot of times the very first approach you see in deep learning

00:17.290 --> 00:18.160
tutorials.

00:18.160 --> 00:22.810
So it's always a good idea to go through it because pretty much every major framework will have some

00:22.810 --> 00:28.750
sort of data set analysis or classification as far as a tutorial on how to use a framework.

00:28.780 --> 00:31.950
So it's really important to understand this particular problem in a framework.

00:31.960 --> 00:35.520
That way you can easily use it as an analogy for other frameworks.

00:35.650 --> 00:41.530
So we're going to use what we know so far about neural networks theory and using tensor flow to solve

00:41.530 --> 00:43.150
the endless classification.

00:43.300 --> 00:47.530
Then after that we get to learn about convolutional neuron that works and see just how much of a performance

00:47.530 --> 00:53.740
improvement CNN's can give you for image classification tasks something CNN's and general are really

00:53.740 --> 00:55.490
well suited for.

00:55.640 --> 00:58.360
The First off we're going to use this more basic approach.

00:58.490 --> 01:04.010
Specifically a soft Max regression approach which is a really common layer and approach you're going

01:04.010 --> 01:06.980
to see throughout more advanced neural networks as well.

01:06.980 --> 01:08.340
So it's important that we go over it.

01:08.390 --> 01:13.500
Now that's actually really similar to the methods we've already used in previous sections of the course.

01:14.570 --> 01:20.720
So let's discuss what the soft Max regression is basically a soft Max regression returns a list of values

01:20.720 --> 01:26.060
between 0 and 1 that when you some of them up or add them up they all add up to just 1.

01:26.210 --> 01:28.660
And we can treat that as a list of probabilities.

01:28.730 --> 01:35.180
So you can imagine that if we have a list of 10 potential labels that is zero through nine we could

01:35.180 --> 01:40.030
then when we apply Sotha Max regression get a list of 10 probabilities.

01:40.130 --> 01:42.760
And when you add up those probabilities that equal 1.

01:42.770 --> 01:48.560
So that means that basically will choose whichever label has the highest probability or likelihood of

01:48.560 --> 01:49.730
being correct.

01:49.730 --> 01:55.250
Now you usually won't get 100 percent for one probability instead you'll get the relatively high probability

01:55.250 --> 02:00.260
for one label then maybe second highest probability for a similar looking number and then everything

02:00.260 --> 02:04.970
else will have not zero but something very close to it just because it's never 100 percent sure for

02:04.970 --> 02:11.640
any one number and we're going to apply the basic soft max function as an activation function.

02:11.640 --> 02:13.150
So let's go through that as well.

02:14.390 --> 02:17.280
So if we use soft Max as our activation function.

02:17.330 --> 02:23.210
The basic idea of soft Max is to define a new type of output layer for our neural networks and we'll

02:23.210 --> 02:25.120
see a diagram of this coming up next.

02:25.190 --> 02:29.810
But it essentially begins the exact same way as if the sigmoid layer something we've already worked

02:29.810 --> 02:30.470
with.

02:30.530 --> 02:38.110
So we form the output Z by having the weights multiplied by an input X plus a biased term.

02:38.300 --> 02:44.590
However once we have that Z the activation function we pass it through is not the sigmoid function but

02:44.600 --> 02:50.870
it's that it's this soft max function and according to this function it's just the exponential of that

02:51.080 --> 02:59.650
output Z divided by the denominator the sum over all the output neurons and that's the soft Max.

02:59.710 --> 03:04.480
And if we were to imagine this in a network format we basically get something that looks similar to

03:04.480 --> 03:06.880
this except they would have a lot more x's.

03:07.060 --> 03:13.450
And basically for each output we compute a weighted sum of X's add a bias and then apply the soft Max

03:13.620 --> 03:17.470
and we can actually then write this out as an equation.

03:17.470 --> 03:23.530
So you would get something like this where you have that output y equal to soft Max applied to the weights

03:23.800 --> 03:27.060
multiplied by each of the inputs plus a biased term.

03:27.130 --> 03:31.600
And then of course we're going to vectorize this procedure which would then look something like this

03:31.720 --> 03:34.840
turning it into matrix multiplication and Vector addition.

03:34.900 --> 03:38.210
And this makes it computationally a lot more efficient.

03:40.000 --> 03:45.130
All right let's go ahead and open up a Jupiter note book and implement this with Python and Tancer flow

03:46.210 --> 03:46.530
OK.

03:46.530 --> 03:48.300
Here I am at the Jupiter notebook.

03:48.330 --> 03:50.100
Let's go ahead and get started.

03:50.100 --> 03:54.360
The first thing you need to do is importance and flow of course and then the next thing we need to do

03:54.360 --> 03:55.760
is actually get the data.

03:56.010 --> 04:00.660
Keep in mind I've already downloaded the data and it's included in the zip file for this course underneath

04:00.660 --> 04:02.940
the convolutional neural networks folder.

04:02.940 --> 04:06.730
There is a folder called data in case you need to download it again.

04:06.750 --> 04:11.910
You can either check out the links in the basic approach notebook to download it manually or you can

04:11.910 --> 04:15.100
use the sensor for commands to download it automatically for you.

04:15.600 --> 04:20.270
And keep in mind if this doesn't work for you you most likely have some sort of firewall problem and

04:20.280 --> 04:25.530
in which case you can always use the links in the basic approach notebook that download this stuff manually

04:26.310 --> 04:30.470
some way or another you will need to have permissions on your computer to download stuff from the Internet.

04:30.480 --> 04:33.950
So if you're on a work computer that may be an issue for you.

04:34.080 --> 04:40.140
So I have it from sensor fluid that examples that's Tauriel and this import input data input data is

04:40.140 --> 04:43.560
going to allow us to download the data set.

04:43.590 --> 04:45.940
So I will say input data.

04:45.950 --> 04:50.530
The read data sets it could just have auto complete that.

04:50.590 --> 04:54.690
And then I will say this underscore data forward slash.

04:54.730 --> 05:01.450
That's going to create a new folder for me with the data and then you also want to say one hot is equal

05:01.450 --> 05:05.640
to true so that the labels are one and coded already for you.

05:05.710 --> 05:11.470
So if you run this inside the neural networks for there which are highly recommended Do you should basically

05:11.500 --> 05:14.480
automatically get something that says extracting the data.

05:14.590 --> 05:21.220
If you do not run this in that folder you'll get some some output that says hey it's downloading a couple

05:21.220 --> 05:22.900
of bytes downloading downloading.

05:22.900 --> 05:24.520
So that depends on your internet connection.

05:24.520 --> 05:28.540
But again if you're running this from another folder you will be downloading the data instead of just

05:28.540 --> 05:30.970
extracting the data that I've already downloaded for you.

05:30.970 --> 05:31.880
Keep that in mind.

05:31.960 --> 05:37.230
I really recommend just run this notebook directly from where it is in the zip file.

05:37.300 --> 05:43.880
So if we take a look at the type of what this is it's basically this kind of specialized tensor flow

05:43.930 --> 05:50.050
data set than it already has a lot of convenient methods built into it later on when we deal with other

05:50.050 --> 05:54.310
data sets we're going to have to basically make our own classes to create these kind of methods.

05:54.310 --> 05:57.620
But for now we're going to take advantage of these convenience methods.

05:58.830 --> 06:03.120
So I can say this and if you do that tap here there's a test train and validation.

06:03.360 --> 06:08.910
So if you choose one of those sets such as train and then hit tab here you can see there's images labels.

06:08.910 --> 06:14.000
There's also a next batch function which is nice basically feeds in batches number of examples epochs

06:14.100 --> 06:15.410
completed etcetera.

06:15.600 --> 06:19.710
So if we take a look at the images themselves you get the output of a bunch of arrays.

06:19.980 --> 06:26.400
So then you can actually ask for how many examples there are as we discussed there's 55000 training

06:26.400 --> 06:29.500
examples and you can check out the other ones as well.

06:29.700 --> 06:34.410
If you want to know how many test examples are those 10000 and there's 5000 validation examples as we've

06:34.410 --> 06:35.490
already discussed.

06:35.490 --> 06:37.400
So let's quickly visualize the data.

06:38.660 --> 06:48.150
Also import map plot lib pipe plot as PBT and Alson is say some time in the notebook that lib inline

06:49.810 --> 06:58.160
Sugoi say this train images and if I check out the shape of this this is 55000 images by 784 pixels

06:58.170 --> 07:00.150
remember that's just 28 times 28.

07:00.260 --> 07:01.840
So I'm going to grab one of these

07:04.680 --> 07:08.130
and this is an image so we can see here it's an image.

07:08.130 --> 07:09.000
So what did you do.

07:09.030 --> 07:18.600
Since it's flattened out I need to reshape this to be 28 by 28 and now I can see that it's been reshaped

07:18.600 --> 07:21.930
and others brackets in here which means I can actually show this.

07:21.930 --> 07:32.370
So I will say this is a single image is equal to that result that I'll say P.L. see him show that single

07:32.370 --> 07:34.130
image run that.

07:34.230 --> 07:36.420
And there we can see it's a handwritten three.

07:36.450 --> 07:37.250
Pretty cool.

07:37.500 --> 07:42.180
Now we're actually not going to be using color instead we're going to be using just a greyscale.

07:42.180 --> 07:46.310
So to kind of show what that would look like you just say one of the color maps.

07:46.320 --> 07:53.840
See map JSE underscore Gray and this is kind of what we're going to be training on.

07:53.850 --> 07:54.540
All right.

07:54.540 --> 07:58.300
So the other thing to note is that the data has already been normalized for you.

07:58.530 --> 08:05.700
So if you take a look at single image and ask for the minimum value it's zero.

08:05.710 --> 08:08.230
And if you ask for the maximum value it's 1.

08:08.230 --> 08:11.440
So this is already normalized for us which is pretty nice.

08:11.470 --> 08:16.540
So now that we understand our data again it's just an array of images that are all flattened out and

08:16.540 --> 08:18.520
you could reshape that to visualize it.

08:18.520 --> 08:20.990
However we won't be reshaping for training purposes.

08:21.100 --> 08:24.080
We're going to do in the very next lecture is actually create the model.

08:24.250 --> 08:25.560
So I will see at the next lecture.

08:25.570 --> 08:28.350
We're going to take off right where we left off here.

08:28.360 --> 08:28.990
I'll see you there.

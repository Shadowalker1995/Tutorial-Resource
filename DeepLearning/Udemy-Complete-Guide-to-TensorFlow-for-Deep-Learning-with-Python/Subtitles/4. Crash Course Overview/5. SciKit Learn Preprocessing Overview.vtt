WEBVTT

00:05.290 --> 00:06.030
Welcome everyone.

00:06.040 --> 00:10.930
This quick overview on psychist learn and we really won't be talking about sikat learn machine learning

00:10.930 --> 00:13.830
models because we're going to be using tent's flow instead.

00:14.140 --> 00:19.270
Well we're going to be talking about in this lecture is using sikat learn for pre-processing and specifically

00:19.270 --> 00:25.050
two things and that is scaling our data and also splitting our data into train test sets.

00:25.060 --> 00:29.260
Let's go ahead and jump to DUPERE notebook and see the convenience functions that sikat learned provides

00:29.260 --> 00:30.250
for us.

00:30.250 --> 00:30.640
OK.

00:30.680 --> 00:32.050
Go ahead and generate some data.

00:32.050 --> 00:39.460
I will import umpire as an p and then we're mainly going to be using preprocessing of sikat learn so

00:39.490 --> 00:43.020
say from S-K learn pre-processing import.

00:43.090 --> 00:46.350
And we'll be importing the minimax scaler.

00:46.810 --> 00:48.950
Let's go ahead and create some data we'll just say.

00:48.950 --> 00:51.990
And we ran them ran in.

00:52.030 --> 00:53.920
And we'll say random data.

00:53.950 --> 00:55.130
Zero to 100.

00:55.450 --> 00:58.430
And we'll have it be 10 rows by two columns.

00:58.450 --> 01:04.180
So if I take a look at my data I see here that I have the 10 rows two columns kind of think of this

01:04.480 --> 01:07.880
as either features in a dataset or labels in a dataset etc..

01:08.970 --> 01:15.150
So if I actually want to run this data into a neural network usually what I should be doing is scaling

01:15.150 --> 01:16.960
that data in order to do that.

01:16.980 --> 01:20.110
I can use the minimax scalar.

01:20.200 --> 01:24.380
So the way we do this is we create an instance of that minimax scalar.

01:24.460 --> 01:26.500
So I'll say scaler underscore model.

01:26.540 --> 01:30.840
Really whatever you want to call it minimax scaler close print sees.

01:30.880 --> 01:34.440
And now I check out the type of that scalar model.

01:34.510 --> 01:37.530
It is a instance of a min max scalar.

01:37.540 --> 01:41.280
And then what we do is offer this scalar model.

01:41.590 --> 01:46.690
We're going to fit it to our data and fitting it to her data.

01:46.900 --> 01:53.310
Basically allows the model to learn what the minimum value and what the maximum value is for each column.

01:53.380 --> 01:55.610
This warning right here you don't need to worry about it too much.

01:55.630 --> 02:00.580
It's just reporting back that it's converting the integers into floating point numbers which makes sense

02:00.580 --> 02:04.840
because we expect these wants her scale to be floating point numbers.

02:04.960 --> 02:11.400
Then once we do that we can go ahead and grab her model and then transform the data and then once you

02:11.400 --> 02:16.890
transform data you'll notice what happens is that the minimum value becomes zero and the maximum value

02:16.890 --> 02:19.750
becomes one and that goes across each column.

02:19.920 --> 02:24.600
So we kind of scroll up here we can see here that in my case with this random data I had a max value

02:24.600 --> 02:28.250
for the first column at 92 and the minimum value was three.

02:28.260 --> 02:34.070
So if we divide each value in this column by the max 92 we end up getting these results.

02:34.080 --> 02:36.520
And that that is what the min max scaler does.

02:36.540 --> 02:38.480
There's other ways of normalizing your data.

02:38.490 --> 02:43.560
But for most of our cases since we're dealing with pretty basic data sets the min max scaler will be

02:43.560 --> 02:44.320
enough.

02:44.470 --> 02:49.770
Now technically I could have done this in one command instead of saying fit the data and then transform

02:49.770 --> 02:55.110
the data the scalar model also has a fit transform.

02:55.470 --> 03:02.280
And if you run that on the data it performs both those steps at once depending on what situation you're

03:02.280 --> 03:07.950
in where you're usually going to do is you're going to fit to your training data and then transform

03:07.950 --> 03:10.660
your training data and then transform your test data.

03:10.680 --> 03:15.420
And the reason for that is because you don't really want to cheat by fitting to your test data as well

03:15.420 --> 03:20.280
as your trained data because you don't want to assume that you're going to know what your test data

03:20.280 --> 03:21.180
is going to look like.

03:21.180 --> 03:26.010
So typically you fit to your training data and then you transform to your test data and your training

03:26.010 --> 03:26.580
data.

03:26.670 --> 03:31.640
But the model itself this scaling model has only been fitted to your training data.

03:31.650 --> 03:38.610
All right let's go ahead and kind of show an example of a train test split so import Pandurs for this

03:39.540 --> 03:46.550
and would create a data frame so we can imagine we just read in some see as we file so we can say PD

03:46.630 --> 03:55.130
that data frames create a data frame and I'm going to say the data is equal to p that ran them ran Diante

03:55.300 --> 03:56.600
I'll say zero.

03:57.590 --> 03:58.240
101.

03:58.280 --> 04:00.450
And then 50 by four.

04:00.500 --> 04:08.260
In effect let's actually just grab that entire thing and put that in and you sell and will have that

04:08.260 --> 04:09.620
be my data.

04:10.000 --> 04:17.330
So if we take a look at my data it's just these integers here it's four columns by 50 rows.

04:17.530 --> 04:20.690
So scrolling down here that's going to be the data pasan.

04:21.040 --> 04:25.040
And you could put the whole command in one line but this makes it a little cleaner here.

04:25.150 --> 04:32.600
And then let's go ahead and if I just pass this in and I take a look at the data and if you want to

04:32.630 --> 04:34.670
call this DFA that it's a little clear.

04:34.940 --> 04:37.530
So I'll say DFAC could repeat the data from there.

04:37.540 --> 04:39.070
And if I take a look at the f.

04:39.200 --> 04:43.550
So now that I'm viewing my data frame I can see here that pair of those has auto named these columns

04:43.550 --> 04:46.000
to be 0 1 2 3.

04:46.010 --> 04:51.240
What it can do to make sure is columns have names is I can provide a list of names.

04:51.290 --> 04:53.340
So let's imagine this as a data set.

04:53.600 --> 04:59.350
So all have three features F1 F2 F3 and then some sort of label.

04:59.390 --> 05:01.790
So I can already tell this is a supervised learning problem.

05:01.790 --> 05:05.160
I have three features and I'm trying to predict this label.

05:05.180 --> 05:09.410
So now if I take a look at my data frame now the columns are named so three features and then the label

05:09.410 --> 05:10.790
column.

05:10.790 --> 05:12.900
Now let's imagine the Zura entire dataset.

05:12.980 --> 05:14.930
And I want to split it into a training set.

05:14.990 --> 05:19.060
And the testing set the way can do that is through the following method.

05:19.060 --> 05:26.050
If I scroll down here I can say my X that is my features is equal to data.

05:26.210 --> 05:33.100
And then I pass on a list of the columns only the feature columns that is F1 F2 F3.

05:33.170 --> 05:34.900
So what's the mixture.

05:34.910 --> 05:38.120
I personally fear there we go.

05:38.120 --> 05:43.550
So now if I take a look at X it's only the feature columns and I'm going to do a similar process for

05:43.550 --> 05:48.580
y for the labels will just say Y is equal to the F label.

05:49.410 --> 05:54.260
Run that then take a look at why it's just that label column.

05:54.260 --> 05:59.820
OK so now that I have my features my actual data and then the label that I'm trying to predict I can

05:59.820 --> 06:10.620
do a train test split seal say from S-K learn the model selection import train test split run that and

06:10.620 --> 06:16.440
then what I like to do is call train to split through shift tab here and then you can expand this to

06:16.440 --> 06:19.370
see the actual documentation.

06:19.380 --> 06:23.250
And if you scroll all the way down there's a nice little example you can kind of copy and paste from.

06:23.400 --> 06:29.400
And it's this one right here X train accessed y train and y to us copy that whole commands and then

06:29.400 --> 06:33.340
we can actually overwrite it here and put this all on one line.

06:33.450 --> 06:36.020
When we zoom out a little bit so we can see the whole thing.

06:36.060 --> 06:38.160
So it is trained to split actually do.

06:38.220 --> 06:42.300
Well you pass in your feature columns and then you press in your label.

06:42.450 --> 06:45.250
And then it's going to ask you what the test size is going to be.

06:45.420 --> 06:49.410
So it's asking you what percentage of the data you want to go to the test set.

06:49.440 --> 06:55.080
A really common value here is 30 percent or 33 percent should be your test 70 percent should be your

06:55.080 --> 06:55.890
training.

06:56.310 --> 07:00.980
And again this is a very situation specific number so there is no right or wrong answer here.

07:00.990 --> 07:03.130
Sometimes a 50/50 split makes sense.

07:03.300 --> 07:06.450
And then they ran them state is basically for repeatability.

07:06.450 --> 07:08.940
Just like we had some PI random set seed.

07:09.060 --> 07:10.820
We can do a seed here as well.

07:10.830 --> 07:16.700
That way you can always make sure that you get the same random splits so you can say you want to 1:42

07:16.780 --> 07:17.150
cetera.

07:17.190 --> 07:18.090
It's up to you.

07:18.240 --> 07:23.100
And then you get back these four variable results you get back in X train in excess a white train and

07:23.100 --> 07:24.560
a white test.

07:24.570 --> 07:32.460
So if I run this and I check out the shapes of these guys leps X train check out the shape.

07:32.460 --> 07:35.520
Notice I have 35 rows by three.

07:35.520 --> 07:37.990
So that means this is the actual x train.

07:38.070 --> 07:41.270
So that is the feature data for the training set.

07:41.400 --> 07:44.230
And if I take a look at X test.

07:44.280 --> 07:46.440
Well let's go ahead and take a look at the shape here.

07:48.320 --> 07:50.040
Then I have 15 by three.

07:50.060 --> 07:53.890
Notice that it's smaller because this is the feature for the test set.

07:53.900 --> 07:59.450
So the basic idea here would be once I actually have my neural network model working intenser flow and

07:59.450 --> 08:05.870
I want to do some sort of training process for supervised learning I would feed in the training sets

08:05.870 --> 08:12.980
for X train and y train and the model would try to basically build some sort of understanding of how

08:12.980 --> 08:20.540
the X training features are able to predict the why training labels once they have that then I can evaluate

08:20.540 --> 08:26.330
my model by feeding it the X test data and then it will try to predict what those labels should be in

08:26.330 --> 08:27.980
order to do the full evaluation.

08:27.980 --> 08:33.920
I can then compare those predictive values to the true test values and that's the reason for a train

08:33.920 --> 08:34.700
to split.

08:34.940 --> 08:36.860
And we're going to see this later on the course.

08:37.140 --> 08:37.580
OK.

08:37.760 --> 08:39.790
That's really all we need to know about sikat learn.

08:39.920 --> 08:45.080
And as a quick note sikat learn is actually its own machine learning library for Python it's one of

08:45.080 --> 08:50.570
most popular out there but it doesn't support the deep neural networks that tensor flow can do so which

08:50.570 --> 08:55.370
is why we're not really going to be using it in this course if you're interested in some of those other

08:55.400 --> 08:57.420
sikat learn machine learning model methods.

08:57.470 --> 09:01.340
You can check out that Python for data science and machine learning boot camp course.

09:01.340 --> 09:01.770
OK.

09:01.910 --> 09:03.890
Thanks everyone and I will see you at the next lecture.

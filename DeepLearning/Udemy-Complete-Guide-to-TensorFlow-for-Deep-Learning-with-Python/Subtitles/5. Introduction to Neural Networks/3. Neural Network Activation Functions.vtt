WEBVTT

00:05.690 --> 00:08.780
Welcome back everyone to an introduction to neural networks.

00:10.290 --> 00:12.720
We've seen how a single percept Trump behaves.

00:12.720 --> 00:16.440
Now let's expand on this concept to get the idea of a neural network.

00:16.620 --> 00:22.890
Let's see how to connect many perceptive thoughts together and how to represent this mathematically.

00:22.900 --> 00:25.810
So what does a multiple perceptions that work actually look like.

00:25.810 --> 00:27.550
Well essentially looks like this.

00:27.610 --> 00:32.650
Here we can see that we have various layers of single percept trons connected to each other through

00:32.650 --> 00:35.040
their inputs and outputs.

00:35.060 --> 00:38.740
In this case we have an input layer all the way on the left which is purple.

00:38.780 --> 00:44.180
We have two hidden layers and hidden layers are the layers that are in-between the input layer and all

00:44.180 --> 00:49.370
the way on the right that output layer essentially hidden layers are layers that don't get to see the

00:49.370 --> 00:49.940
outside.

00:49.940 --> 00:56.130
That is all the way the inputs on the left or all the way the outputs on the right so input layers those

00:56.140 --> 01:00.710
are real values from the data so they take in the actual data as their input.

01:00.790 --> 01:04.850
Then you have hidden layers and those layers between the input output layers.

01:04.990 --> 01:10.930
And if you have three or more hidden layers that's considered a deep network and then an output layer

01:11.050 --> 01:17.590
where you have some sort of final estimate of whatever the output that you're trying to estimate is.

01:17.810 --> 01:22.620
So as you go forwards through more layers the level of abstraction increases.

01:22.760 --> 01:26.060
Let's not discuss the activation function in a little more detail.

01:28.070 --> 01:33.560
Previously our activation function was just a really simple function the output 0 or 1.

01:33.590 --> 01:37.940
Remember we were just taking the some of the inputs in that simple percept Tron model and then had an

01:38.000 --> 01:44.390
activation function that just had an output of either or 1 based off whether that number was positive

01:44.420 --> 01:45.180
or negative.

01:45.340 --> 01:51.680
And if we plotted out that looks like this we can say the output is either 0 or 1 on the y axis and

01:51.680 --> 01:54.410
then the x axis we see our marking at zero.

01:54.620 --> 02:00.980
And if we have a negative value we output 0 and if we have a positive value we output 1 and we did this

02:00.980 --> 02:04.390
by having the x axis B W X plus B.

02:04.550 --> 02:10.390
That is the wait times the inputs plus that bias and it's really common to just set that equal to Z.

02:10.490 --> 02:15.150
So you can refer to Z instead of having to constantly for two W X plus B.

02:15.170 --> 02:23.200
So whenever I say Z I'm essentially referring to those wait times those inputs plus that buys So unfortunately

02:23.200 --> 02:27.330
this is actually a pretty dramatic function since small changes aren't really reflected.

02:27.520 --> 02:35.390
So you can have kind of really small changes in Z for instance the change from 0.5 and Z to 0.6.

02:35.440 --> 02:39.310
Doesn't really matter it's still going to output one as long as it's positive or the changes that are

02:39.310 --> 02:43.130
really dramatic from negative one to negative thousand doesn't really matter.

02:43.150 --> 02:45.320
It's going to be zero.

02:45.640 --> 02:48.320
So it'd be nice if we could have a more dynamic function.

02:48.340 --> 02:54.630
For example this red line here you can see here that our outputs now are no longer just 0 or 1.

02:54.640 --> 02:56.550
We have a little more detail to it.

02:56.560 --> 02:58.890
There's a little more nuance here.

02:58.960 --> 03:01.300
So lucky for us this is actually the sigmoid function.

03:01.300 --> 03:05.340
This red line function already exists and it's known as the sigmoid function.

03:05.380 --> 03:13.030
And they say my function is just f of x is equal to 1 over 1 plus E to the negative x and we can replace

03:13.120 --> 03:14.760
X here with Z.

03:14.770 --> 03:17.440
So we have Z is equal to W X plus be.

03:17.440 --> 03:21.290
So we just take the whole thing that Z and plug it into the sigmoid function.

03:21.370 --> 03:26.280
So we'd have f of Z is equal to 1 over 1 plus it's the negative Z.

03:26.290 --> 03:32.760
And that would be our sieve function for the activation function so changing the activation function

03:32.790 --> 03:38.030
used can actually be really beneficial depending on the particular task we're trying to complete.

03:38.070 --> 03:42.830
Now we'll get into more detail on that later on when we actually implement our own that works.

03:44.040 --> 03:49.040
Let's discuss a few more activation of functions that we're going to encounter throughout this course.

03:49.110 --> 03:52.610
So a really common function that's used is an activation function.

03:52.620 --> 03:59.070
Besides just one function is the hyperbolic tinge of function or 10 H or tannish of Z or Z remembers

03:59.070 --> 04:05.850
just W X plus B and that looks really similar to the sigmoid function except note that the output can

04:05.850 --> 04:08.040
then be from negative 1 to 1.

04:08.130 --> 04:10.370
Unlike Previously we had zero to 1.

04:10.440 --> 04:16.020
So if we look at the Sigma function again that went from 0 to 1 and that was equal to 1 over 1 plus

04:16.080 --> 04:19.300
it's a negative x or negative Z whatever the input is.

04:19.320 --> 04:25.290
And now for the Tench or hyperbolic tangent function it goes from negative 1 to 1 but still has it sort

04:25.290 --> 04:26.690
of same behavior shape.

04:26.760 --> 04:31.830
And if you're interested in how to express the hyperbolic tangent mathematically over here on the right

04:31.830 --> 04:40.800
hand side or the actual equations for then there's also the rectified linear unit or r e l u.

04:41.360 --> 04:45.610
And this is a really common activation function but it's actually relatively simple.

04:45.710 --> 04:52.340
And the way it works is the following method you basically passing your z value into this Max equation.

04:52.350 --> 04:55.160
So you say max of 0 or Z.

04:55.460 --> 04:56.720
So what does that actually mean.

04:56.720 --> 05:01.370
Well the rectified linear unit that actual activation function works the following manner.

05:01.520 --> 05:04.720
It says based off your z value and zero.

05:04.790 --> 05:11.900
Just return the maximum value and that actually means that if your value of z is negative then the max

05:11.900 --> 05:15.030
between zero and any negative number is always going to be zero.

05:15.230 --> 05:17.950
So we can see here that is z.

05:17.990 --> 05:26.720
If it's negative the function always returns 0 and then between Max 0 comma's Z if z is positive it's

05:26.720 --> 05:28.500
always going to return the value of z.

05:28.670 --> 05:31.980
So essentially of just y is equal to Z here.

05:32.090 --> 05:35.920
So on the right hand side that's where you get that sort of straight linear line.

05:35.930 --> 05:41.240
So again relatively simple function but this is kind of considered at certain points in time state of

05:41.240 --> 05:41.990
the art.

05:42.020 --> 05:48.590
So we're going to definitely be seeing this activation function a lot throughout our lecturers so that

05:48.590 --> 05:53.090
rectified linear unit and hyperbolic tangent tend to have the best performance so we're going to really

05:53.090 --> 05:58.230
focus on these two throughout the course but deep libraries in general have these built in force.

05:58.250 --> 06:02.620
So we don't really need to worry about having to implement these activation functions manually.

06:02.660 --> 06:05.970
We'll essentially be able to select them from our deep learning library.

06:07.290 --> 06:11.280
As we continue on we'll also talk about some more State of the activation functions.

06:11.300 --> 06:16.560
But up next we're going to do is discuss cost functions which will allow us to measure how well these

06:16.560 --> 06:18.590
neurons are actually performing.

06:18.600 --> 06:24.810
So right now we've understood two major concepts the perception and then the activation function inside

06:24.810 --> 06:26.340
that particular neuron.

06:26.340 --> 06:28.350
Up next we'll discuss cost functions.

06:28.530 --> 06:29.950
Thanks and I'll see it there.

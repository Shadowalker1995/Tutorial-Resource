WEBVTT

00:05.380 --> 00:07.120
Welcome back everyone to this lecture.

00:07.120 --> 00:10.080
We're going to be talking about Jim observations.

00:11.430 --> 00:16.400
So you may have noticed that we were calling each and every step and then passing an action into it

00:16.980 --> 00:22.170
the environment step function that we saw earlier returns back really useful objects that we can use

00:22.440 --> 00:24.040
with our learning agent.

00:25.230 --> 00:30.120
So let's go ahead and discuss what actually gets returned back when we call that step function.

00:30.120 --> 00:32.640
There's four key things that get returned back.

00:32.640 --> 00:35.520
One of them is called an observation variable.

00:35.520 --> 00:40.440
So you call that step function off your environment you pass an action and one of the things is going

00:40.440 --> 00:45.780
to give you back is the observation and that's the environment specific information that represents

00:45.860 --> 00:47.550
the environment observations.

00:47.580 --> 00:51.630
So those can be things like angles of velocities the state of a game etc..

00:51.870 --> 00:56.880
In our case there's reason the cart pull we actually already know that there's a fourth dimension array

00:57.150 --> 01:02.550
which has information such as the velocity at the current position angular velocity and the angle of

01:02.550 --> 01:03.630
the pole itself.

01:03.630 --> 01:08.140
So that's the observation that gets returned once you call that step function.

01:08.160 --> 01:11.880
There's three more things that also get returned the pot calling the step function.

01:11.880 --> 01:13.560
The next one is the reward.

01:13.680 --> 01:17.040
And that's the amount of reward achieved by the previous action.

01:17.040 --> 01:22.000
Remember we passed an action in that step function and then we can get back in a reward amount.

01:22.410 --> 01:28.380
So the actual scale of this reward it varies based off what environment you're using so certain environments

01:28.380 --> 01:33.150
such as car pool are going to be different in their scale of reward versus a go game.

01:33.270 --> 01:38.640
But overall the agent should always want to increase that reward level so you can think about it as

01:38.640 --> 01:40.280
you always want to increase the reward.

01:40.380 --> 01:43.910
The actual scale of that number may change though depending on your environment.

01:45.390 --> 01:49.120
Another thing that gets returned upon passing an action is the step function.

01:49.260 --> 01:50.930
Is this done variable.

01:51.060 --> 01:55.300
And that's just a boolean indicating whether or not the environment needs to be reset.

01:55.380 --> 01:59.750
So you can imagine this is going to be really useful because you can pass this into a WHILE loop.

01:59.760 --> 02:05.520
So while this environment is not done yet you want to keep passing in the actions so that allows you

02:05.520 --> 02:10.860
to basically set up your agent ready to learn and you keep doing this over and over and over again for

02:10.860 --> 02:16.350
a certain number of iterations and then you stop those intermediate iterations based off the environment

02:16.350 --> 02:17.610
being done or not.

02:17.610 --> 02:22.110
So what would actually indicate whether the environment is done or completed.

02:22.260 --> 02:27.490
Well that can be things like losing the game of Pac-Man that would issue the done variable to be true.

02:27.660 --> 02:30.170
Or for instance your pull tipped over all the way.

02:30.240 --> 02:36.840
That means it's done it's time for the agent to start over and reset etc. and then finally there is

02:36.840 --> 02:39.130
an info object that also gets returned.

02:39.270 --> 02:42.110
That's a dictionary object with some diagnostic information.

02:42.120 --> 02:43.650
It's usually used for debugging.

02:43.680 --> 02:46.580
We don't really see it use that much in these lectures.

02:46.680 --> 02:48.450
But it also gets returned.

02:48.450 --> 02:51.200
Let's go ahead and explore what this looks like in practice.

02:51.210 --> 02:54.490
I'm going to open up my editor and create another postscript.

02:54.710 --> 02:55.050
OK.

02:55.050 --> 03:00.030
This is the postscript we were working with earlier we imported Jim we created that Karpel environment.

03:00.030 --> 03:02.250
We reset the environment to its default state.

03:02.460 --> 03:06.270
And then for about a thousand time steps we kept rendering the environment that was a little image we

03:06.270 --> 03:06.930
saw.

03:06.990 --> 03:12.260
And here we have this environment thought step action and then we passed then some sort of action.

03:12.270 --> 03:15.150
And in this case we haven't really talked that much about the actual space.

03:15.150 --> 03:16.560
We will the next lecture.

03:16.740 --> 03:22.890
But this right here this line in the action underscore space that sample that just provides a random

03:22.890 --> 03:23.590
action.

03:23.760 --> 03:27.940
And eventually if some of the random actions the cart just swings off the page.

03:27.990 --> 03:32.380
OK so let's explore what else this step function actually returns.

03:32.380 --> 03:37.430
And for this we actually are even going to render the image so you won't see an image pop up.

03:37.430 --> 03:38.630
The environment will still go.

03:38.630 --> 03:40.880
We won't just be rendering the visualization of it.

03:40.880 --> 03:45.410
And we're just going to run this for one timestep because I want to actually run this for a two time

03:45.410 --> 03:45.940
steps.

03:45.940 --> 03:49.450
Does it really matter because I want to see what's actually returned here.

03:49.790 --> 03:51.520
So let's go ahead and do this.

03:51.560 --> 03:56.470
When we say environment reset we're going to set that equal to observation.

03:56.690 --> 04:01.230
And essentially we're going to be doing here is saying this is our initial observation.

04:01.250 --> 04:10.410
So initial observation and then let's go ahead and print out the observation.

04:10.510 --> 04:11.070
Okay.

04:11.140 --> 04:12.960
So we have our initial observation.

04:13.000 --> 04:18.250
We reset the environment and then we print out the observation and this observation is basically just

04:18.250 --> 04:20.820
going to show what the environment state is right now.

04:20.830 --> 04:24.630
So we're going to see an umpire rate of four steps basically.

04:24.640 --> 04:28.960
So let's save that and let's run this script will say Python.

04:28.960 --> 04:36.150
My test whip's my test Jim PI enter.

04:36.460 --> 04:38.200
And here is our initial observation.

04:38.200 --> 04:41.090
So this is the state that the environment starts out.

04:41.080 --> 04:44.660
And so that is what return is return when you reset the state.

04:44.760 --> 04:50.320
This the array here and this basically represents that poll standing straight up with the cart in the

04:50.320 --> 04:51.290
middle position.

04:52.110 --> 04:56.940
And some things are barely off zero so you can see here that it's already starting to feel a little

04:56.940 --> 04:57.220
bit.

04:57.270 --> 05:01.380
If it was straight up it would just stay that way so we needed to be tilted a little bit so that it

05:01.380 --> 05:03.390
can continue to start moving.

05:03.390 --> 05:03.780
OK.

05:04.020 --> 05:07.260
So let's go ahead and separate this line right here.

05:08.920 --> 05:10.740
And set that as action.

05:10.750 --> 05:13.240
So remember we haven't talked that much about actions yet.

05:13.480 --> 05:16.590
But again this is just performing a random action.

05:16.660 --> 05:20.800
So in that environment the action space we will talk about this term for the next lecture.

05:20.800 --> 05:25.780
Sample It basically take a random action and set that as reaction then I'm going to pass that action

05:26.230 --> 05:28.210
into step later on.

05:28.360 --> 05:31.990
Our agent is actually going to take care of doing these actions.

05:33.320 --> 05:34.140
So we have our action.

05:34.160 --> 05:37.790
And we passed that into step and the step function returns for things.

05:37.790 --> 05:40.330
And these are the four things we were discussing in the slides.

05:40.640 --> 05:51.100
So return back in observation a reward a done variable and the info and we can set that equal to environment

05:51.100 --> 05:51.560
step.

05:51.700 --> 05:56.600
So here I'm just doing tuple and packing to grab the four things that are returned as a result of passing

05:56.620 --> 05:59.720
an action into the next step of the environment.

05:59.740 --> 06:05.950
You reward your get back an observation a reward the stun variable and info and I'm just going to copy

06:05.950 --> 06:11.500
and paste from the notes a bunch of print statements that print these out along with a little string

06:11.530 --> 06:12.510
indicating what they are.

06:12.520 --> 06:13.900
Only place that in here.

06:14.050 --> 06:16.020
All this is just from your notes.

06:16.030 --> 06:23.000
But all I'm doing here is I'm printing out each of these with a little bit of spacing for readability.

06:23.050 --> 06:27.460
So we're going to perform one random action that I'm going to print out the observation print out the

06:27.460 --> 06:28.620
reward.

06:28.620 --> 06:31.770
Print out the done variable and then print out info.

06:32.200 --> 06:34.450
So let's save that and run this again.

06:35.400 --> 06:39.620
On my test Jim and I'm going to expand this because basically we it render render everything we just

06:39.620 --> 06:41.170
have a bunch of stuff printed out.

06:41.300 --> 06:43.340
So let's scroll back up here.

06:43.340 --> 06:47.170
So we have our initial observation that is this right here.

06:47.360 --> 06:51.590
And then we performed the one random action and you can see here now our observation has changed after

06:51.590 --> 06:52.990
performing that random action.

06:53.210 --> 06:55.990
Our reward return back is just one point zero.

06:56.060 --> 06:56.850
Is it done.

06:56.880 --> 06:58.280
No it's false it's not done yet.

06:58.280 --> 07:03.200
Which kind of makes sense after one timestep you're unlikely to have the pool fall over yet and that

07:03.200 --> 07:05.760
info because we don't have any debugging issues.

07:05.830 --> 07:07.040
Just an empty dictionary.

07:07.310 --> 07:09.080
Then we perform another random action.

07:09.080 --> 07:10.830
We get back another observation.

07:11.000 --> 07:13.520
The reward right now is just 1.0 done.

07:13.550 --> 07:15.540
False info etc..

07:15.710 --> 07:23.180
So you can imagine is if we're performing an action that moves the poll with a less angle across that

07:23.180 --> 07:26.210
vertical basically fixed in the poll so it stands straight again.

07:26.210 --> 07:28.240
Our reward is going to be bigger.

07:28.430 --> 07:33.890
So eventually we're going to want our agent to take in these four values and do whatever it needs to

07:33.890 --> 07:40.040
do action wise either go left or right to initiate a bigger reward and right now convert based reward

07:40.040 --> 07:44.410
is 1.0 because since we're performing random actions we're not really doing anything.

07:44.740 --> 07:45.250
OK.

07:45.530 --> 07:47.480
So that's really all we need to understand about this.

07:47.480 --> 07:52.580
We can get back an observation a reward a done variable indicating whether or not we're done with this

07:52.580 --> 07:56.900
particular iteration the environment and then back this in for debugging.

07:56.900 --> 08:00.890
All right thanks everyone and I'll see you at the next lecture where we begin to discuss how we can

08:00.890 --> 08:03.950
actually set actions into the environment.

08:03.970 --> 08:04.540
I'll see you there.

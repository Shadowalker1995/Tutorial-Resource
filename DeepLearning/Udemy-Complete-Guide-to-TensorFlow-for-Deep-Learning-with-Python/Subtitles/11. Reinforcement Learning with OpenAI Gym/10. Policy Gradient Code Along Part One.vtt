WEBVTT

00:05.490 --> 00:06.470
Welcome back everybody.

00:06.540 --> 00:10.960
Let's go ahead and get started coding out our policy greens.

00:11.070 --> 00:12.520
First we need our imports.

00:12.540 --> 00:23.740
We're going to be using tensor flow of course as well as Jim Lewis as well as Jim and pi so say import

00:24.430 --> 00:26.570
comp. that isn't.

00:26.820 --> 00:28.120
All next step.

00:28.120 --> 00:30.680
Let's go ahead and define our variables.

00:30.700 --> 00:32.310
Very similar to what we did before.

00:33.310 --> 00:35.850
We're going to say the number of inputs will we know that's four.

00:35.860 --> 00:38.020
There's four observations coming back.

00:38.020 --> 00:42.970
Number of hidden neurons let's also keep it for and we'll just do something simple like just one layer

00:42.970 --> 00:44.350
here and then.

00:44.360 --> 00:47.050
Number outputs is going to be one again.

00:47.050 --> 00:52.450
Basically for that action whether you're going left or right and this time we're also going to be finding

00:52.450 --> 00:57.040
a learning rate for our sense of say 0.01.

00:57.370 --> 00:59.850
And we will be using again the T.F. layers API.

00:59.860 --> 01:07.130
So we're going to have an initialiser again will say initialiser is equal to T.F. contrib layers.

01:07.180 --> 01:16.790
And we're going to do it just like we did before the variants scaling initialiser or so we have our

01:16.800 --> 01:17.420
variables.

01:17.430 --> 01:21.930
Let's go ahead and create the network it's going to look similar to what we did last time with a few

01:21.930 --> 01:23.570
minor changes.

01:23.610 --> 01:31.380
So first we are going to create our placeholder and the placeholder is going to hold 32 and the shape

01:31.440 --> 01:38.500
just like last time it's going to be by the number of inputs and then we'll have our first layer let's

01:38.500 --> 01:46.040
call it the hidden layer that's going to be T.F. layers but then Slayer that's connected takes and XTi

01:46.070 --> 01:52.550
inputs has four hidden units or four neurons I should say the activation function.

01:52.580 --> 01:54.070
We're going to go ahead and choose.

01:54.660 --> 01:57.510
Let's go in and choose Eliu for this.

01:57.680 --> 02:01.670
When I was testing around that perform a little better than the rectified linear unit units an exponential

02:01.670 --> 02:02.750
linear unit.

02:02.750 --> 02:12.570
And we also need our kernel initialiser initialiser synthetical to the initialiser we find before and

02:12.570 --> 02:16.770
then our next step here is to actually get the outputs and we're going to do this in two steps.

02:16.800 --> 02:17.770
One of the logic.

02:17.790 --> 02:20.990
And then on the outputs and we'll explain why in a second here.

02:21.390 --> 02:24.200
But we're going to do the following.

02:24.280 --> 02:31.270
We're going to say T.F. Clear's dance Pessin or hidden layer.

02:31.760 --> 02:35.320
And then we want the number outputs out basically just one.

02:35.740 --> 02:43.880
And then we're also going to then find outputs and we're going to pass this sigmoid function.

02:43.920 --> 02:46.530
And the reason for that we're doing this kind of two separate steps.

02:46.530 --> 02:51.500
Previously we essentially did all this in one layer is because we're going to be using both these variables

02:51.510 --> 02:55.250
before and after the function is applied.

02:55.260 --> 02:56.550
So right now it's go ahead and do.

02:56.560 --> 03:00.980
Well we're going to do after this him would function is applied which is get those probabilities.

03:01.080 --> 03:07.790
So we'll say probabilities just like we did last time is equal to if we're going to concatenate along

03:07.800 --> 03:14.380
X to 1 values and it's going to be outputs and one minus outputs.

03:16.880 --> 03:23.180
Then we're going to here is have an action and that's going to be able to multinomial just like we did

03:23.180 --> 03:31.400
last time and we're going to pass in the probabilities and say a number of samples equal to one.

03:31.420 --> 03:31.630
OK.

03:31.630 --> 03:36.190
So very similar to what we did last time except here we're basically splitting this up into two steps

03:36.190 --> 03:38.640
because we're really using both these variables.

03:38.740 --> 03:45.490
And then finally we want to do is actually get some sort of output y to train the network on.

03:45.610 --> 03:51.710
So we're going to say why is equal to and we'll say it's going to be one minus the actual action.

03:51.730 --> 03:54.640
So the action is the probability to go left remember that.

03:54.640 --> 04:00.350
So unfortunately we can't just say one minus action because this action is actually still a tenser and

04:00.370 --> 04:01.290
intenser flow.

04:01.390 --> 04:05.130
But we can easily convert it to a floating point number using tensor flow.

04:05.290 --> 04:09.130
You can say T.F. to flow on that action.

04:10.270 --> 04:13.670
And that way we can interact with it with a normal floating point number.

04:13.780 --> 04:18.890
Right now it's a sensor here to spy action so you can only deal with it if you were to have another

04:18.970 --> 04:20.900
tensor representing just one point zero.

04:20.920 --> 04:24.940
But because we actually want this just to be a floating point number at the end we're going to go ahead

04:24.940 --> 04:29.830
and translate that action or transform that action into a floating point number here.

04:31.090 --> 04:34.730
Next up is for the last function in optimization.

04:35.050 --> 04:38.110
So we're going to be defining cross entropy here.

04:39.470 --> 04:48.080
So T.F. and I we're going to say sigmoid cross entropy with Blodgett's

04:52.520 --> 04:57.080
and we'll say the labels is equal to Y and largest sequel to lodge it's here

05:00.960 --> 05:01.330
OK.

05:01.410 --> 05:06.270
Then the optimizer set that equal to T.F. train and we're going to use an atom optimizer

05:09.210 --> 05:12.050
and then we're going to apply the learning rate from earlier.

05:12.340 --> 05:12.710
OK.

05:12.800 --> 05:15.440
And this is where it's going to differ a little bit from what we've done before.

05:15.620 --> 05:16.400
Well we're going to do.

05:16.410 --> 05:23.940
Instead of minimizing the optimizer we're going to compute the gradients using cross entropy.

05:24.500 --> 05:30.530
So we're going to see gradients and variables because this returns back actually two things we're going

05:30.540 --> 05:37.250
to grab the optimizer and instead of saying minimise based off cross entropy we're going to say compute

05:37.340 --> 05:43.670
the gradients so kind of this in-between step here.

05:43.800 --> 05:47.040
So usually when we did end up doing is saying optimizer minimize cross-bench B.

05:47.100 --> 05:51.020
Right now we're competing the gradients because we're going to multiply these later on by the discounted

05:51.030 --> 05:51.840
scores.

05:53.700 --> 05:58.200
Now in order to work with these actual gradients and variables that are return back when we compute

05:58.200 --> 06:01.910
the gradients later on we're going to want to apply the gradients.

06:01.950 --> 06:06.180
So what we need to do here is a little bit of kind of tensor flow code.

06:06.200 --> 06:10.930
I'm going to copy and paste this from the notes because it's a bit of a lot of code here.

06:10.940 --> 06:14.940
So you can go out and feel free to copy and paste this but let's walk through what this code is actually

06:14.940 --> 06:15.360
doing.

06:15.360 --> 06:20.760
So we've computed the gradients and according to this cross entropy and then we make these three variables

06:20.760 --> 06:26.520
here and lists the gradients themselves the Greaney and placeholders and then the gradients and variables

06:26.520 --> 06:27.870
feed.

06:27.900 --> 06:30.330
So then we're doing a little bit of tuple unpacking here.

06:30.330 --> 06:34.920
So for the Greaney and variables that are returned when you compute the gradients off your optimizer

06:35.250 --> 06:38.560
we're going to append just a gradient to our list of gradients.

06:38.670 --> 06:42.540
Then we're going to create a placeholder here so say gradient placeholder.

06:42.540 --> 06:44.850
Notice the singular here instead of placeholders.

06:45.000 --> 06:50.780
We create a single placeholder for that specific rate and then we append that to this list of placeholders

06:50.790 --> 06:54.990
and that is here we have a list of the gradients analyst the placeholders for the gradients and then

06:55.000 --> 06:59.040
we're going to do here is we have this gradients and variables feed.

06:59.160 --> 07:03.400
We append both the grace the gradient placeholder and the variable here.

07:03.540 --> 07:06.000
Basically creating a list of tuples.

07:06.000 --> 07:11.580
Then once we've done that we're going to at the end of all this say that our actual training operation

07:12.780 --> 07:19.390
that we're doing here is the optimizer and we're going to then apply the gradients and that's going

07:19.390 --> 07:23.180
to be that final feed we just made there.

07:23.190 --> 07:23.570
All right.

07:23.610 --> 07:29.100
So kind of some heavy tension flow just to make sure that we can separate out these steps of actual

07:29.100 --> 07:34.290
gradients instead of the typical minimized function we did before then we're going to do here is quickly

07:34.290 --> 07:40.480
create in it that we did for T.F. global variables initialiser

07:45.110 --> 07:50.460
and then also say saver's equal to T.F. train saver because we're going to save our model later on.

07:52.060 --> 07:52.650
OK.

07:52.870 --> 07:57.570
Now we need to create some or award functions and you can also check out the resource notes there's

07:57.580 --> 08:04.690
a few links or basically URLs that have hints and tips and useful links according to some of the code

08:04.690 --> 08:10.870
we did specific things like how optimized can be gradients how to calculate programmatically what the

08:10.870 --> 08:13.690
atom optimizer actually returns or what methods are available.

08:13.870 --> 08:15.750
And then there's two functions here.

08:15.790 --> 08:17.580
This helper at this count rewards function.

08:17.590 --> 08:19.750
And at this can normalize rewards function.

08:19.750 --> 08:24.670
So I'm going to copy and paste them again from the notes because that's a lot of code to kind of just

08:24.880 --> 08:26.730
go through as far as timing is concerned.

08:26.730 --> 08:29.680
So instead of just going to basically explain it.

08:30.190 --> 08:34.350
So there's two helper functions here will really help her function.

08:34.360 --> 08:39.730
And then one main function this helper function is going to calculate the discount rewards.

08:39.730 --> 08:44.560
So let's walk through this it basically takes in the rewards and then applies the discount rate you

08:44.560 --> 08:46.540
get back the discounted rewards.

08:46.570 --> 08:52.550
So we create this 00 array that has the same length as the rewards that you pass in.

08:52.780 --> 08:57.770
And then we've also taken a discount rate that can be 0.9 5 or 0.9 9 ET cetera.

08:58.000 --> 09:01.970
And then right now we have the cumulative rewards equal to zero.

09:02.060 --> 09:05.990
So we're going to say first step in reversed range.

09:05.990 --> 09:07.040
Length of rewards.

09:07.160 --> 09:09.770
So notice how we're basically going backwards here.

09:09.770 --> 09:15.770
We're going to say that the cumulative rewards is equal to the rewards of that current step plus the

09:15.770 --> 09:20.750
cumulative rewards times the discount rate then we're going to say that this kind of rewards of that

09:20.750 --> 09:23.380
step is equal to the cumulative rewards.

09:23.390 --> 09:28.840
So I think what makes it a little confusing here is the reversed function being used.

09:28.880 --> 09:33.710
But basically the reverse allows you to go backwards starting from the length the awards all the way

09:33.710 --> 09:35.270
to zero here.

09:35.270 --> 09:39.640
So that's what we're doing here and basically what we end up returning is an array of the discounted

09:39.650 --> 09:47.030
towards Once we have the counter awards we can end up doing is by passing in all rewards and the discount

09:47.030 --> 09:52.430
rate we're going to use this help or this rewards function and then normalize them using the mean and

09:52.430 --> 09:54.010
standard deviation.

09:54.020 --> 09:57.610
So this is the main function that we're going to be using later on and what it's going to do is taken

09:57.620 --> 09:59.570
all the rewards and the discount rate.

09:59.570 --> 10:03.710
So I notice that this is going to take quite a while to actually calculate this function as we play

10:03.710 --> 10:09.320
more and more games that you take in all the awards particularly at this country you're using then you

10:09.320 --> 10:14.270
create this empty list called all this kind of rewards and then you going to save for every single reward

10:14.690 --> 10:22.680
in this list of all rewards will you end up doing is you append the actual result of help or this rewards.

10:22.910 --> 10:26.900
So the rewards and discount rate so so what's happening here.

10:27.050 --> 10:30.620
We're going to have a giant list of a bunch rewards from all these games that you've played.

10:30.740 --> 10:36.050
And eventually we're going to pass these in to the help her this kind of rewards which is this function

10:36.050 --> 10:38.570
right here that actually calculates the discount.

10:38.570 --> 10:42.770
So this calculates the discount and then once you have that well we end up doing is we flatten this

10:42.860 --> 10:49.130
out using concatenate and then you calculate the mean deviation and then you perform the normalization

10:49.130 --> 10:54.440
here subtracting the mean and dividing by the standard deviation for every set of counter awards in

10:54.530 --> 10:56.000
all this kind of rewards.

10:56.000 --> 11:01.430
So this is essentially if we scroll back to the left here this this kind of rewards that we end up creating

11:01.930 --> 11:07.880
is going to be a giant list of a bunch of rewards for each of the games that we've been playing.

11:08.880 --> 11:09.610
All right.

11:09.610 --> 11:14.710
Once we've done that the next thing to do is to actually just code out our actual training session.

11:14.740 --> 11:18.520
So we're going to do that in the next video but once you come to the training session that we can then

11:18.520 --> 11:22.250
run the train model on environment and then visualize how it performed.

11:22.510 --> 11:24.310
Thanks everyone and I'll see you at the next lecture.

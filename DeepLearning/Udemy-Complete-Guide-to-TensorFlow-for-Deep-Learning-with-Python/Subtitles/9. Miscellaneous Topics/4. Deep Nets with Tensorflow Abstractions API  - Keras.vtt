WEBVTT

00:05.370 --> 00:11.070
Welcome everyone in this lecture we're going to be showing you how to use the Keres API as a T.F. abstraction

00:11.340 --> 00:13.950
and carious is actually now included within tensor flow.

00:14.070 --> 00:16.750
So we'll show you how you can use it from tensor flow.

00:16.980 --> 00:20.180
Let's jump to Jupiter notebook and discuss a few things about cars.

00:20.290 --> 00:20.580
OK.

00:20.580 --> 00:27.050
Here I am in the Jupiter notebook and as a quick note I've gone ahead and reran all the data code and

00:27.060 --> 00:31.790
i deleted from the previous lecture video all our stuff about the S-meter pay.

00:31.860 --> 00:36.950
So we're going to start fresh with carrots and if you go to the carrots documentation page here.

00:37.080 --> 00:38.880
It's a python deep learning library.

00:38.910 --> 00:42.870
And basically what it is it's a high level neural network API.

00:42.870 --> 00:48.440
It's written in Python and it doesn't just run on top of tensor flow it can actually run on top of C.A.

00:48.440 --> 00:49.790
Kate or C.A..

00:49.980 --> 00:55.440
And if you want to use it in that manner you'll need to download keras as a separate library.

00:55.440 --> 01:01.650
With the release of some newer versions of course what happened was cars eventually became part of tensor

01:01.650 --> 01:02.250
flow.

01:02.460 --> 01:08.160
And as I mentioned earlier in the beginning of the abstractions lecture series the contrib portion of

01:08.160 --> 01:12.630
tensor flow has that section of things that are contributed to tensor flow.

01:12.750 --> 01:14.810
So that's where Carse's.

01:14.970 --> 01:23.250
So if we come back here after imports says T.F. I can also show you this from tensor flow dot contrib.

01:23.550 --> 01:29.250
If you hit tab here you'll eventually see a list of all the options.

01:29.260 --> 01:34.630
So here they are we see there's Android based flow Bouzid trees distribution's factorization framework

01:34.960 --> 01:37.620
etc. layers which we'll discuss later on.

01:37.630 --> 01:43.780
There's also a legacy code in here that will probably not be updated in the future metrics losses predictor.

01:43.780 --> 01:47.050
And one of them you can eventually see is curse.

01:47.080 --> 01:51.670
So basically the entire library is now with intenser full here.

01:51.850 --> 01:57.620
So now you can import anything that's available in cars and there's activations applications etc..

01:57.760 --> 02:00.980
So the first single import is models.

02:01.120 --> 02:04.410
Keep in mind what we're going to show here is a really small part.

02:04.450 --> 02:09.910
It's just building a very simple deep densely connected neural network curiousest can actually do things

02:09.910 --> 02:11.650
like convolutional or one that works for current near.

02:11.650 --> 02:13.790
That works all the higher level API.

02:13.930 --> 02:15.720
Again you're sacrificing control for that.

02:15.790 --> 02:19.200
But if you're more Istomin cares go ahead and check out the current documentation.

02:19.210 --> 02:24.090
Right now we're showing how you can use it as a basic setup for a high level API.

02:24.330 --> 02:24.700
OK.

02:24.850 --> 02:26.320
So important models.

02:26.320 --> 02:31.080
So I go ahead and with carriers we end up doing is you first create a model.

02:31.300 --> 02:36.910
So I'll say my deep neural network care model is equal to models dots.

02:37.000 --> 02:42.370
And here you can and that doing various models but the ones are mainly going to call his sequential

02:42.490 --> 02:47.290
model and it's sequential because you basically just have a sequence of layers.

02:47.320 --> 02:56.230
So in order to add layers of the model what you need to do is call from tensor flow contrib keris and

02:56.230 --> 03:01.570
we're going to import layers from Keres and curious layers.

03:01.570 --> 03:07.240
If you take a look at it do layers tab you realize that there's actually a bunch of layers already created

03:07.240 --> 03:11.940
for you and Cara's There's the average pooling layer convolutional layers to the 3D.

03:12.070 --> 03:16.330
All this stuff but we're really going to be focusing on the simplest layer which is just a dense layer

03:16.600 --> 03:17.530
that's connected.

03:17.550 --> 03:19.590
There's also drop layers et cetera.

03:20.260 --> 03:26.170
So you can explore that later on but will first create a layer and what you end up doing is you take

03:26.170 --> 03:27.060
your model.

03:27.250 --> 03:33.940
That model we just define densely or near that work Cara's model and you call ad and then you end up

03:33.970 --> 03:36.600
adding layers to it these layer objects.

03:36.610 --> 03:41.710
So for dense network what you need end up defining is the number of units in this particular layer.

03:41.740 --> 03:47.140
So we have 13 units in this layer and you also need to fine if it's the first one the input dimensions

03:47.230 --> 03:50.640
and the input dimensions for us is going to be 13 features.

03:50.770 --> 03:53.380
And then you can also define an activation.

03:53.380 --> 03:59.260
So we'll go ahead and pass in the street code for activation as our E-L you are rectified linear unit

03:59.710 --> 04:03.820
and later on we'll show you how you can discover the string codes for that.

04:03.820 --> 04:07.310
You could just go to the doc documentation but you can actually see it quickly.

04:07.390 --> 04:10.440
But we'll talk about that later on.

04:10.490 --> 04:12.160
Forget the last prince who's there.

04:13.000 --> 04:13.450
OK.

04:13.650 --> 04:18.160
So let's basically mimic what we did before and added a couple more layers here.

04:18.300 --> 04:20.520
Well again say Kurus model.

04:20.530 --> 04:25.260
I'm going to add and here you know you could technically add whatever kind of players you want if you

04:25.260 --> 04:28.100
for whatever reason needed a convolutional ere you do that as well.

04:28.380 --> 04:31.690
But keeping things simple We'll have another dance layer.

04:31.920 --> 04:36.870
We just say units is 13 because it's no longer the first input layer we don't need to find the dimensions

04:37.230 --> 04:42.070
or we need to say again is the activation function which will pass in the street code there.

04:42.450 --> 04:44.230
So I'm going to copy this and paste this.

04:44.300 --> 04:51.720
So then we essentially add two more layers there and then finally when we get to our last layer we'll

04:51.720 --> 04:58.260
say model and I'm going to add in other dense layer but the number of units is going to have is just

04:58.300 --> 04:59.470
equal to three.

04:59.470 --> 05:06.490
And the reason for that is because we're going to be using the activation function of a soft Max activation.

05:06.490 --> 05:12.060
Remember back to the digits data lecture we had the one hot encoded classes.

05:12.220 --> 05:16.960
So we're essentially going to be doing a very similar thing here except carious is going to take care

05:16.960 --> 05:18.030
of all that in the background.

05:18.040 --> 05:19.430
So we'll explain that in a little bit.

05:19.600 --> 05:25.570
But essentially we just have three units one for each class using a soft mass Max activation there and

05:25.570 --> 05:28.210
that's going to be our last layer.

05:28.240 --> 05:28.560
OK.

05:28.600 --> 05:30.730
Next we want to compile the model.

05:30.970 --> 05:39.610
So the next thing I'm going to do is say from tensor flow contrib thuck Harris and going to import losses

05:40.090 --> 05:45.050
optimizers metrics and activations.

05:45.300 --> 05:49.790
So previously I said there's a quick way to figure out what activations are available.

05:49.860 --> 05:53.970
So you don't just have to memorize these particular string codes and the quick way to do that is once

05:53.970 --> 06:02.740
you've imported these you can just say activations dots that tab here and we'll show you all the activations

06:02.740 --> 06:03.800
that are available for you.

06:04.000 --> 06:07.860
And basically all the ones that you would expect are here for you their sigmoid activation and soft

06:07.870 --> 06:10.760
Max rectifies linear unit etc..

06:10.930 --> 06:13.860
Hyperbolic tangent all the basic ones are here for you.

06:14.110 --> 06:16.230
So keep that in mind.

06:16.450 --> 06:21.400
And if you're looking for whatever optimizers are available free you can do optimizers thought and then

06:21.430 --> 06:22.000
hit tab.

06:22.000 --> 06:25.000
Adam itemizes is available armis prop.

06:25.120 --> 06:26.050
Gradient descent.

06:26.110 --> 06:28.290
Cetera.

06:28.350 --> 06:31.420
So we're going to do here is the following.

06:31.700 --> 06:35.620
We are going to say grab our model.

06:35.720 --> 06:40.700
So the final step after he added the layers is to compile it which basically sets up all the layers

06:40.760 --> 06:41.680
as they should be.

06:41.990 --> 06:45.800
So the compiling step needs an optimizer and it's a loss function.

06:45.980 --> 06:52.570
And if you're doing classification you can pasand some metrics to classify by say optimizer and we're

06:52.570 --> 06:57.580
going to pass that string code Adam which we saw right now how we can get it we just say optimizers

06:57.610 --> 06:59.980
dot and then see the list of itemizes available.

07:00.160 --> 07:01.050
And that for a.

07:01.080 --> 07:10.780
We're going to do is the following will say sparse underscore categorical underscore cross entropy.

07:11.040 --> 07:15.690
And for this one I would suggest looking into the documentation as far as which ones are appropriate.

07:15.690 --> 07:18.500
It kind of depends on what format your data is in.

07:18.630 --> 07:25.410
If your data is not one hot encoded coming in in order for Caris understand that we need to do is say

07:25.410 --> 07:32.130
sparse categorical cross entropy if your data is already hot and coded then you can just do categorical

07:32.130 --> 07:33.340
cross entropy.

07:33.360 --> 07:34.410
So keep that in mind.

07:34.410 --> 07:37.280
There's a lot more detail in the current documentation about this.

07:37.320 --> 07:40.350
You won't really find this sort of detail in the text for the documentation.

07:40.350 --> 07:45.000
Instead they would expect you to go to Paris and read about it because essentially everything that's

07:45.000 --> 07:48.290
included in this is already here for you contrib that cares.

07:48.300 --> 07:54.500
So we're using sparse Ketek we'll cross entropy because we're using categories as we have three of them.

07:54.510 --> 07:58.940
Number those three classes and we're using sparse here because coming in they're not one high end code

07:58.980 --> 08:05.200
already where you could do is one high end code them and then just have categorical entropy if you wanted

08:05.200 --> 08:06.410
to.

08:06.460 --> 08:10.960
So all the same metrics is going to be accuracy in case you ever want to do some sort of evaluation

08:11.940 --> 08:15.110
so you compile that model AUPs that should just be single.

08:15.310 --> 08:16.090
Yes.

08:16.140 --> 08:20.300
That you run that and now it's ready to be trained.

08:20.520 --> 08:25.830
So again the steps here was just to import your model create an instance of that sequential model and

08:25.830 --> 08:27.070
you just add layers to it.

08:27.090 --> 08:29.290
Pretty straightforward once you out of the layers.

08:29.310 --> 08:34.230
You compile it with whatever optimize you want the last function you want and then if it's classification

08:34.230 --> 08:37.220
you can add things like metrics.

08:37.260 --> 08:42.960
So let's go ahead and train this model will say the end of this model and we're going to fit it to our

08:42.960 --> 08:44.190
scaled data.

08:44.190 --> 08:45.950
So scale x train.

08:46.050 --> 08:53.450
And then our right train and then go in and train this for I don't know 50 parks or something so run

08:53.450 --> 08:55.260
that you should see some output like this.

08:55.250 --> 08:58.500
It probably won't run as fast as that here but mine is already done.

08:58.520 --> 09:01.910
You'll see the loss and the accuracy metric kind of go up.

09:01.910 --> 09:05.570
So that's basically what we were asked for right here as far as the metrics.

09:05.630 --> 09:10.300
So if you scroll all the way down hopefully you started reaching close to 95.

09:10.340 --> 09:15.410
So if you want to get predictions off of this it's actually quite easy you just say predictions as equal

09:15.410 --> 09:19.320
to whatever your model name was.

09:19.320 --> 09:21.200
And you can call predict.

09:21.200 --> 09:26.500
And if you just can't predict that's going to return back the actual like soft Max the output layer.

09:26.660 --> 09:33.710
So we won't predict classes here and there we're going to pass our scaled X test data and then let's

09:33.710 --> 09:35.930
print out our classification report.

09:36.350 --> 09:46.340
So let me actually import that from S-K learn dot metrics import classification report and let's run

09:46.340 --> 09:49.350
that classification report.

09:49.500 --> 09:54.890
In our predictions and then pass an the test values.

09:54.900 --> 10:00.260
So if you're in that you should get somewhere around 80s or 90s percent.

10:00.450 --> 10:05.580
I believe you ran this enough you should be able to get up to 95 percent maybe even higher.

10:05.600 --> 10:08.480
OK so let's quickly review what we just did here of course.

10:08.490 --> 10:12.660
It's a really great library especially just starting out learning of machine learning and you're not

10:12.660 --> 10:19.430
sure what actual framework you want to use because you can basically put this on top of C A.K. or thanto.

10:19.630 --> 10:25.390
But again you just tent's food that contribute Cara's import models create a sequential model instance

10:25.750 --> 10:29.980
and then whatever layers you want you just add those layers on.

10:30.040 --> 10:34.900
Once you've done that you go ahead and compile it using whatever optimized you want the loss and the

10:34.900 --> 10:37.090
metrics and then you can fit this.

10:37.090 --> 10:41.590
Now keep in mind this is a really simple example because we're just doing a densely connected neural

10:41.590 --> 10:44.260
network and it's pretty straightforward.

10:44.260 --> 10:48.910
But this sometimes gets a little more complicated as you add in more complex layers like recurrent neural

10:48.910 --> 10:51.300
network layers or convolutional layers.

10:51.310 --> 10:55.770
It's still intended to be much simpler than just doing it straight in pure tensor flow.

10:55.870 --> 10:57.130
But keep that in mind.

10:57.400 --> 11:01.330
All right so that's if you're interested in learning more about care since this course is mainly focused

11:01.390 --> 11:06.390
on tensor flow I would recommend you check out the curiousest documentation is actually quite good and

11:06.410 --> 11:09.840
they even have things like getting started guys for like a sequential model.

11:09.970 --> 11:14.200
So you click here and it will tell you how to do a specific sequential model so specify an input shape

11:14.620 --> 11:16.840
the compilation of it training etc..

11:16.870 --> 11:20.130
So lots and lots of examples even has a full example folder.

11:20.140 --> 11:24.670
If you go to that page it has lots of examples here on how to do various models.

11:24.670 --> 11:26.110
It's a really great library.

11:26.290 --> 11:30.180
And in my opinion it's the future of abstractions for tensor flow.

11:30.220 --> 11:34.210
I believe this is probably the number one abstraction in the future.

11:34.210 --> 11:39.640
Granted I may be totally wrong about that but Keres is probably the strongest one and that's especially

11:39.640 --> 11:45.680
noted because it was an entirely separate library that got contributed into tensor flow again.

11:45.730 --> 11:50.080
Don't forget about the estimator API and coming up next we're going to show you the layers API which

11:50.080 --> 11:51.330
is also really popular.

11:51.580 --> 11:54.300
So we'll cover the layers API in the very next lecture.

WEBVTT

00:05.320 --> 00:10.510
Welcome everyone to part one of our code along we're going to create a generative adversarial network

00:10.840 --> 00:13.580
to generate this data digits.

00:13.600 --> 00:16.100
Let's go ahead and open up a notebook and get started.

00:16.420 --> 00:23.620
All right let's start of our imports will do tensor flow of course as T.F. and then we're also going

00:23.620 --> 00:29.570
to be using pi and we'll be plotting out just a little bit just see what the numbers look like.

00:29.710 --> 00:35.760
So let's go ahead and import pipeline and we'll say map lib in line.

00:37.080 --> 00:37.370
OK.

00:37.380 --> 00:39.210
So let's go and grab our data.

00:39.600 --> 00:44.250
Remember senseful has a really nice function that allows you to quickly grab the data as you say from

00:44.250 --> 00:48.660
examples tutorials and just import input data

00:51.650 --> 00:57.680
and then we're going to say this is equal to input underscored data and we're going to say read data

00:57.680 --> 01:02.220
sets and I'm going to pass in the exact file path from the notebook.

01:02.370 --> 01:04.980
That way I don't accidently read download all the data.

01:05.130 --> 01:09.630
So I'm just going to copy and paste this to make sure I don't mess it up here.

01:09.630 --> 01:10.120
There we go.

01:10.170 --> 01:13.920
So I'm gonna grab it again from the convolutional neural networks and this dataset.

01:14.040 --> 01:19.500
When you run this you should only see extracting if you're running this in the same folder as the notebook

01:19.500 --> 01:19.980
expect.

01:19.980 --> 01:23.050
So keep that in mind though you don't need to download everything again.

01:23.100 --> 01:30.040
So a quick review remember if we want to show the data you can say this just training.

01:30.080 --> 01:36.080
And then there's images here and you can grab just a summary in the image here so whatever is the 12th

01:36.080 --> 01:42.510
index the image and then you need to reshape it to a 28 by 28 pixel run that and that should show you

01:42.510 --> 01:44.910
whatever image it is in this case it's a 1.

01:44.910 --> 01:49.510
So if we took out another image then this one looks like a 2 or 9.

01:50.100 --> 01:51.210
This one's a 7.

01:51.240 --> 01:52.080
Clearly.

01:52.080 --> 01:52.500
OK.

01:52.710 --> 01:59.670
And then remember that we can also say See map so it can say grace here just to see the actual white

01:59.670 --> 02:00.770
background.

02:01.290 --> 02:01.530
All right.

02:01.530 --> 02:06.240
Now it's time to actually build out the networks the first network we're going to start with is the

02:06.270 --> 02:11.190
generator network and then we're going to create the discriminated network for both these networks we're

02:11.190 --> 02:14.160
going to create what's called a variable scope.

02:14.160 --> 02:17.370
So let's first start with some functions.

02:17.370 --> 02:21.030
So we're going to create a generator function that we can easily call it.

02:21.140 --> 02:27.660
It takes Z as an input as he is going to be that random noise that we start the generator with and then

02:27.660 --> 02:34.580
it's going to have this redos variable that will explain later on when we talk about the variable scope.

02:34.590 --> 02:41.530
So we'll say with T.F. variable underscore scope we're going to pass in a string code here.

02:41.560 --> 02:46.660
If you shift tab you can see that there's a name or scope piercing and passenger string code will say

02:46.850 --> 02:49.570
GM for generator.

02:49.740 --> 02:52.650
And they're going to say reuse is equal to reuse.

02:52.650 --> 02:58.030
So you can either set this reuse as one or later on you're going to see is all set it as true.

02:58.050 --> 03:01.620
So you might be wondering well what is this variable scope for.

03:01.620 --> 03:10.320
Well the goal of variable scopes in general is to allow you to basically have modular sections or subsets

03:10.320 --> 03:16.290
of parameters so those belonging to certain layers that way when an architecture of a layer is repeated.

03:16.290 --> 03:21.090
So when you call this generator function again the same names of those variables can be used within

03:21.150 --> 03:22.620
each layer scope.

03:22.770 --> 03:26.370
That's going to make a little more sense when we actually see how we use this function because we're

03:26.370 --> 03:28.360
going to be using it over and over again.

03:28.410 --> 03:34.030
So keep that in mind that we're going to be using this variable's scope and this reuse.

03:34.040 --> 03:36.890
We're going to pass in later to be true.

03:36.950 --> 03:45.310
So what kind of touch on that later on right now will set the hidden layer for society of.

03:45.870 --> 03:48.890
And it's going to be a dense layer so densely connected.

03:49.170 --> 03:54.180
The inputs here is just going to be that random noise Z that will create later on and the number of

03:54.180 --> 03:57.020
units will just say 128 as we did before.

03:57.030 --> 04:03.660
When dealing with the state digits data set and then what we're going to do here is for our hidden layer

04:04.170 --> 04:11.190
the activation function we're going to be using is a leaky rectified linear unit So remember we talked

04:11.190 --> 04:16.020
about leaky rectified linear units we're talking about various activation functions.

04:16.050 --> 04:23.450
Unfortunately tensor flow in 1.3 still doesn't actually have a leaky rectified linear unit.

04:23.700 --> 04:27.330
And if you look this up on Stack Overflow let me bring in the link real quick.

04:27.330 --> 04:31.140
You'll get something like this where someone's asking well how can you use a leaky rectified linear

04:31.140 --> 04:31.740
unit.

04:31.860 --> 04:33.210
And there's a couple of good answers here.

04:33.210 --> 04:35.460
Let me zoom in so you can see this.

04:35.680 --> 04:39.680
If you take a look one of them actually just defines the full function here.

04:39.820 --> 04:46.240
And then another person has a really clever solution where you just use T.F. that maximum X or versus

04:46.270 --> 04:51.880
Alpha times X where you choose that Alpha basically essentially the leak rate of that leaky rectified

04:51.880 --> 04:52.640
linear unit.

04:52.840 --> 04:55.880
And this is a lot more efficient to use so we're going to be using that.

04:55.950 --> 05:01.780
And you'll notice at the very last post answered pretty recently is that there's going to be a leaky

05:01.810 --> 05:07.060
rectified linear unit upon the release of 1.4 which at the time of this filming isn't out yet and the

05:07.060 --> 05:09.940
environment we provide is not using 1.4.

05:09.940 --> 05:15.850
So eventually in the future you'll just be able to pasan TFA and that leaky rectified linear unit as

05:15.850 --> 05:17.170
the activation function here.

05:17.170 --> 05:24.220
Because remember for these layers if you do shift tab here there's going to be an activation option.

05:24.260 --> 05:28.300
OK so let's go ahead and do the maximum method.

05:28.330 --> 05:34.660
If we come back here we're going to be using this maximum Athen to simulate the activation function.

05:34.690 --> 05:36.840
So let's come over here and start typing that out.

05:38.020 --> 05:45.920
So we're going to choose an alpha of 0.01 for Aliki rectified linear unit and then we're going to do

05:45.920 --> 05:56.050
that kind of maximun logic here so I'll say T.F. maximum and we get to say Alpha times hit in one or

05:56.050 --> 05:59.200
hit and one says it's just going to choose the maximum out of that.

06:00.040 --> 06:05.280
And then we're going to do another hit and we'll say hit and Larry to again that's going to be T.F.

06:06.040 --> 06:13.360
layers that really connected the inputs for this one is going to be the up or hit in one and we'll have

06:13.360 --> 06:19.240
this be 120 units again and again something being play around of how many units you want.

06:19.240 --> 06:23.600
He wanted to you could have a really really large network if you wanted very good generation results

06:23.980 --> 06:25.800
but 128 should be more than fine.

06:25.810 --> 06:31.460
In fact you can kind of play around with if you only want one hidden layer but let's say hit insue now

06:31.980 --> 06:40.170
is going to be the same thing T.F. maximum and it will be Alpha times it into or hit into whichever

06:40.170 --> 06:42.060
one's maximum there.

06:42.160 --> 06:48.030
OK finally we're going to have our output and that's going to be T.F. layer's that's connected will

06:48.070 --> 06:50.030
take in it and layer 2.

06:50.430 --> 06:51.500
And then the units here.

06:51.510 --> 06:52.770
Because as the output.

06:52.790 --> 06:55.260
And we're actually generating a new picture.

06:55.290 --> 06:57.300
We're going to have the full 28 by 28.

06:57.540 --> 07:04.140
So we expect 784 to be our output here because we're actually trying to generate that vector image.

07:04.500 --> 07:10.650
And then the activation function when kind of playing around with this I notice that the hyperbolic

07:10.650 --> 07:13.710
tangent tended to work better than a sigmoid.

07:13.710 --> 07:16.790
So we'll go ahead and set up our network to use that.

07:16.800 --> 07:17.880
So say 10 H there.

07:17.910 --> 07:22.400
And later on you'll see that we're passing in random noise instead of going from zero to 1.

07:22.410 --> 07:24.040
We're going to go from negative 1 to 1.

07:24.060 --> 07:26.440
Just like the hyperbolic tangent does.

07:26.550 --> 07:30.240
And then finally we're going to return the output.

07:30.270 --> 07:33.000
OK so that is our generator function.

07:33.030 --> 07:37.770
We're going to do something really similar for our discriminator murmured the discriminator is going

07:37.770 --> 07:43.490
to try to tell whether something is real or fake from the generator.

07:43.530 --> 07:50.040
So let's go ahead and make this a discriminator and this is not going to take in Z.

07:50.050 --> 07:54.110
Instead it's going to take in data X but we'll have this for use.

07:54.150 --> 07:59.870
And instead of Jeon here we'll go ahead and say yes for the discriminator hit and one that's going to

07:59.870 --> 08:05.680
be the same thing except the inputs here is going to be X we'll go ahead and do the leaky rectified

08:05.680 --> 08:08.040
linear unit using this method of Alpha.

08:08.050 --> 08:09.970
So this is going to be the same.

08:09.970 --> 08:14.290
This is again going to be the same we'll have to hit in later here and at the very end the set of this

08:14.290 --> 08:17.880
activation function being a hyperbolic tangent we just want to know.

08:17.890 --> 08:19.030
True or false.

08:19.180 --> 08:22.420
Is it a real data point or not.

08:22.660 --> 08:28.840
So we're going to do is separate this out into two steps to say logic's themselves see a Flair's thence

08:29.450 --> 08:35.950
pass and head into and say units is equal to just 1 because we basically want the probability of it

08:35.950 --> 08:37.240
being real or fake.

08:37.410 --> 08:44.590
And then also the output is T.F. sigmoid of the logic's.

08:44.600 --> 08:46.670
OK so that's our scrimmage there.

08:46.670 --> 08:51.980
So now we have these two functions that we're going to be able to quickly call upon when we want to

08:51.980 --> 08:54.890
build out our generators and discriminators.

08:54.890 --> 08:55.310
OK.

08:55.520 --> 08:59.990
So in the very next lecture we're going to do is begin to actually use these functions along with some

08:59.990 --> 09:04.020
placeholders and then define a loss function as well as the optimizer.

09:04.040 --> 09:05.000
I'll see at the next lecture.

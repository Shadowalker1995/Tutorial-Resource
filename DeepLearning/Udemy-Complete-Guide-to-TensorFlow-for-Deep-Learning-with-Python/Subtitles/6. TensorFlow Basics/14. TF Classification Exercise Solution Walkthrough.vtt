WEBVTT

00:05.250 --> 00:09.360
Welcome everyone to the solution lecture for the tense flow classification exercise.

00:09.450 --> 00:12.780
Let's go to the Juber notebook in code along the solutions.

00:12.780 --> 00:14.410
All right so let's get started.

00:14.670 --> 00:15.960
Firstly in the data.

00:16.050 --> 00:25.260
So I need to import it also import Pandurs as PD and then I'm going to say census is equal to PD read

00:25.260 --> 00:29.360
C S V and then I'm going to Redan census data that CXXVIII.

00:29.490 --> 00:31.370
Remember you have to pass in the whole file path.

00:31.380 --> 00:38.680
If you're not in the same directory and then we'll say sensus head so we can check out the data so you

00:38.680 --> 00:43.720
can see here I have a lot of categorical categorical data where we have strings as the categories so

00:43.710 --> 00:48.060
we need to work with that later on with tensor Flo's estimator API.

00:48.490 --> 00:54.930
Up next we need to say that the label columns of less than or equal to 50 k are greater than 50 k those

00:54.940 --> 01:00.400
are strings and we need to convert them to zero and ones in order for tent's flow to understand.

01:00.400 --> 01:06.670
So the way I did this is first off if you ever want to figure out what are the actual unique values

01:07.060 --> 01:15.800
inside a column you can just say unique as a method of a column and report back that unique values here.

01:15.800 --> 01:19.400
So we know now that these are the strings we need to convert.

01:19.460 --> 01:23.060
So the way we can do this is using the panels to apply method.

01:23.120 --> 01:28.340
So I create a function that converts these two strings to either zero or 1 and then apply it to the

01:28.340 --> 01:29.670
entire column.

01:29.810 --> 01:33.320
So I'll say label 6.

01:33.520 --> 01:38.710
It takes in a label that is either the string or the string and we'll do the following.

01:38.740 --> 01:44.800
If that label is equal to the string right here and again note the spacing there's a space right there

01:44.800 --> 01:46.040
in that string.

01:46.120 --> 01:51.700
Then I'm going to return a zero else I'll return a 1.

01:51.910 --> 01:55.840
And if you check out the solutions book there's kind of a nice little land expression that does this

01:55.870 --> 01:56.780
all in one line.

01:56.950 --> 01:59.610
But I think this one is really clear on what it's actually doing.

02:00.750 --> 02:03.230
And Chips equals equals.

02:03.240 --> 02:04.000
There we go.

02:04.380 --> 02:14.840
And then I'm going to say sensus income bracket is not equal to sensus income bracket how about a complete

02:15.350 --> 02:22.030
and then I'm going to say apply and I will apply the function so labeled 6.

02:22.100 --> 02:25.130
I know here I'm not calling the function I'm just passing it in there.

02:25.430 --> 02:25.910
Perfect.

02:25.910 --> 02:33.370
So now if I take a look at my data from again census had run that and if I go all the way to the right

02:33.750 --> 02:37.910
you know Notice here that the income brackets are either zeros or ones.

02:37.910 --> 02:39.940
Now I want to perform a train test split on the data.

02:39.940 --> 02:45.160
It's pretty straightforward from S-K learn model selection.

02:45.160 --> 02:50.470
Import train to split run that you want to grab our data.

02:50.800 --> 02:57.450
So our actual features that's going to be sensus and then we're going to drop the income bracket column.

02:57.740 --> 03:03.200
So income bracket and we want to drop that along access is equal to 1.

03:03.220 --> 03:07.020
Then we also want to get our y true or Y labels.

03:07.660 --> 03:11.960
And that's just going to be equal to the income bracket column.

03:12.360 --> 03:13.040
There we go.

03:13.110 --> 03:16.880
And then we call train to split shift enter.

03:16.980 --> 03:18.590
Expand on this.

03:18.630 --> 03:23.410
Scroll down until we find this nice little convenience line copy.

03:24.870 --> 03:28.470
Paste that in and then we just need to do a couple of replacements here.

03:28.470 --> 03:37.800
Change this to be ex-state to change that to be white labels test size is just 30 percent.

03:38.280 --> 03:40.780
And I do want one for my random state.

03:41.130 --> 03:41.360
OK.

03:41.370 --> 03:45.400
So there's a train to split up next you want to create the feature columns.

03:45.810 --> 03:51.360
So I always recommend using the name your data frame and then calling the columns to get a list of the

03:51.360 --> 03:52.050
column names.

03:52.050 --> 03:54.500
This gives you the exact strings in the passen.

03:54.510 --> 03:57.940
First off we're going to create the columns for the categorical values.

03:58.100 --> 04:02.430
I want to show you two examples and then I get a copy and paste the rest from the actual notebook.

04:02.430 --> 04:07.630
But first off I need to import tensor flow as T.F..

04:07.640 --> 04:11.700
OK so let's create the feature columns or do two of them.

04:11.710 --> 04:17.640
So for the gender column I would need to say T.F. feature column is a categorical column.

04:17.740 --> 04:22.540
And since I know there's only two genders female and male I'll say with a vocab list.

04:24.000 --> 04:29.850
So I pasan gender is the name of the column and then I pass in a list of possible options which is just

04:30.390 --> 04:34.060
female and male.

04:34.150 --> 04:38.890
So that's one way to do it the other way is let's say you have the occupation column where I have no

04:38.890 --> 04:43.070
idea how many categories there are and I don't want to provide a full vocab list.

04:43.360 --> 04:45.700
Well I just create an occupation variable here.

04:46.150 --> 04:48.070
It's going to be a feature column.

04:48.070 --> 04:49.650
Again a categorical column.

04:49.660 --> 04:51.510
But instead I'm going to use a hash bucket.

04:53.100 --> 04:58.450
We'll say occupation the name of the column and then I just choose the hash bucket size large enough

04:58.450 --> 05:04.570
that I know I will run into any limitation so I can just put a thousand here because I know there's

05:04.660 --> 05:07.250
no more than 1000 occupation's.

05:07.390 --> 05:07.720
All right.

05:07.750 --> 05:12.250
And you can do the same thing for the various other ones either provide a vocab list or provide the

05:12.250 --> 05:13.100
hash buckets.

05:13.150 --> 05:15.300
Obviously hash book it's a lot easier.

05:15.310 --> 05:17.830
So let me just copy and paste from the solutions book here.

05:18.790 --> 05:24.160
And there we have now all our categorical columns so that the marital status relationship education

05:24.190 --> 05:26.410
or class and then native country.

05:26.440 --> 05:30.680
So I just set the hash bucket size 2000 because I know that's a large enough number.

05:30.760 --> 05:32.610
It's not going to make a thousand categories.

05:32.620 --> 05:35.340
It's just going to make up to 1000 categories.

05:35.440 --> 05:35.990
OK.

05:36.190 --> 05:37.600
Now we have the continuous features.

05:37.600 --> 05:39.700
So let me just do one example here.

05:39.700 --> 05:46.800
We've done this before we say a feature column numeric column and then we'd just say age.

05:46.800 --> 05:47.080
All right.

05:47.080 --> 05:51.900
So then I'm going to copy and paste the rest of these from the solutions.

05:51.910 --> 05:55.510
No but I have these for you so you can copy and paste this as well.

05:55.510 --> 05:58.170
Now we want to put all these variables into a single list.

05:58.390 --> 06:02.150
So we just say feet calls is equal to.

06:02.170 --> 06:08.910
And then we just create our list with these variables so gender occupation marital status etc..

06:08.950 --> 06:13.170
So I'm going to copy and paste this so you don't see me just type a bunch of variable names here.

06:15.380 --> 06:16.640
There we go.

06:16.670 --> 06:18.410
So that is our feature column.

06:18.410 --> 06:20.560
So we just created for the T.F. estimator.

06:20.570 --> 06:23.090
Now it's time for the input function.

06:23.180 --> 06:24.880
So the bet size is up to you.

06:24.980 --> 06:27.440
But I mentioned that I did choose a batch size of 100.

06:27.440 --> 06:30.060
You can kind of play around to see what works best for your model.

06:31.720 --> 06:38.200
So same tensor flow estimator inputs and we're dealing with panel data frames so we'll choose a Pancho's

06:38.260 --> 06:39.120
input function.

06:39.960 --> 06:48.570
And then we're going to say x is equal to x train Y is equal to y train I'll choose batch size equal

06:48.570 --> 06:53.550
to 100 will leave number of the pox as none that is default.

06:53.590 --> 06:59.300
And then we also want to say shuffle is equal to true because the training input function.

06:59.470 --> 07:00.950
Next we want to create our model.

07:01.000 --> 07:05.710
So let's go and do that we'll say model is equal to T.F. estimator.

07:06.040 --> 07:07.970
And we're going to use a linear classifier.

07:08.230 --> 07:13.270
As I mentioned if you want to use a densely neural network classifier are connected in your own that

07:13.270 --> 07:19.030
were classifier then back for these categorical columns you'll need to convert them into embedded categorical

07:19.030 --> 07:19.610
columns.

07:19.720 --> 07:23.290
So you need to pass all of these into that embedding function.

07:23.290 --> 07:25.320
So go ahead and review the lectures and we did that.

07:25.630 --> 07:28.480
But for right now we're using linear classifiers we don't need to do that.

07:28.480 --> 07:33.760
We basically just need to say what the feature columns are which is feek calls.

07:33.760 --> 07:35.320
All right so we have our linear classifier.

07:35.350 --> 07:37.330
Now it's time to trayner data.

07:37.330 --> 07:45.430
So I'll say model train input function we'll say that's equal to the input phunk we created.

07:45.580 --> 07:47.700
And you provide how many training steps you can do.

07:47.920 --> 07:49.350
So it's really up to you.

07:49.560 --> 07:56.750
I'll just do 10000 pretty fast computer and then I'm going to hop forward in time for the evaluation.

07:56.890 --> 07:58.500
All right so hopping forward into the future.

07:58.510 --> 08:00.220
My model is now done training.

08:00.220 --> 08:02.880
It's time to create a prediction function.

08:03.250 --> 08:04.410
So let's do that.

08:04.400 --> 08:11.330
We'll say my prediction function is going to have some major thought inputs that pand those input function.

08:11.500 --> 08:18.890
And because I'm using prediction not evaluation I really only need the features if I was doing evaluation

08:18.980 --> 08:23.310
then I would end up needing something to evaluate against such as white tests.

08:23.360 --> 08:31.230
But in this case I'm just going to say test will say batch size is equal to the length of X test.

08:31.560 --> 08:37.200
And then because we want to use this for prediction we'll say shuffle is equal to false and then I just

08:37.200 --> 08:45.110
use model that predict and I pass that input function is equal to that prediction function.

08:45.110 --> 08:46.050
I just made.

08:46.280 --> 08:52.970
And that's going to give us a generator results so let's say prediction generator is equal to that model

08:53.000 --> 09:01.070
prediction and then let's say predictions themselves is a list or converting that.

09:01.370 --> 09:04.760
So that takes a little bit of time because it's actually generating the results.

09:04.760 --> 09:09.510
So now if I take a look at predictions it's a list of dictionaries where I have class IDs.

09:09.620 --> 09:10.970
So that's what I actually want to grab.

09:10.970 --> 09:12.600
So let's go ahead and do that.

09:12.620 --> 09:16.650
You can do this for loop or with a list comprehension.

09:16.670 --> 09:23.560
So he basically wants pred the class.

09:23.980 --> 09:25.940
Let's do this like this.

09:26.200 --> 09:30.740
We want pred for class underscore IDs.

09:30.760 --> 09:32.290
We want the first item there.

09:33.920 --> 09:36.290
For prayer in prediction.

09:36.290 --> 09:37.920
So this is of a list comprehension.

09:37.970 --> 09:42.670
You can check up these solutions book if you want the actual For loop that does this.

09:42.680 --> 09:46.380
But then we'll just set the sequel to final sets.

09:46.440 --> 09:46.790
There we go.

09:46.790 --> 09:48.770
So there's our list comprehension.

09:48.920 --> 09:50.770
And as I mentioned we have those dictionaries.

09:50.780 --> 09:53.140
But if I take a look at final spreads.

09:53.630 --> 10:00.620
Now I just have a big list of all the values all the labels that predicted.

10:00.680 --> 10:06.090
So once I have that list which is basically what we did here I can use classification report from scalar

10:06.110 --> 10:13.580
metrics so let's do that from S-K learn metrics import the classification report.

10:13.760 --> 10:18.830
And this is a really nice tool that essentially gives you a precision and recall value or a fun score

10:19.100 --> 10:21.740
for binary classification and we use it.

10:21.770 --> 10:26.810
If you just say classification and poor you shift tab here Ill report back the White shirt or excuse

10:26.810 --> 10:26.980
me.

10:26.990 --> 10:30.760
It needs provided the y true values and the way predicted values.

10:30.800 --> 10:35.680
So I know the y true values equal to white test and then I know the predicted values.

10:35.690 --> 10:42.200
Those are the ones that you just create appear which you have as final Pretz.

10:42.230 --> 10:43.110
So there we have it.

10:43.130 --> 10:44.940
And then when I run this it looks a little weird.

10:44.950 --> 10:47.770
That's because it's meant to be printed.

10:47.850 --> 10:51.000
So we need to print this out.

10:51.250 --> 10:55.930
And now we get this nice little format so you can see here I got around 85 percent precision recall

10:55.930 --> 10:59.630
and EF 1 score which is definitely not so bad given the data set.

10:59.650 --> 11:05.140
That's pretty much close to the best you can do using regularisation you may be able to push it a little

11:05.140 --> 11:07.830
further up but that's pretty good.

11:07.840 --> 11:08.240
All right.

11:08.260 --> 11:12.940
If you have any questions feel free to post the Kewney forums but basically ends the section on using

11:13.240 --> 11:16.060
the dancefloor basics for things like regression and classification.

11:16.270 --> 11:20.740
Coming up next in future sections we're going to start tackling problems that really only tensor flow

11:20.950 --> 11:25.890
are only deep learning has the capabilities of solving things like image classification.

11:25.900 --> 11:27.340
All right I'll see you there.

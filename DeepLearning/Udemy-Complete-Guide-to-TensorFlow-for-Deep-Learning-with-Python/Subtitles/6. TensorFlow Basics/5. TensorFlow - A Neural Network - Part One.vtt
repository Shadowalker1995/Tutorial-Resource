WEBVTT

00:05.570 --> 00:10.790
Welcome everyone to this lecture we're actually going to construct our very first tensor float neural

00:10.790 --> 00:12.390
network.

00:12.540 --> 00:17.520
We've learned about all the pieces that work with tensor flow such as sessions graphs variables and

00:17.520 --> 00:23.070
placeholders and with these building blocks we can now create our first neuron and our neural network.

00:23.070 --> 00:28.290
We're going to create a neural network that performs a very simple linear fit to some to them internal

00:28.290 --> 00:34.050
data that is we're going to have some features and then we're going to just say what is why given this

00:34.050 --> 00:37.360
X source steps are the following.

00:37.410 --> 00:42.780
We'll build the graph initiate the session and then feed data in and get some sort of output back and

00:42.780 --> 00:47.460
we're going to use the basic components such as placeholders variables that we've learned so far to

00:47.460 --> 00:48.770
accomplish this task.

00:49.770 --> 00:53.600
So what does or graph actually going to look like when we designed this linear fit.

00:53.850 --> 00:59.370
Well it should look like in matrix form something like W X plus B is equal to Z and you can see here

00:59.370 --> 01:04.290
that it's has lots of parallels with a simple linear line of y m x plus B.

01:04.410 --> 01:09.630
Except we're kind of choosing different variable terms here to line up more with kind of the classic

01:09.630 --> 01:10.320
neuron.

01:10.650 --> 01:15.510
So we're going to have a weight variable w and then we're going to multiply it by a placeholder object

01:15.560 --> 01:16.080
x.

01:16.110 --> 01:19.050
So that's what we're actually going to feed in our data.

01:19.080 --> 01:23.940
We're gonna have an operation and matrix multiplication and then to that result we'll have another note

01:23.940 --> 01:29.370
in the graph which is going to be another operation where we add in that be variable that we can pass

01:29.460 --> 01:34.530
into an activation function such as the sigmoid function.

01:34.680 --> 01:39.420
Afterwards we can add in the cost function in order to train your network to optimize the parameters

01:39.780 --> 01:41.280
such as that w term.

01:41.310 --> 01:44.730
And that beat her let's go ahead and build out the neural network.

01:44.730 --> 01:50.780
We're going to start with just a really simple review of using placeholders variables graphs and sessions.

01:50.910 --> 01:55.560
Just going to do some simple additive operations and multiplicative operations then will actually build

01:55.560 --> 01:58.780
out the neural network but chip to chip it up and get started.

01:59.190 --> 02:01.890
OK first thing to do in the notebook is a couple of imports.

02:01.930 --> 02:08.790
I'm going to import some pies and P and sensor flow as tya and then I'm also going to be setting some

02:08.790 --> 02:10.890
random seed of values here.

02:10.920 --> 02:13.310
That way you can make sure you get the same random results.

02:13.350 --> 02:22.830
I do and T.F. are a tensor flow also has a set random seed Suddenlink say and P that ran that seed set

02:22.830 --> 02:25.910
it to 101 as well as tensor Flo's random seed.

02:26.160 --> 02:29.580
And you want to make sure you run the cells in the same order I do.

02:29.610 --> 02:34.080
If you run a cell more than once you may not get the same random numbers that I'm getting here.

02:34.080 --> 02:38.970
So pay attention to this little encounter OPERATOR And you can always restart everything but just saying

02:38.970 --> 02:47.440
kernel restart and run all or you can just go sell run all let's come over here and create some random

02:47.440 --> 02:48.280
data.

02:48.310 --> 02:50.150
So this is just for demonstration purposes.

02:50.200 --> 02:55.720
I kind of want to show you how you could build a very simple session for simple operation such as addition

02:55.750 --> 02:56.830
and multiplication.

02:56.920 --> 03:00.130
Once we have that down we'll actually create our neural network.

03:00.130 --> 03:03.010
Right now I'm just going to create some random data points.

03:03.190 --> 03:05.450
So we'll say Ampyra in that uniform.

03:05.500 --> 03:11.010
Going from zero to 100 and then let's ask it to be in the shape five by five.

03:11.850 --> 03:13.890
And let's get our Random data out.

03:13.900 --> 03:18.210
So here we have a random array and you should have the exact same number of values I do here.

03:19.220 --> 03:25.870
Then we're going to do the same thing for a random B and P ran them uniform and the only thing that's

03:25.870 --> 03:32.280
going be different here is the shape of this it's that five by five it's going to be 5 by 1.

03:32.420 --> 03:33.750
And then if you want check it out.

03:33.860 --> 03:34.990
It should look like this.

03:35.000 --> 03:38.070
And again you should have the exact same numbers I do here.

03:38.120 --> 03:42.800
If you are concerned you're not going in same numbers just run the seed in the same line or the same

03:42.800 --> 03:45.650
cell as these random uniforms.

03:45.720 --> 03:51.060
Up next we need to create placeholders for these random uniform objects so let's go in and create them

03:51.550 --> 03:56.520
or create two variables A and B they're both going to be placeholders.

03:56.520 --> 04:02.750
And the only thing I really need to provide here is what data point or what data type I expect so we'll

04:02.760 --> 04:05.230
say to float 32.

04:05.240 --> 04:12.550
Now we're going to say B is equal to T.F. placeholder to float 32.

04:12.610 --> 04:16.820
Later on when we talk about convolutional neural networks that shape parameters going to be more important.

04:16.960 --> 04:22.320
But right now a and b just default shape Nunn's fine to float through to both of them.

04:22.330 --> 04:23.730
That's totally fine.

04:23.730 --> 04:28.990
Now let's create some operations to really simple operations just additive and multiplicative.

04:29.290 --> 04:30.610
So there's two ways I could do this.

04:30.610 --> 04:33.470
One is I could call these methods off the tensor flow.

04:33.670 --> 04:36.430
So there is an add operation directly up tensor flow.

04:36.460 --> 04:42.940
And if you do shift enter here it says returns x plus y element wise thing to do is called Multiply

04:42.940 --> 04:44.150
here.

04:44.420 --> 04:51.310
Multiply tensor for has that as well and its performs X times Y element wise and then it also has a

04:51.340 --> 04:53.160
matrix multiplication at all.

04:53.180 --> 04:56.790
And it says multiplies matrix A matrix B cetera.

04:56.920 --> 05:04.310
But what's nice about tensor flow is that it can actually understand the builtin Python operations or

05:04.310 --> 05:10.490
operator symbols so we can say a plus b and it's going to understand when we actually pass that intersession

05:10.580 --> 05:11.910
that I'm just asking for.

05:11.910 --> 05:19.320
In addition there Cendant going to also say Moel up for multiplication operation is equal to a times

05:19.340 --> 05:21.440
b.

05:21.440 --> 05:27.260
Now what I'm going to do is create some sessions that they can use graphs with feed dictionaries to

05:27.260 --> 05:29.910
actually get results off of these operations.

05:31.420 --> 05:43.890
I'll type out with session as S S S I want to get the results of this ad and I will say session or cesse

05:43.880 --> 05:50.000
thought run and then I need a passen the operation as well as a feed dictionary.

05:50.050 --> 05:57.130
So we have our operation here at up and that's going to add a plus b remember a and b those are placeholder

05:57.130 --> 05:57.660
objects.

05:57.700 --> 05:59.200
So they still need data.

05:59.290 --> 06:05.950
And the way we provide data using Tusser flow is with a feed dictionary our feed dictionary is going

06:05.950 --> 06:12.130
to be a dictionary where the keys are the actual placeholders and then the values corresponding to those

06:12.130 --> 06:16.030
keys is the data that has to be filled into those placeholders.

06:16.030 --> 06:25.000
For example I can say a is going to be the value 10 and B is going to be the value 20.

06:25.540 --> 06:36.160
So I can say print ad results and when I run this I should see 30 because 10 plus 20 is 30.

06:36.200 --> 06:42.000
Now we can go ahead and it's it's just 10 we're going to add in our random data here.

06:44.330 --> 06:50.660
And now when I run this I see the results of adding those together and this is Matrix addition because

06:50.660 --> 06:53.800
member had a 5 by 5 and 8 5 by 1 here.

06:53.810 --> 06:59.150
But you get the idea that essentially you're just having a field dictionary have a placeholder and then

06:59.150 --> 07:01.180
that data you're going to feed it.

07:01.180 --> 07:03.990
Let's do the same thing for our multiplication result.

07:04.100 --> 07:08.360
So say multiply result is equal to session run.

07:08.570 --> 07:15.950
And then our location OPERATOR We have a feed dictionary here and then same thing we can say Rande a

07:16.820 --> 07:25.410
and then B is brand the B and then we're just going to print those results or say multiply results run

07:25.410 --> 07:25.760
that.

07:25.800 --> 07:27.180
And now we should see two results here.

07:27.180 --> 07:31.020
So we see two matrices and if you want you can add a print new line.

07:31.300 --> 07:35.180
So they're a little more space than there you have it.

07:35.360 --> 07:35.650
OK.

07:35.680 --> 07:40.240
So that's a really simple example of running a session to create a graph with a feed dictionary.

07:40.240 --> 07:44.650
Up next in the next lecture we're actually going to create the example neural that work that we discussed

07:44.860 --> 07:46.120
in the sleights.

07:46.120 --> 07:46.810
I'll see you there.

WEBVTT

00:05.360 --> 00:10.610
Welcome everyone to the solution lecture for the free Russian exercise let's go to the Jupiter notebook

00:10.700 --> 00:12.740
and code along with the solutions.

00:12.740 --> 00:14.260
OK here we are at the notebook.

00:14.270 --> 00:16.430
Let's go ahead and start off.

00:16.490 --> 00:18.440
First thing I want to do is actually get the data.

00:18.470 --> 00:24.170
So I'm going to import pandas as PD and I'm hitting alt enter there it's Create a couple of more cells

00:24.610 --> 00:31.840
and I will read in the data so that housing is equal to PD ritziest we obviously can call the data frame

00:31.850 --> 00:32.730
whatever you want.

00:32.900 --> 00:34.940
And then we say how housing clean.

00:34.950 --> 00:40.720
See the file read that in and we can check out the head of the data frame.

00:41.010 --> 00:44.390
So I see here I have housing median age total rooms total bedrooms.

00:44.390 --> 00:46.190
Population households et cetera.

00:46.470 --> 00:50.010
Now we're going to go ahead and do is do the train test split.

00:50.330 --> 00:56.520
So in order to do that it's come at this out and let's just describe here showing the school information

00:56.520 --> 00:57.830
in case you're interested in that.

00:58.020 --> 00:59.800
But we need to do the train to split.

00:59.820 --> 01:03.130
So I want to grab all the features and then the white label.

01:03.330 --> 01:07.480
So say the y value is equal to that housing data frame.

01:07.800 --> 01:09.960
And I just want the median house value.

01:09.960 --> 01:11.850
Remember that's what I'm trying to predict.

01:11.850 --> 01:13.650
And let's grab the rest of those features.

01:13.710 --> 01:24.870
So we'll say X data is equal to housing drop and then I'm going to drop the median house value column

01:27.930 --> 01:34.710
and remembered to do that along X equal to 1 then we're going to say from Let's zoom in one more level

01:34.710 --> 01:40.530
here so we can see this clearly from S-K learn model selection.

01:40.870 --> 01:46.460
Import train split run that then train to split.

01:46.460 --> 01:50.970
Go ahead and do shift enter there which is a nice little convenient way of scrolling down through the

01:50.970 --> 01:55.110
docstring until you find this line so you can easily just copy and paste it here.

01:56.540 --> 02:04.310
All right so now I have X train x test Y train Y test at test size to be 30 percent and then we want

02:04.310 --> 02:09.990
to make sure this x is x data and this Y is why y.

02:10.220 --> 02:15.640
And if I scroll over to the right I always say random stay as one to one because I always remember that.

02:15.650 --> 02:17.940
But again that doesn't really matter here.

02:17.960 --> 02:19.330
You can set it to whatever you want.

02:19.430 --> 02:22.100
Your results just may be a little different than mine.

02:22.100 --> 02:24.040
Up next we want to scale the feature data.

02:24.050 --> 02:30.730
So we're going to use as Kaylor and pre-processing in order to create minimax scalar for saying it is

02:30.730 --> 02:32.280
actually important.

02:32.290 --> 02:40.870
So from a scalar and pre-processing import minimax scalar that I'm going to say scalar is equal to an

02:40.870 --> 02:43.260
instance of minimax scalar.

02:43.270 --> 02:45.330
And then we want to fit our scalar.

02:45.390 --> 02:49.900
And we're only going to fit it on the training data otherwise it would kind of be cheating because we

02:49.910 --> 02:54.860
want to assume that we have prior knowledge of test sets.

02:55.000 --> 03:00.720
So we fit onto the training data and then whoops we'll say X train.

03:00.800 --> 03:05.000
Is that equal to Peetie data frame.

03:05.440 --> 03:13.520
And I'm going to have the data here be equal to scalar transform extreme.

03:14.410 --> 03:19.560
And I'm doing this all in one line once that you could kind of separate this out to multiple steps than

03:19.570 --> 03:20.590
the columns here.

03:20.590 --> 03:29.290
This puts us on a new line is going to be called to X train columns and then the index here is going

03:29.290 --> 03:32.040
to be equal to x train dot index.

03:32.050 --> 03:38.560
So this is just a way to basically reset X train to be the scaled version of the data because the fight

03:38.810 --> 03:40.610
were to transform this just by itself.

03:40.610 --> 03:46.810
So if I were to grab this line just by itself let me show you what would happen if I run just this line.

03:46.930 --> 03:48.590
I get back a pie array.

03:48.610 --> 03:53.930
But instead what I want is a panel data frame like I had appear of just the features.

03:53.980 --> 04:00.670
So to make sure that happens I'm going to pass this array in as data to PD that data frame and then

04:00.670 --> 04:06.100
to make sure the columns and the index are the same and with say column 0 to X train that columns and

04:06.170 --> 04:08.410
index is equal to x train the index.

04:08.530 --> 04:13.730
And if you wanted to you could call this X train something like scaled X train et cetera but for unmelted

04:13.750 --> 04:19.440
kind of overwrite it with X train and then we're going to do the same thing for X test.

04:19.540 --> 04:30.380
So we're only doing this for the features data frame and then we just copy and paste this here except

04:30.510 --> 04:31.520
that a train.

04:31.550 --> 04:38.620
Going to be test overhears well and then over here as well.

04:39.880 --> 04:40.180
All right.

04:40.210 --> 04:46.300
So we successfully split our data using a train to split and now the feature data is scaled as well.

04:46.570 --> 04:49.680
So once we have that will scroll down and create the feature columns.

04:49.690 --> 04:51.910
So we want to create the necessary feature columns.

04:51.910 --> 04:58.060
So again I can do that easily just by saying housing columns to get a list of the columns themselves

04:58.440 --> 05:04.590
then going to import tensor flow as T.F..

05:04.930 --> 05:06.610
And then I want to create those feature columns.

05:06.610 --> 05:10.060
So I'll just do one of them and then copy the rest from the solutions.

05:10.060 --> 05:15.100
So if you want to get to the age column so that's the housing median age I'll say a variable called

05:15.130 --> 05:20.110
Age is equal to T.F. feature column numeric column.

05:20.140 --> 05:21.490
And then I'll pass in the list there.

05:21.490 --> 05:26.070
So housing median age.

05:26.250 --> 05:28.870
All right so that's one example you want to do that for the rest of them.

05:29.040 --> 05:33.480
So you want it could set this up for a loop that's kind of looping through these columns and then setting

05:33.480 --> 05:38.130
them up through some sort of variable dictionary but instead we're just going to do them all individually

05:38.250 --> 05:42.610
by copying and pasting and then run that.

05:42.710 --> 05:48.260
All right so we have a feature column and they also want to make sure to aggregate these all into some

05:48.260 --> 05:56.930
sort of variable so of say feet calls for the feature columns is equal to age rooms bedrooms pop for

05:56.930 --> 06:00.700
population household income.

06:00.700 --> 06:04.160
All right so I have my list of feature columns where these are tense full of feature columns.

06:04.310 --> 06:08.600
They're all continuous values and just using numeric column for all of them that I want to create an

06:08.660 --> 06:10.460
input function for the estimator object.

06:10.480 --> 06:22.400
Let's go ahead and do that say input func is equal to T.F. estimator inputs and I'm using panderers

06:22.400 --> 06:31.050
as my input impiously used panderers input function and then I set X as equal to x train will say Y

06:31.050 --> 06:41.920
is equal to y train and then I'll choose a batch size of 10 and let's say a number of POCs is equal

06:41.920 --> 06:43.290
to 1000.

06:43.330 --> 06:49.750
And you can play around with these values and we'll say shuffle is equal to true here because this is

06:49.750 --> 06:54.640
our training function that we want to create the estimator model and we're going to use a dense neural

06:54.640 --> 06:57.970
network Tresor you can go out and play around the hidden units.

06:58.030 --> 07:02.760
I'm going to set it to be three layers of six neurons.

07:03.020 --> 07:12.660
So say ATF estimator that's neural network repressor and I will say it in units and then we just pass

07:12.660 --> 07:13.740
in a list here.

07:13.810 --> 07:18.630
So I'm choosing six because the inputs I have are six features essentially.

07:18.770 --> 07:23.590
Again there's no real right or wrong answer you can really play around with these units as much as you

07:23.590 --> 07:26.400
want as we've seen in sensor flow playground.

07:26.590 --> 07:32.150
But right now we'll keep it simple just three layers six units each and then I also need to pass in

07:32.150 --> 07:37.670
the feature columns which we've already defined as fit calls so run that that should then create your

07:37.670 --> 07:41.420
model and then you going to train the model for a thousand steps.

07:41.640 --> 07:44.190
And then later you can come back to it and train it for more.

07:44.250 --> 07:53.430
I'll go ahead and just say model train pass in our input functions or that was that input funk we created

07:54.180 --> 08:02.800
and let's say steps that steps equal to just 20000 for it now since I it's a pretty fast computer so

08:02.800 --> 08:06.200
while that's training we're also going to do is create a prediction.

08:06.200 --> 08:09.400
So let that train let me scroll down.

08:09.420 --> 08:12.920
I'll probably end up hopping forward here as it continues.

08:12.920 --> 08:13.200
All right.

08:13.210 --> 08:14.120
Jump forward in time.

08:14.120 --> 08:17.680
My model is now finished training for 20000 steps on my computer.

08:17.680 --> 08:20.950
That took about 30 seconds and I'm a pretty fast computer.

08:21.020 --> 08:26.840
So we're going to do now it's create a prediction input function that I can use the Predict method with.

08:26.890 --> 08:34.820
So let's go ahead and do that will say predict underscore input underscore phunk is equal to T.F. estimator

08:36.170 --> 08:44.840
inputs and we're still using Pansy's input function that we're going to say x is equal to x test then

08:44.860 --> 08:50.440
we'll say the batch size is equal to what we chose before which is 10.

08:50.770 --> 08:52.600
Here I'm only using this for predict.

08:52.600 --> 09:00.250
So I'm not going to have a Y value you'd use that for evaluation then we'll say number of the POCs since

09:00.250 --> 09:03.680
I only want to run this one just to get one set of predictions I'll say one.

09:03.880 --> 09:07.240
And then I want to make sure shuffle is equal to false because I'm using this for predictions.

09:07.240 --> 09:09.040
I don't want the shuffling to occur.

09:09.450 --> 09:11.280
So predict input function.

09:11.560 --> 09:13.600
So let's get a prediction generator.

09:13.630 --> 09:18.730
Remember the return generator objects will need to cast them to a list so we'll same model and then

09:18.730 --> 09:21.230
we'll call that predict off of.

09:21.240 --> 09:29.700
So we say that predict and then I'm going to passen that predict input function so that we have that

09:29.780 --> 09:37.670
that predict generator and then we'll say predictions is equal to a list or cast that to a list.

09:37.710 --> 09:38.420
There we go.

09:38.640 --> 09:41.960
So now I ran that generator I have these predictions here.

09:42.060 --> 09:47.150
So if I check those out let me insert a cell below so to insert itself below.

09:47.400 --> 09:53.490
If I check our predictions it's essentially this list of dictionary items where we have a prediction

09:53.850 --> 09:59.650
and then some sort of array value indicating the predicted housing price.

09:59.660 --> 10:03.100
So now it's time to calculate the mean squared error.

10:03.160 --> 10:04.190
There's different ways to do this.

10:04.190 --> 10:09.400
You can use manual coding but as you learn metric metrics is really useful.

10:09.530 --> 10:11.380
So I'll show you how to do that.

10:11.420 --> 10:16.880
First off I'm going to make some final predictions here will say final Pretz is an empty list and I'll

10:16.880 --> 10:26.830
say for Fred in the predictions list that I just showed you I'm going to say final Freda's and I'm going

10:26.830 --> 10:28.950
to append this dictionary value.

10:28.960 --> 10:34.330
So essentially just grabbing all this array information instead of having a list that dictionaries will

10:34.390 --> 10:41.720
say Fred for the actions and we could have also done this with Philos. comprehension.

10:43.220 --> 10:48.940
Will say from a scalar metrics import mean squared error.

10:49.060 --> 10:53.380
And that's basically what this link takes you to if you go to click on it I'll take you to the documentation

10:53.380 --> 10:54.140
for that.

10:54.190 --> 10:59.270
But S-K learn has means square error built into it which means if you want root mean squared.

10:59.320 --> 11:01.530
You just take the square root of that.

11:01.570 --> 11:03.540
So if you take a look at it mean squared error takes.

11:03.550 --> 11:07.030
It just takes in the true values and the predictive values.

11:07.030 --> 11:08.630
So let's pass those in.

11:08.800 --> 11:13.900
We know we have Y test wups we know we have y underscore it test.

11:14.140 --> 11:18.610
And we also know we have our final predictions and then we want the square root of that.

11:18.700 --> 11:21.580
So we'll say the power of 0.5.

11:21.700 --> 11:24.580
We run that and we should get around 100000.

11:24.580 --> 11:28.420
So that means or mean squirt air is around hundred thousand dollars.

11:28.420 --> 11:30.220
If you take a look at the median price.

11:30.220 --> 11:35.450
So if we come back here and take a look at some of this the school information of our dataset so let's

11:35.500 --> 11:38.590
say housing.

11:39.490 --> 11:47.290
Describe Let's transpose that and we run that if we take a look here median house value it looks like

11:47.440 --> 11:49.980
the mean is around 200000.

11:50.260 --> 11:57.100
So we're not doing super great for our model if our mean square error is 100000 So we're not really

11:57.100 --> 11:59.040
doing quite a great fit here.

11:59.050 --> 12:04.870
So something you may want to do is trade training for more steps maybe shoot this up to 100000 or try

12:04.900 --> 12:11.140
editing your model more hidden units and we can also do is play around with a linear repressor not a

12:11.140 --> 12:14.500
dense neural network regressors and see if that performs better.

12:14.500 --> 12:20.400
But hopefully through this exercise we're able to understand the basics of the T.F. estimator API.

12:20.620 --> 12:26.080
So the main goal of this exercise was not to actually create the best model ever but it was to understand

12:26.320 --> 12:29.380
the general process using TFT estimator.

12:29.410 --> 12:29.780
OK.

12:29.800 --> 12:32.600
If you have any questions please feel free to post the Q&amp;A forms.

12:32.770 --> 12:33.970
I'll see you at the next lecture.
